1)What is extra information to network that decoder can receive ?
 -ans1 in the context vector they have weight information between decorde and encoder information 
 

2) How  Attention interface and End to End combine with self supervised and few shot learning or one shotlearning ?

3) what are possible tasks that use above combination

4) Sparse Transformer: what is self relative and factored self-attention 

5) Set Transformer: what are the benefits of reducing self attention from quadatic to linear ? 




