Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.bias', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using RobertaForSequenceClassification ...
Vocab size: 50265
Load weight patial done !
Loading Pretain Model done!
Freeze Backboned layers
the number of freezing layers 8
dataset : CLINC150
len of datasets : 750
the numbers of intents 150
the number of maximum of batching : 47
DataLoader Done !
lamda : 0.05
temperature : 0.3
name on tensorboard: Load=True_roberta-base_22_12_2021_08:18_B=16_lr _5e-06_lambda=0.05_temp=0.3
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [8]
current labels : repeat
pair class : ['repeat']
mask: [False False False False False False False False  True False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [15]
current labels : make_call
pair class : ['make_call']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [8, 15]
['maybe', 'repeat', 'report_lost_card', 'damaged_card', 'what_can_i_ask_you', 'redeem_rewards', 'make_call', 'restaurant_reservation', 'repeat', 'income', 'goodbye', 'what_are_your_hobbies', 'cook_time', 'restaurant_reviews', 'cancel_reservation', 'make_call']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.5039, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : routing
pair class : ['routing']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['todo_list', 'insurance_change', 'credit_limit', 'order_status', 'routing', 'vaccines', 'book_flight', 'yes', 'are_you_a_bot', 'credit_limit_change', 'travel_alert', 'timer', 'bill_balance', 'spending_history', 'oil_change_when', 'routing']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4632, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [15]
current labels : oil_change_when
pair class : ['oil_change_when']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [9]
current labels : who_made_you
pair class : ['who_made_you']
mask: [False False False False False False False False False  True False False
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 9]
['travel_suggestion', 'translate', 'payday', 'weather', 'insurance_change', 'rollover_401k', 'oil_change_when', 'food_last', 'who_made_you', 'who_made_you', 'no', 'date', 'change_user_name', 'who_do_you_work_for', 'reminder', 'oil_change_when']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.2851, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [5]
current labels : calculator
pair class : ['calculator']
mask: [False False False False False  True False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [15]
current labels : schedule_maintenance
pair class : ['schedule_maintenance']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [5, 15]
['calculator', 'lost_luggage', 'food_last', 'international_visa', 'carry_on', 'calculator', 'schedule_meeting', 'schedule_maintenance', 'rollover_401k', 'income', 'transactions', 'meeting_schedule', 'restaurant_reservation', 'card_declined', 'smart_home', 'schedule_maintenance']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.4869, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : insurance_change
pair class : ['insurance_change']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['ingredient_substitution', 'what_can_i_ask_you', 'maybe', 'freeze_account', 'play_music', 'roll_dice', 'timer', 'share_location', 'pto_request_status', 'income', 'next_song', 'order_checks', 'insurance_change', 'flip_coin', 'change_ai_name', 'insurance_change']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(3.8635, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [6]
current labels : todo_list_update
pair class : ['todo_list_update']
mask: [False False False False False False  True False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [14]
current labels : directions
pair class : ['directions']
mask: [False False False False False False False False False False False False
 False False  True False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : where_are_you_from
pair class : ['where_are_you_from']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [6, 14, 15]
['translate', 'calendar', 'sync_device', 'confirm_reservation', 'todo_list_update', 'spelling', 'todo_list_update', 'alarm', 'how_busy', 'directions', 'calories', 'user_name', 'where_are_you_from', 'ingredients_list', 'directions', 'where_are_you_from']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(0.8959, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [6]
current labels : pin_change
pair class : ['pin_change']
mask: [False False False False False False  True False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [12]
current labels : uber
pair class : ['uber']
mask: [False False False False False False False False False False False False
  True False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : measurement_conversion
pair class : ['measurement_conversion']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [6, 12, 15]
['change_language', 'pin_change', 'who_do_you_work_for', 'reminder_update', 'account_blocked', 'interest_rate', 'pin_change', 'next_song', 'traffic', 'calendar', 'calculator', 'uber', 'uber', 'calories', 'measurement_conversion', 'measurement_conversion']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(0.6967, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : apr
pair class : ['apr']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['min_payment', 'interest_rate', 'date', 'bill_due', 'apr', 'reminder', 'text', 'ingredient_substitution', 'find_phone', 'carry_on', 'greeting', 'meaning_of_life', 'pin_change', 'are_you_a_bot', 'how_old_are_you', 'apr']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(6.5194, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [11]
current labels : what_song
pair class : ['what_song']
mask: [False False False False False False False False False False False  True
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : reminder
pair class : ['reminder']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [11, 15]
['lost_luggage', 'what_song', 'todo_list', 'what_are_your_hobbies', 'book_hotel', 'reminder_update', 'plug_type', 'apr', 'reminder', 'insurance', 'food_last', 'what_song', 'card_declined', 'replacement_card_duration', 'change_speed', 'reminder']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.1515, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [7]
current labels : new_card
pair class : ['new_card']
mask: [False False False False False False False  True False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 13
current skips : [15]
current labels : calendar
pair class : ['calendar']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [7, 15]
['jump_start', 'new_card', 'international_visa', 'mpg', 'change_accent', 'food_last', 'change_volume', 'new_card', 'freeze_account', 'timer', 'accept_reservations', 'spelling', 'vaccines', 'calendar', 'rewards_balance', 'calendar']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0289, grad_fn=<DivBackward0>)

[1,    10] loss_total: 40.354 loss_supervised_contrasive:  2.689 loss_intent :753.299 
skip_time: 0
total : 10
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : ingredient_substitution
pair class : ['ingredient_substitution']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['pto_used', 'travel_alert', 'damaged_card', 'plug_type', 'ingredient_substitution', 'ingredients_list', 'income', 'expiration_date', 'thank_you', 'credit_limit', 'timer', 'pto_request', 'calendar_update', 'mpg', 'transfer', 'ingredient_substitution']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(3.8745, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [11]
current labels : recipe
pair class : ['recipe']
mask: [False False False False False False False False False False False  True
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [15]
current labels : rewards_balance
pair class : ['rewards_balance']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [11, 15]
['greeting', 'report_fraud', 'are_you_a_bot', 'alarm', 'definition', 'recipe', 'order_status', 'gas_type', 'smart_home', 'taxes', 'rewards_balance', 'recipe', 'credit_limit_change', 'bill_balance', 'text', 'rewards_balance']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.3615, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [7]
current labels : reset_settings
pair class : ['reset_settings']
mask: [False False False False False False False  True False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : shopping_list
pair class : ['shopping_list']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [7, 15]
['measurement_conversion', 'expiration_date', 'reset_settings', 'spelling', 'cancel', 'what_is_your_name', 'distance', 'reset_settings', 'shopping_list', 'play_music', 'credit_score', 'roll_dice', 'pto_used', 'gas_type', 'travel_alert', 'shopping_list']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.3262, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [15]
current labels : exchange_rate
pair class : ['exchange_rate']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [8]
current labels : jump_start
pair class : ['jump_start']
mask: [False False False False False False False False  True False False False
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 8]
['book_flight', 'schedule_meeting', 'oil_change_how', 'exchange_rate', 'jump_start', 'nutrition_info', 'meal_suggestion', 'last_maintenance', 'jump_start', 'order', 'meaning_of_life', 'bill_due', 'find_phone', 'change_speed', 'calendar_update', 'exchange_rate']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.4086, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [15]
current labels : shopping_list
pair class : ['shopping_list']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['shopping_list', 'tell_joke', 'current_location', 'lost_luggage', 'calories', 'transactions', 'apr', 'uber', 'change_user_name', 'play_music', 'next_holiday', 'where_are_you_from', 'flip_coin', 'application_status', 'how_busy', 'shopping_list']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(6.1459, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : freeze_account
pair class : ['freeze_account']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['transfer', 'uber', 'time', 'user_name', 'freeze_account', 'pto_used', 'find_phone', 'flight_status', 'routing', 'order_status', 'mpg', 'next_holiday', 'exchange_rate', 'report_fraud', 'reminder_update', 'freeze_account']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.7070, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [7]
current labels : measurement_conversion
pair class : ['measurement_conversion']
mask: [False False False False False False False  True False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : update_playlist
pair class : ['update_playlist']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [7, 15]
['measurement_conversion', 'update_playlist', 'carry_on', 'routing', 'replacement_card_duration', 'no', 'oil_change_how', 'measurement_conversion', 'order_checks', 'account_blocked', 'plug_type', 'change_ai_name', 'damaged_card', 'calories', 'card_declined', 'update_playlist']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.3123, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [3]
current labels : shopping_list_update
pair class : ['shopping_list_update']
mask: [False False False  True False False False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [12]
current labels : translate
pair class : ['translate']
mask: [False False False False False False False False False False False False
  True False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [15]
current labels : spending_history
pair class : ['spending_history']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [3, 12, 15]
['shopping_list_update', 'find_phone', 'translate', 'shopping_list_update', 'thank_you', 'spending_history', 'reset_settings', 'credit_limit_change', 'direct_deposit', 'distance', 'cancel', 'schedule_meeting', 'translate', 'what_is_your_name', 'definition', 'spending_history']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(0.7991, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [15]
current labels : traffic
pair class : ['traffic']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['report_lost_card', 'bill_due', 'next_song', 'traffic', 'interest_rate', 'goodbye', 'where_are_you_from', 'sync_device', 'cancel', 'spending_history', 'direct_deposit', 'do_you_have_pets', 'todo_list', 'cancel_reservation', 'travel_notification', 'traffic']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.2411, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : reset_settings
pair class : ['reset_settings']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [11]
current labels : restaurant_suggestion
pair class : ['restaurant_suggestion']
mask: [False False False False False False False False False False False  True
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 11]
['pin_change', 'reset_settings', 'bill_due', 'taxes', 'whisper_mode', 'rollover_401k', 'change_language', 'replacement_card_duration', 'todo_list_update', 'restaurant_suggestion', 'application_status', 'restaurant_suggestion', 'pto_request', 'change_speed', 'damaged_card', 'reset_settings']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.6606, grad_fn=<DivBackward0>)

[1,    20] loss_total: 40.348 loss_supervised_contrasive:  5.373 loss_intent :1506.588 
skip_time: 0
total : 20
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [10]
current labels : are_you_a_bot
pair class : ['are_you_a_bot']
mask: [False False False False False False False False False False  True False
 False False False False]
count: 16
numbers of pairs one label : 1
----
masking label debug : ['maybe' 'maybe']
current labels  maybe
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 4
current skips : [12 15]
current labels : maybe
pair class : ['maybe' 'maybe']
mask: [False False False False False False False False False False False False
  True False False  True]
count: 16
numbers of pairs one label : 2
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [10, 12, 15]
['transactions', 'insurance', 'international_fees', 'are_you_a_bot', 'maybe', 'book_flight', 'pto_balance', 'date', 'who_made_you', 'who_do_you_work_for', 'are_you_a_bot', 'alarm', 'maybe', 'interest_rate', 'tire_pressure', 'maybe']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(0.8363, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [4]
current labels : last_maintenance
pair class : ['last_maintenance']
mask: [False False False False  True False False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [15]
current labels : timezone
pair class : ['timezone']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [4, 15]
['accept_reservations', 'travel_notification', 'last_maintenance', 'tire_pressure', 'last_maintenance', 'schedule_maintenance', 'spelling', 'traffic', 'recipe', 'travel_suggestion', 'timezone', 'routing', 'application_status', 'current_location', 'order_status', 'timezone']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.4759, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [15]
current labels : fun_fact
pair class : ['fun_fact']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 13
current skips : [14]
current labels : meeting_schedule
pair class : ['meeting_schedule']
mask: [False False False False False False False False False False False False
 False False  True False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 14]
['redeem_rewards', 'goodbye', 'current_location', 'date', 'directions', 'taxes', 'fun_fact', 'roll_dice', 'gas', 'definition', 'book_hotel', 'lost_luggage', 'apr', 'meeting_schedule', 'meeting_schedule', 'fun_fact']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.9044, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : tire_change
pair class : ['tire_change']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['spending_history', 'tire_change', 'change_accent', 'reset_settings', 'order', 'what_can_i_ask_you', 'flight_status', 'expiration_date', 'report_fraud', 'shopping_list', 'maybe', 'credit_limit', 'ingredient_substitution', 'credit_score', 'time', 'tire_change']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.8959, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [11]
current labels : how_old_are_you
pair class : ['how_old_are_you']
mask: [False False False False False False False False False False False  True
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : user_name
pair class : ['user_name']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [11, 15]
['jump_start', 'flight_status', 'how_old_are_you', 'meaning_of_life', 'meeting_schedule', 'make_call', 'min_payment', 'travel_notification', 'bill_balance', 'user_name', 'balance', 'how_old_are_you', 'alarm', 'schedule_maintenance', 'weather', 'user_name']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.1546, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 13
current skips : [15]
current labels : greeting
pair class : ['greeting']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['international_fees', 'last_maintenance', 'update_playlist', 'improve_credit_score', 'travel_suggestion', 'freeze_account', 'payday', 'what_is_your_name', 'exchange_rate', 'meal_suggestion', 'oil_change_how', 'what_are_your_hobbies', 'flip_coin', 'greeting', 'car_rental', 'greeting']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(6.2548, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [15]
current labels : nutrition_info
pair class : ['nutrition_info']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['current_location', 'transfer', 'international_visa', 'weather', 'balance', 'nutrition_info', 'who_do_you_work_for', 'how_busy', 'what_can_i_ask_you', 'update_playlist', 'min_payment', 'ingredients_list', 'application_status', 'transactions', 'distance', 'nutrition_info']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.6525, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [15]
current labels : timezone
pair class : ['timezone']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['tire_change', 'book_flight', 'update_playlist', 'redeem_rewards', 'travel_alert', 'timezone', 'yes', 'lost_luggage', 'flip_coin', 'how_old_are_you', 'jump_start', 'credit_limit', 'card_declined', 'current_location', 'change_ai_name', 'timezone']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.5561, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : time
pair class : ['time']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['change_user_name', 'what_song', 'insurance', 'gas_type', 'order_status', 'w2', 'gas', 'order', 'cook_time', 'sync_device', 'oil_change_how', 'measurement_conversion', 'pto_balance', 'timezone', 'time', 'time']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.8756, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : travel_notification
pair class : ['travel_notification']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [13]
current labels : min_payment
pair class : ['min_payment']
mask: [False False False False False False False False False False False False
 False  True False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 13]
['make_call', 'bill_balance', 'travel_notification', 'order', 'application_status', 'change_ai_name', 'international_fees', 'credit_limit', 'min_payment', 'play_music', 'timezone', 'pto_request_status', 'yes', 'min_payment', 'change_user_name', 'travel_notification']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.7803, grad_fn=<DivBackward0>)

[1,    30] loss_total: 41.194 loss_supervised_contrasive:  8.912 loss_intent :2259.694 
skip_time: 0
total : 30
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [2]
current labels : payday
pair class : ['payday']
mask: [False False  True False False False False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [13]
current labels : restaurant_reviews
pair class : ['restaurant_reviews']
mask: [False False False False False False False False False False False False
 False  True False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [15]
current labels : damaged_card
pair class : ['damaged_card']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [2, 13, 15]
['payday', 'w2', 'payday', 'restaurant_reviews', 'flight_status', 'order_checks', 'what_can_i_ask_you', 'oil_change_when', 'update_playlist', 'pto_used', 'damaged_card', 'next_holiday', 'todo_list_update', 'restaurant_reviews', 'goodbye', 'damaged_card']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.1843, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [11]
current labels : share_location
pair class : ['share_location']
mask: [False False False False False False False False False False False  True
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [15]
current labels : sync_device
pair class : ['sync_device']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [11, 15]
['tire_change', 'nutrition_info', 'tire_pressure', 'calories', 'share_location', 'make_call', 'next_song', 'sync_device', 'cancel_reservation', 'balance', 'greeting', 'share_location', 'change_volume', 'international_visa', 'credit_score', 'sync_device']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.4581, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : new_card
pair class : ['new_card']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [13]
current labels : fun_fact
pair class : ['fun_fact']
mask: [False False False False False False False False False False False False
 False  True False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 13]
['carry_on', 'mpg', 'definition', 'reminder', 'meal_suggestion', 'rewards_balance', 'redeem_rewards', 'tire_change', 'new_card', 'pay_bill', 'fun_fact', 'insurance_change', 'what_is_your_name', 'fun_fact', 'pto_used', 'new_card']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.4531, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [15]
current labels : repeat
pair class : ['repeat']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['who_made_you', 'routing', 'order_checks', 'transfer', 'ingredients_list', 'change_language', 'repeat', 'where_are_you_from', 'restaurant_reservation', 'shopping_list_update', 'pto_request_status', 'do_you_have_pets', 'restaurant_reviews', 'directions', 'account_blocked', 'repeat']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.0592, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [6]
current labels : plug_type
pair class : ['plug_type']
mask: [False False False False False False  True False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 13
current skips : [15]
current labels : next_song
pair class : ['next_song']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [6, 15]
['rollover_401k', 'book_hotel', 'plug_type', 'replacement_card_duration', 'cancel', 'gas_type', 'plug_type', 'user_name', 'yes', 'reminder_update', 'thank_you', 'do_you_have_pets', 'pay_bill', 'next_song', 'payday', 'next_song']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.9220, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [15]
current labels : meeting_schedule
pair class : ['meeting_schedule']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [13]
current labels : travel_suggestion
pair class : ['travel_suggestion']
mask: [False False False False False False False False False False False False
 False  True False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 13]
['meeting_schedule', 'cancel_reservation', 'where_are_you_from', 'travel_suggestion', 'expiration_date', 'yes', 'order_checks', 'pay_bill', 'meaning_of_life', 'change_accent', 'restaurant_reviews', 'nutrition_info', 'balance', 'travel_suggestion', 'schedule_maintenance', 'meeting_schedule']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0160, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [15]
current labels : todo_list
pair class : ['todo_list']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['next_holiday', 'pto_balance', 'text', 'whisper_mode', 'credit_limit_change', 'play_music', 'todo_list', 'time', 'share_location', 'pto_request', 'interest_rate', 'tell_joke', 'confirm_reservation', 'thank_you', 'change_volume', 'todo_list']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(3.8279, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [15]
current labels : cook_time
pair class : ['cook_time']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [6]
current labels : restaurant_reservation
pair class : ['restaurant_reservation']
mask: [False False False False False False  True False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [11]
current labels : tell_joke
pair class : ['tell_joke']
mask: [False False False False False False False False False False False  True
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [15, 6, 11]
['calendar_update', 'restaurant_suggestion', 'how_old_are_you', 'cook_time', 'improve_credit_score', 'restaurant_reservation', 'restaurant_reservation', 'do_you_have_pets', 'travel_notification', 'pto_request', 'tell_joke', 'tell_joke', 'international_fees', 'account_blocked', 'transfer', 'cook_time']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(0.8618, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [11]
current labels : no
pair class : ['no']
mask: [False False False False False False False False False False False  True
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : translate
pair class : ['translate']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [11, 15]
['calculator', 'no', 'oil_change_when', 'weather', 'book_flight', 'greeting', 'what_are_your_hobbies', 'pto_request', 'confirm_reservation', 'translate', 'car_rental', 'no', 'change_volume', 'taxes', 'report_fraud', 'translate']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.8530, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : time
pair class : ['time']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['date', 'time', 'uber', 'distance', 'next_holiday', 'what_song', 'meaning_of_life', 'credit_score', 'balance', 'schedule_meeting', 'restaurant_suggestion', 'how_busy', 'direct_deposit', 'smart_home', 'calculator', 'time']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(3.8647, grad_fn=<DivBackward0>)

[1,    40] loss_total: 40.004 loss_supervised_contrasive:  11.262 loss_intent :3012.772 
skip_time: 0
total : 40
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [10]
current labels : fun_fact
pair class : ['fun_fact']
mask: [False False False False False False False False False False  True False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 13
current skips : [15]
current labels : change_accent
pair class : ['change_accent']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [10, 15]
['meal_suggestion', 'gas', 'repeat', 'goodbye', 'whisper_mode', 'spelling', 'cancel', 'fun_fact', 'pto_request_status', 'text', 'fun_fact', 'calendar', 'change_volume', 'change_accent', 'shopping_list_update', 'change_accent']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.6810, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [15]
current labels : vaccines
pair class : ['vaccines']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['todo_list', 'book_hotel', 'insurance_change', 'international_visa', 'smart_home', 'vaccines', 'sync_device', 'car_rental', 'cook_time', 'recipe', 'oil_change_when', 'improve_credit_score', 'schedule_maintenance', 'tell_joke', 'gas', 'vaccines']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.6140, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : improve_credit_score
pair class : ['improve_credit_score']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['roll_dice', 'book_hotel', 'travel_alert', 'nutrition_info', 'whisper_mode', 'card_declined', 'income', 'direct_deposit', 'how_busy', 'meal_suggestion', 'distance', 'calendar_update', 'improve_credit_score', 'bill_due', 'account_blocked', 'improve_credit_score']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.0575, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : timer
pair class : ['timer']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['change_ai_name', 'change_accent', 'direct_deposit', 'cook_time', 'last_maintenance', 'rewards_balance', 'ingredients_list', 'vaccines', 'w2', 'improve_credit_score', 'oil_change_how', 'food_last', 'transactions', 'report_lost_card', 'timer', 'timer']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.0493, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [15]
current labels : expiration_date
pair class : ['expiration_date']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['reminder_update', 'repeat', 'text', 'pto_balance', 'flight_status', 'expiration_date', 'rollover_401k', 'share_location', 'carry_on', 'alarm', 'ingredient_substitution', 'insurance', 'rewards_balance', 'new_card', 'smart_home', 'expiration_date']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.8854, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : calendar
pair class : ['calendar']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['spending_history', 'report_lost_card', 'calendar', 'do_you_have_pets', 'tire_pressure', 'gas_type', 'credit_score', 'credit_limit_change', 'reminder', 'bill_balance', 'what_is_your_name', 'car_rental', 'w2', 'thank_you', 'what_are_your_hobbies', 'calendar']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.6711, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [13]
current labels : apr
pair class : ['apr']
mask: [False False False False False False False False False False False False
 False  True]
count: 14
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [13]
['todo_list_update', 'freeze_account', 'car_rental', 'tire_change', 'definition', 'pto_request_status', 'w2', 'apr', 'accept_reservations', 'mpg', 'report_fraud', 'find_phone', 'traffic', 'apr']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([14, 14])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.9170, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [13]
current labels : reset_settings
pair class : ['reset_settings']
mask: [False False False False False False False False False False False False
 False  True False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [15]
current labels : change_speed
pair class : ['change_speed']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [13, 15]
['report_fraud', 'reset_settings', 'confirm_reservation', 'reminder_update', 'definition', 'plug_type', 'weather', 'whisper_mode', 'schedule_maintenance', 'replacement_card_duration', 'change_speed', 'insurance', 'international_visa', 'reset_settings', 'thank_you', 'change_speed']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.5854, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [15]
current labels : tell_joke
pair class : ['tell_joke']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['rewards_balance', 'ingredients_list', 'no', 'calories', 'meaning_of_life', 'transactions', 'play_music', 'restaurant_suggestion', 'where_are_you_from', 'direct_deposit', 'tell_joke', 'todo_list', 'traffic', 'smart_home', 'transfer', 'tell_joke']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(3.8442, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : carry_on
pair class : ['carry_on']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['pin_change', 'travel_alert', 'expiration_date', 'travel_suggestion', 'direct_deposit', 'timer', 'weather', 'timezone', 'balance', 'share_location', 'gas_type', 'carry_on', 'restaurant_reviews', 'credit_limit', 'travel_notification', 'carry_on']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5091, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : text
pair class : ['text']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['plug_type', 'do_you_have_pets', 'vaccines', 'sync_device', 'what_is_your_name', 'tire_pressure', 'find_phone', 'schedule_meeting', 'text', 'exchange_rate', 'report_fraud', 'repeat', 'gas_type', 'transactions', 'insurance_change', 'text']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.7892, grad_fn=<DivBackward0>)

----
masking label debug : ['shopping_list' 'shopping_list']
current labels  shopping_list
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 12
current skips : [13 15]
current labels : shopping_list
pair class : ['shopping_list' 'shopping_list']
mask: [False False False False False False False False False False False False
 False  True False  True]
count: 16
numbers of pairs one label : 2
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [13, 15]
['do_you_have_pets', 'uber', 'pto_request_status', 'yes', 'measurement_conversion', 'oil_change_when', 'improve_credit_score', 'change_volume', 'freeze_account', 'restaurant_reviews', 'insurance', 'payday', 'shopping_list', 'shopping_list', 'min_payment', 'shopping_list']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0117, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [10]
current labels : pto_request
pair class : ['pto_request']
mask: [False False False False False False False False False False  True False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [15]
current labels : last_maintenance
pair class : ['last_maintenance']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [10, 15]
['spelling', 'tire_change', 'nutrition_info', 'vaccines', 'pto_request', 'restaurant_reservation', 'last_maintenance', 'calories', 'apr', 'cancel', 'pto_request', 'smart_home', 'change_user_name', 'insurance', 'who_do_you_work_for', 'last_maintenance']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0813, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : redeem_rewards
pair class : ['redeem_rewards']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['interest_rate', 'directions', 'next_holiday', 'thank_you', 'book_hotel', 'cook_time', 'order_checks', 'expiration_date', 'change_speed', 'taxes', 'gas_type', 'pto_used', 'shopping_list', 'insurance_change', 'redeem_rewards', 'redeem_rewards']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(6.2755, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [8]
current labels : interest_rate
pair class : ['interest_rate']
mask: [False False False False False False False False  True False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [15]
current labels : find_phone
pair class : ['find_phone']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [8, 15]
['ingredients_list', 'transfer', 'interest_rate', 'calendar', 'freeze_account', 'ingredient_substitution', 'account_blocked', 'find_phone', 'interest_rate', 'pto_request', 'accept_reservations', 'reminder', 'order_status', 'spelling', 'reminder_update', 'find_phone']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.4667, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 13
current skips : [15]
current labels : w2
pair class : ['w2']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['distance', 'travel_notification', 'payday', 'interest_rate', 'sync_device', 'flight_status', 'spelling', 'report_fraud', 'no', 'car_rental', 'maybe', 'nutrition_info', 'roll_dice', 'w2', 'cancel', 'w2']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.0924, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : travel_suggestion
pair class : ['travel_suggestion']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['schedule_meeting', 'calculator', 'are_you_a_bot', 'new_card', 'reminder_update', 'pay_bill', 'time', 'replacement_card_duration', 'travel_suggestion', 'translate', 'recipe', 'plug_type', 'text', 'credit_limit_change', 'last_maintenance', 'travel_suggestion']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.4889, grad_fn=<DivBackward0>)

[2,    10] loss_total: 41.366 loss_supervised_contrasive:  3.714 loss_intent :753.032 
skip_time: 0
total : 57
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [7]
current labels : calendar_update
pair class : ['calendar_update']
mask: [False False False False False False False  True False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : rollover_401k
pair class : ['rollover_401k']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [7, 15]
['freeze_account', 'text', 'how_old_are_you', 'calendar_update', 'rollover_401k', 'application_status', 'reset_settings', 'calendar_update', 'card_declined', 'gas', 'insurance_change', 'what_song', 'pay_bill', 'change_language', 'uber', 'rollover_401k']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.9672, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : where_are_you_from
pair class : ['where_are_you_from']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [9]
current labels : credit_score
pair class : ['credit_score']
mask: [False False False False False False False False False  True False False
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 9]
['w2', 'vaccines', 'balance', 'change_speed', 'where_are_you_from', 'book_flight', 'meal_suggestion', 'credit_score', 'report_lost_card', 'credit_score', 'how_busy', 'what_song', 'cook_time', 'what_is_your_name', 'new_card', 'where_are_you_from']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.7250, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [15]
current labels : travel_suggestion
pair class : ['travel_suggestion']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['travel_alert', 'what_is_your_name', 'report_fraud', 'income', 'change_volume', 'change_accent', 'change_speed', 'yes', 'calendar', 'ingredients_list', 'travel_suggestion', 'timer', 'weather', 'card_declined', 'reminder', 'travel_suggestion']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(6.2753, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : new_card
pair class : ['new_card']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['last_maintenance', 'spending_history', 'order', 'who_made_you', 'what_are_your_hobbies', 'freeze_account', 'restaurant_reviews', 'application_status', 'roll_dice', 'new_card', 'meeting_schedule', 'pto_balance', 'fun_fact', 'change_user_name', 'expiration_date', 'new_card']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.8957, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : change_user_name
pair class : ['change_user_name']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['time', 'account_blocked', 'change_user_name', 'flip_coin', 'tire_pressure', 'restaurant_reservation', 'tell_joke', 'thank_you', 'change_language', 'goodbye', 'book_flight', 'who_made_you', 'schedule_maintenance', 'repeat', 'bill_balance', 'change_user_name']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.8062, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [15]
current labels : mpg
pair class : ['mpg']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['sync_device', 'what_is_your_name', 'tire_change', 'w2', 'repeat', 'min_payment', 'mpg', 'time', 'weather', 'change_volume', 'transactions', 'yes', 'payday', 'rollover_401k', 'todo_list_update', 'mpg']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.8379, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : replacement_card_duration
pair class : ['replacement_card_duration']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['credit_limit_change', 'shopping_list_update', 'replacement_card_duration', 'recipe', 'next_holiday', 'current_location', 'calculator', 'alarm', 'plug_type', 'timezone', 'flight_status', 'timer', 'text', 'order_checks', 'who_do_you_work_for', 'replacement_card_duration']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.8435, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [15]
current labels : change_volume
pair class : ['change_volume']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['where_are_you_from', 'account_blocked', 'flip_coin', 'change_volume', 'goodbye', 'exchange_rate', 'cancel_reservation', 'taxes', 'confirm_reservation', 'calendar_update', 'report_fraud', 'mpg', 'lost_luggage', 'apr', 'timezone', 'change_volume']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(3.8705, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [10]
current labels : replacement_card_duration
pair class : ['replacement_card_duration']
mask: [False False False False False False False False False False  True False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [15]
current labels : change_ai_name
pair class : ['change_ai_name']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [10, 15]
['ingredient_substitution', 'replacement_card_duration', 'what_song', 'book_flight', 'goodbye', 'gas_type', 'change_ai_name', 'routing', 'pto_request_status', 'reset_settings', 'replacement_card_duration', 'fun_fact', 'international_fees', 'carry_on', 'rewards_balance', 'change_ai_name']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.3600, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : flip_coin
pair class : ['flip_coin']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['cancel_reservation', 'ingredient_substitution', 'yes', 'shopping_list_update', 'flip_coin', 'pay_bill', 'mpg', 'next_holiday', 'direct_deposit', 'definition', 'todo_list_update', 'meaning_of_life', 'apr', 'how_busy', 'balance', 'flip_coin']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.1456, grad_fn=<DivBackward0>)

[2,    20] loss_total: 41.620 loss_supervised_contrasive:  7.687 loss_intent :1505.973 
skip_time: 0
total : 67
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [1]
current labels : date
pair class : ['date']
mask: [False  True False False False False False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [15]
current labels : meeting_schedule
pair class : ['meeting_schedule']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [1, 15]
['date', 'date', 'redeem_rewards', 'rollover_401k', 'damaged_card', 'meeting_schedule', 'greeting', 'gas', 'update_playlist', 'car_rental', 'pto_request', 'alarm', 'next_song', 'cancel_reservation', 'interest_rate', 'meeting_schedule']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.5968, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : apr
pair class : ['apr']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['no', 'change_user_name', 'apr', 'pto_used', 'food_last', 'calendar_update', 'how_old_are_you', 'pto_request_status', 'international_fees', 'accept_reservations', 'oil_change_when', 'share_location', 'restaurant_suggestion', 'account_blocked', 'what_can_i_ask_you', 'apr']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(3.8962, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : lost_luggage
pair class : ['lost_luggage']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['make_call', 'change_language', 'lost_luggage', 'exchange_rate', 'tire_change', 'cook_time', 'transfer', 'cancel_reservation', 'book_hotel', 'plug_type', 'pto_balance', 'order_status', 'min_payment', 'ingredients_list', 'meal_suggestion', 'lost_luggage']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.8251, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 13
current skips : [15]
current labels : calculator
pair class : ['calculator']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['flip_coin', 'oil_change_how', 'travel_notification', 'user_name', 'shopping_list', 'food_last', 'min_payment', 'how_busy', 'shopping_list_update', 'international_fees', 'cancel', 'routing', 'calories', 'calculator', 'calendar_update', 'calculator']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.2999, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : food_last
pair class : ['food_last']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['user_name', 'update_playlist', 'confirm_reservation', 'date', 'meeting_schedule', 'greeting', 'translate', 'application_status', 'order_status', 'spending_history', 'transactions', 'food_last', 'directions', 'income', 'traffic', 'food_last']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4638, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [15]
current labels : directions
pair class : ['directions']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['rollover_401k', 'meeting_schedule', 'reset_settings', 'roll_dice', 'routing', 'directions', 'weather', 'change_volume', 'tell_joke', 'what_are_your_hobbies', 'calculator', 'text', 'book_hotel', 'tire_pressure', 'damaged_card', 'directions']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(3.8989, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [15]
current labels : traffic
pair class : ['traffic']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['jump_start', 'next_holiday', 'alarm', 'share_location', 'pto_request_status', 'make_call', 'traffic', 'credit_score', 'book_hotel', 'order', 'todo_list_update', 'new_card', 'pin_change', 'order_checks', 'carry_on', 'traffic']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(3.8336, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : credit_limit_change
pair class : ['credit_limit_change']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['order_status', 'carry_on', 'what_are_your_hobbies', 'credit_limit', 'pto_balance', 'expiration_date', 'date', 'reminder', 'change_accent', 'credit_limit_change', 'rewards_balance', 'change_language', 'measurement_conversion', 'international_visa', 'meal_suggestion', 'credit_limit_change']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.5445, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [15]
current labels : what_song
pair class : ['what_song']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [14]
current labels : credit_limit
pair class : ['credit_limit']
mask: [False False False False False False False False False False False False
 False False  True False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 14]
['smart_home', 'current_location', 'car_rental', 'restaurant_reservation', 'bill_balance', 'transfer', 'international_visa', 'translate', 'redeem_rewards', 'restaurant_suggestion', 'what_song', 'credit_limit', 'change_speed', 'meal_suggestion', 'credit_limit', 'what_song']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.8846, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : improve_credit_score
pair class : ['improve_credit_score']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['next_holiday', 'payday', 'travel_alert', 'meal_suggestion', 'what_can_i_ask_you', 'repeat', 'meaning_of_life', 'travel_suggestion', 'calendar', 'improve_credit_score', 'damaged_card', 'distance', 'user_name', 'order_checks', 'are_you_a_bot', 'improve_credit_score']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(3.8458, grad_fn=<DivBackward0>)

[2,    30] loss_total: 41.659 loss_supervised_contrasive:  11.696 loss_intent :2258.969 
skip_time: 0
total : 77
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [15]
current labels : order
pair class : ['order']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['how_old_are_you', 'credit_limit', 'change_ai_name', 'ingredient_substitution', 'bill_balance', 'damaged_card', 'rollover_401k', 'order', 'w2', 'freeze_account', 'maybe', 'how_busy', 'recipe', 'thank_you', 'ingredients_list', 'order']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(6.0493, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : application_status
pair class : ['application_status']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [13]
current labels : next_song
pair class : ['next_song']
mask: [False False False False False False False False False False False False
 False  True False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 13]
['what_are_your_hobbies', 'change_user_name', 'new_card', 'are_you_a_bot', 'meaning_of_life', 'calendar', 'lost_luggage', 'traffic', 'application_status', 'carry_on', 'next_song', 'card_declined', 'time', 'next_song', 'update_playlist', 'application_status']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.7761, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : flip_coin
pair class : ['flip_coin']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['pin_change', 'how_busy', 'flip_coin', 'next_song', 'whisper_mode', 'gas_type', 'roll_dice', 'find_phone', 'measurement_conversion', 'what_can_i_ask_you', 'ingredient_substitution', 'last_maintenance', 'income', 'play_music', 'redeem_rewards', 'flip_coin']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.2114, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : jump_start
pair class : ['jump_start']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['direct_deposit', 'order', 'lost_luggage', 'car_rental', 'calories', 'card_declined', 'change_language', 'definition', 'confirm_reservation', 'jump_start', 'roll_dice', 'routing', 'who_do_you_work_for', 'update_playlist', 'oil_change_when', 'jump_start']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.9838, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [8]
current labels : share_location
pair class : ['share_location']
mask: [False False False False False False False False  True False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : restaurant_reservation
pair class : ['restaurant_reservation']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [8, 15]
['cancel', 'bill_due', 'find_phone', 'change_accent', 'apr', 'accept_reservations', 'expiration_date', 'share_location', 'share_location', 'oil_change_when', 'book_flight', 'restaurant_reservation', 'todo_list_update', 'reminder', 'order_status', 'restaurant_reservation']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.7300, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [15]
current labels : mpg
pair class : ['mpg']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['current_location', 'application_status', 'calories', 'greeting', 'timer', 'pin_change', 'mpg', 'user_name', 'vaccines', 'schedule_maintenance', 'jump_start', 'smart_home', 'taxes', 'credit_score', 'whisper_mode', 'mpg']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(3.8858, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : who_made_you
pair class : ['who_made_you']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['tell_joke', 'todo_list', 'find_phone', 'pay_bill', 'insurance', 'do_you_have_pets', 'play_music', 'todo_list_update', 'travel_alert', 'schedule_meeting', 'sync_device', 'oil_change_when', 'who_made_you', 'recipe', 'travel_notification', 'who_made_you']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(3.8975, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : schedule_maintenance
pair class : ['schedule_maintenance']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['smart_home', 'tire_change', 'flight_status', 'make_call', 'taxes', 'change_accent', 'international_fees', 'measurement_conversion', 'calculator', 'thank_you', 'insurance_change', 'income', 'schedule_maintenance', 'what_can_i_ask_you', 'min_payment', 'schedule_maintenance']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.0669, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [5]
current labels : uber
pair class : ['uber']
mask: [False False False False False  True False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : cancel_reservation
pair class : ['cancel_reservation']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [5, 15]
['current_location', 'directions', 'uber', 'spelling', 'insurance', 'uber', 'shopping_list_update', 'cook_time', 'definition', 'cancel_reservation', 'reminder_update', 'flight_status', 'change_ai_name', 'maybe', 'international_fees', 'cancel_reservation']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.6588, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : who_do_you_work_for
pair class : ['who_do_you_work_for']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['shopping_list', 'restaurant_suggestion', 'whisper_mode', 'last_maintenance', 'who_do_you_work_for', 'book_flight', 'order', 'pto_used', 'bill_due', 'nutrition_info', 'gas', 'tell_joke', 'pto_request', 'todo_list', 'translate', 'who_do_you_work_for']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.9231, grad_fn=<DivBackward0>)

[2,    40] loss_total: 41.468 loss_supervised_contrasive:  15.514 loss_intent :3011.973 
skip_time: 0
total : 87
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [15]
current labels : cook_time
pair class : ['cook_time']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['schedule_meeting', 'cancel', 'traffic', 'user_name', 'transfer', 'do_you_have_pets', 'goodbye', 'taxes', 'todo_list', 'w2', 'cook_time', 'nutrition_info', 'timezone', 'who_do_you_work_for', 'how_old_are_you', 'cook_time']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(3.9807, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [9]
current labels : distance
pair class : ['distance']
mask: [False False False False False False False False False  True False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : damaged_card
pair class : ['damaged_card']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [9, 15]
['are_you_a_bot', 'food_last', 'distance', 'card_declined', 'routing', 'timezone', 'timer', 'bill_balance', 'damaged_card', 'distance', 'uber', 'change_ai_name', 'balance', 'goodbye', 'play_music', 'damaged_card']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.8314, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : jump_start
pair class : ['jump_start']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['pto_used', 'calendar', 'what_can_i_ask_you', 'credit_limit_change', 'restaurant_reviews', 'where_are_you_from', 'nutrition_info', 'tire_pressure', 'vaccines', 'book_hotel', 'travel_notification', 'flight_status', 'make_call', 'account_blocked', 'jump_start', 'jump_start']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.6399, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : alarm
pair class : ['alarm']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['income', 'meaning_of_life', 'travel_suggestion', 'recipe', 'no', 'do_you_have_pets', 'where_are_you_from', 'pto_used', 'maybe', 'gas', 'fun_fact', 'are_you_a_bot', 'reminder_update', 'bill_due', 'alarm', 'alarm']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.2901, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : change_accent
pair class : ['change_accent']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [6]
current labels : report_lost_card
pair class : ['report_lost_card']
mask: [False False False False False False  True False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [13]
current labels : oil_change_how
pair class : ['oil_change_how']
mask: [False False False False False False False False False False False False
 False  True False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [15, 6, 13]
['tire_pressure', 'reminder', 'todo_list', 'bill_due', 'change_accent', 'report_lost_card', 'report_lost_card', 'shopping_list_update', 'date', 'lost_luggage', 'greeting', 'oil_change_how', 'mpg', 'oil_change_how', 'redeem_rewards', 'change_accent']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(0.6725, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : improve_credit_score
pair class : ['improve_credit_score']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['repeat', 'measurement_conversion', 'pay_bill', 'payday', 'car_rental', 'accept_reservations', 'what_are_your_hobbies', 'how_old_are_you', 'spending_history', 'next_song', 'jump_start', 'improve_credit_score', 'yes', 'pto_balance', 'meeting_schedule', 'improve_credit_score']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(3.8536, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [13]
current labels : what_song
pair class : ['what_song']
mask: [False False False False False False False False False False False False
 False  True]
count: 14
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [13]
['time', 'translate', 'no', 'direct_deposit', 'pin_change', 'what_song', 'order_checks', 'greeting', 'restaurant_suggestion', 'bill_balance', 'oil_change_how', 'definition', 'whisper_mode', 'what_song']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([14, 14])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.2520, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : change_speed
pair class : ['change_speed']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['international_fees', 'change_speed', 'timer', 'todo_list_update', 'freeze_account', 'change_volume', 'cancel_reservation', 'meaning_of_life', 'pto_request', 'shopping_list', 'distance', 'current_location', 'bill_due', 'meeting_schedule', 'credit_limit_change', 'change_speed']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.1686, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [7]
current labels : weather
pair class : ['weather']
mask: [False False False False False False False  True False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : transfer
pair class : ['transfer']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [7, 15]
['travel_alert', 'weather', 'transfer', 'todo_list', 'restaurant_suggestion', 'apr', 'improve_credit_score', 'weather', 'flip_coin', 'flight_status', 'lost_luggage', 'balance', 'food_last', 'change_language', 'yes', 'transfer']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.4953, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [14]
current labels : recipe
pair class : ['recipe']
mask: [False False False False False False False False False False False False
 False False  True False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : report_fraud
pair class : ['report_fraud']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [9]
current labels : what_song
pair class : ['what_song']
mask: [False False False False False False False False False  True False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [13]
current labels : translate
pair class : ['translate']
mask: [False False False False False False False False False False False False
 False  True False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 4
pairs see from labels :  4
All skippings : [14, 15, 9, 13]
['recipe', 'directions', 'report_fraud', 'payday', 'pay_bill', 'make_call', 'balance', 'what_song', 'smart_home', 'what_song', 'translate', 'play_music', 'update_playlist', 'translate', 'recipe', 'report_fraud']
---------------------------------------------
concatenate got h_i : torch.Size([4, 768])
concatenate got h_j :  torch.Size([4, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([4, 1])
pos_sim.shape : torch.Size([4])
len(neg) : 4
loss_s_cl: tensor(0.5109, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [15]
current labels : direct_deposit
pair class : ['direct_deposit']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['pay_bill', 'fun_fact', 'international_fees', 'repeat', 'definition', 'user_name', 'tell_joke', 'roll_dice', 'expiration_date', 'what_is_your_name', 'direct_deposit', 'distance', 'calories', 'next_holiday', 'schedule_meeting', 'direct_deposit']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.0428, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [15]
current labels : meaning_of_life
pair class : ['meaning_of_life']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['whisper_mode', 'mpg', 'apr', 'bill_due', 'travel_alert', 'alarm', 'update_playlist', 'make_call', 'jump_start', 'spelling', 'meaning_of_life', 'user_name', 'change_language', 'travel_suggestion', 'freeze_account', 'meaning_of_life']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.5809, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : damaged_card
pair class : ['damaged_card']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['alarm', 'plug_type', 'replacement_card_duration', 'meal_suggestion', 'balance', 'reminder', 'where_are_you_from', 'pto_balance', 'new_card', 'pin_change', 'schedule_meeting', 'damaged_card', 'change_volume', 'find_phone', 'cancel_reservation', 'damaged_card']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.0896, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [15]
current labels : rollover_401k
pair class : ['rollover_401k']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['change_speed', 'book_hotel', 'roll_dice', 'date', 'jump_start', 'credit_limit', 'tire_change', 'rollover_401k', 'carry_on', 'accept_reservations', 'measurement_conversion', 'food_last', 'card_declined', 'travel_suggestion', 'pin_change', 'rollover_401k']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(3.8934, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [15]
current labels : spending_history
pair class : ['spending_history']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['improve_credit_score', 'fun_fact', 'redeem_rewards', 'greeting', 'transactions', 'spending_history', 'credit_limit', 'travel_suggestion', 'calendar', 'reminder', 'restaurant_suggestion', 'ingredient_substitution', 'rollover_401k', 'change_accent', 'oil_change_when', 'spending_history']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.6305, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [15]
current labels : credit_limit
pair class : ['credit_limit']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['change_volume', 'user_name', 'what_can_i_ask_you', 'credit_limit', 'shopping_list', 'how_old_are_you', 'who_do_you_work_for', 'bill_due', 'replacement_card_duration', 'calendar_update', 'shopping_list_update', 'min_payment', 'todo_list_update', 'change_language', 'international_visa', 'credit_limit']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.0117, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : ingredient_substitution
pair class : ['ingredient_substitution']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [12]
current labels : reminder_update
pair class : ['reminder_update']
mask: [False False False False False False False False False False False False
  True False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 12]
['oil_change_how', 'routing', 'calendar_update', 'calories', 'ingredient_substitution', 'flip_coin', 'plug_type', 'who_do_you_work_for', 'shopping_list_update', 'bill_due', 'book_flight', 'reminder_update', 'reminder_update', 'pay_bill', 'international_fees', 'ingredient_substitution']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.6448, grad_fn=<DivBackward0>)

[3,    10] loss_total: 41.253 loss_supervised_contrasive:  3.607 loss_intent :752.918 
skip_time: 0
total : 104
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [4]
current labels : exchange_rate
pair class : ['exchange_rate']
mask: [False False False False  True False False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [15]
current labels : pin_change
pair class : ['pin_change']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [4, 15]
['change_user_name', 'change_ai_name', 'exchange_rate', 'pto_request_status', 'exchange_rate', 'todo_list_update', 'pin_change', 'bill_balance', 'reset_settings', 'payday', 'reminder', 'who_made_you', 'uber', 'travel_alert', 'maybe', 'pin_change']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.8128, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [15]
current labels : travel_notification
pair class : ['travel_notification']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [12]
current labels : text
pair class : ['text']
mask: [False False False False False False False False False False False False
  True False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 12]
['ingredient_substitution', 'calendar_update', 'measurement_conversion', 'spelling', 'who_do_you_work_for', 'definition', 'change_language', 'travel_notification', 'time', 'card_declined', 'text', 'reminder', 'text', 'cancel', 'report_fraud', 'travel_notification']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.6891, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [15]
current labels : goodbye
pair class : ['goodbye']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [13]
current labels : nutrition_info
pair class : ['nutrition_info']
mask: [False False False False False False False False False False False False
 False  True False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [9]
current labels : confirm_reservation
pair class : ['confirm_reservation']
mask: [False False False False False False False False False  True False False
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [15, 13, 9]
['goodbye', 'car_rental', 'freeze_account', 'flight_status', 'nutrition_info', 'are_you_a_bot', 'expiration_date', 'meal_suggestion', 'confirm_reservation', 'confirm_reservation', 'meeting_schedule', 'date', 'restaurant_reservation', 'nutrition_info', 'what_song', 'goodbye']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.1028, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [15]
current labels : change_speed
pair class : ['change_speed']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['sync_device', 'book_flight', 'damaged_card', 'user_name', 'insurance_change', 'change_speed', 'apr', 'order_checks', 'reset_settings', 'restaurant_suggestion', 'new_card', 'confirm_reservation', 'where_are_you_from', 'w2', 'direct_deposit', 'change_speed']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(3.9095, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : payday
pair class : ['payday']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['application_status', 'roll_dice', 'todo_list', 'schedule_maintenance', 'travel_suggestion', 'interest_rate', 'flight_status', 'redeem_rewards', 'payday', 'how_old_are_you', 'translate', 'how_busy', 'book_hotel', 'traffic', 'thank_you', 'payday']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.6115, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [15]
current labels : whisper_mode
pair class : ['whisper_mode']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['international_fees', 'pto_used', 'nutrition_info', 'balance', 'next_song', 'bill_balance', 'whisper_mode', 'schedule_maintenance', 'car_rental', 'pay_bill', 'cook_time', 'tire_pressure', 'application_status', 'redeem_rewards', 'greeting', 'whisper_mode']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(6.3363, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : next_holiday
pair class : ['next_holiday']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [12]
current labels : what_is_your_name
pair class : ['what_is_your_name']
mask: [False False False False False False False False False False False False
  True False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [7]
current labels : lost_luggage
pair class : ['lost_luggage']
mask: [False False False False False False False  True False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [15, 12, 7]
['maybe', 'next_holiday', 'todo_list_update', 'tire_change', 'what_is_your_name', 'lost_luggage', 'sync_device', 'lost_luggage', 'insurance_change', 'pto_request', 'vaccines', 'shopping_list_update', 'what_is_your_name', 'calendar_update', 'card_declined', 'next_holiday']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.1809, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : credit_score
pair class : ['credit_score']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [8]
current labels : find_phone
pair class : ['find_phone']
mask: [False False False False False False False False  True False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [5]
current labels : timer
pair class : ['timer']
mask: [False False False False False  True False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [15, 8, 5]
['current_location', 'credit_score', 'order_status', 'find_phone', 'timer', 'timer', 'who_do_you_work_for', 'transactions', 'find_phone', 'pto_request', 'account_blocked', 'sync_device', 'cook_time', 'insurance', 'user_name', 'credit_score']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(0.9182, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : redeem_rewards
pair class : ['redeem_rewards']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['smart_home', 'alarm', 'change_speed', 'nutrition_info', 'income', 'mpg', 'rewards_balance', 'weather', 'traffic', 'apr', 'pay_bill', 'oil_change_when', 'redeem_rewards', 'ingredients_list', 'book_flight', 'redeem_rewards']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.9842, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : time
pair class : ['time']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['rewards_balance', 'international_fees', 'traffic', 'spending_history', 'report_lost_card', 'calculator', 'next_holiday', 'reminder_update', 'time', 'w2', 'what_are_your_hobbies', 'whisper_mode', 'smart_home', 'account_blocked', 'alarm', 'time']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(3.8627, grad_fn=<DivBackward0>)

[3,    20] loss_total: 40.784 loss_supervised_contrasive:  6.748 loss_intent :1505.773 
skip_time: 0
total : 114
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : replacement_card_duration
pair class : ['replacement_card_duration']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['smart_home', 'report_lost_card', 'gas', 'shopping_list', 'last_maintenance', 'sync_device', 'yes', 'balance', 'exchange_rate', 'reset_settings', 'improve_credit_score', 'measurement_conversion', 'flip_coin', 'time', 'replacement_card_duration', 'replacement_card_duration']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.5807, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [13]
current labels : do_you_have_pets
pair class : ['do_you_have_pets']
mask: [False False False False False False False False False False False False
 False  True False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : confirm_reservation
pair class : ['confirm_reservation']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [13, 15]
['fun_fact', 'account_blocked', 'w2', 'goodbye', 'change_volume', 'do_you_have_pets', 'schedule_meeting', 'how_old_are_you', 'roll_dice', 'cancel', 'tire_pressure', 'meaning_of_life', 'confirm_reservation', 'do_you_have_pets', 'gas_type', 'confirm_reservation']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.7449, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [11]
current labels : cancel
pair class : ['cancel']
mask: [False False False False False False False False False False False  True
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [15]
current labels : book_hotel
pair class : ['book_hotel']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [11, 15]
['calendar_update', 'change_volume', 'shopping_list_update', 'transactions', 'order_checks', 'cancel', 'recipe', 'pto_request_status', 'greeting', 'pto_used', 'book_hotel', 'cancel', 'new_card', 'restaurant_suggestion', 'tell_joke', 'book_hotel']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.3066, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [2]
current labels : share_location
pair class : ['share_location']
mask: [False False  True False False False False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [15]
current labels : change_user_name
pair class : ['change_user_name']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [2, 15]
['share_location', 'ingredients_list', 'share_location', 'damaged_card', 'meaning_of_life', 'schedule_maintenance', 'rollover_401k', 'change_user_name', 'timezone', 'carry_on', 'oil_change_how', 'thank_you', 'travel_notification', 'make_call', 'direct_deposit', 'change_user_name']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.5550, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [5]
current labels : spelling
pair class : ['spelling']
mask: [False False False False False  True False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [15]
current labels : restaurant_reviews
pair class : ['restaurant_reviews']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [5, 15]
['calendar', 'spelling', 'pto_balance', 'bill_balance', 'flip_coin', 'spelling', 'share_location', 'restaurant_reviews', 'interest_rate', 'directions', 'meaning_of_life', 'accept_reservations', 'sync_device', 'new_card', 'expiration_date', 'restaurant_reviews']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.4992, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [11]
current labels : credit_score
pair class : ['credit_score']
mask: [False False False False False False False False False False False  True
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : reminder_update
pair class : ['reminder_update']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [11, 15]
['international_visa', 'how_busy', 'application_status', 'yes', 'transfer', 'change_accent', 'accept_reservations', 'credit_score', 'are_you_a_bot', 'credit_limit_change', 'gas', 'credit_score', 'reminder_update', 'translate', 'expiration_date', 'reminder_update']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.4005, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [15]
current labels : tire_change
pair class : ['tire_change']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['order_status', 'travel_notification', 'find_phone', 'tire_change', 'oil_change_how', 'damaged_card', 'distance', 'income', 'maybe', 'meeting_schedule', 'restaurant_reservation', 'taxes', 'routing', 'play_music', 'definition', 'tire_change']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.6504, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [9]
current labels : todo_list
pair class : ['todo_list']
mask: [False False False False False False False False False  True False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [15]
current labels : flight_status
pair class : ['flight_status']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [9, 15]
['pin_change', 'todo_list', 'order_status', 'next_song', 'jump_start', 'gas_type', 'bill_balance', 'freeze_account', 'make_call', 'todo_list', 'flight_status', 'repeat', 'bill_due', 'oil_change_when', 'credit_limit_change', 'flight_status']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.5623, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [13]
current labels : car_rental
pair class : ['car_rental']
mask: [False False False False False False False False False False False False
 False  True False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : routing
pair class : ['routing']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [13, 15]
['transfer', 'text', 'car_rental', 'mpg', 'order_checks', 'who_do_you_work_for', 'find_phone', 'damaged_card', 'reminder', 'routing', 'tire_pressure', 'repeat', 'new_card', 'car_rental', 'food_last', 'routing']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.2951, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : mpg
pair class : ['mpg']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['direct_deposit', 'bill_balance', 'spending_history', 'greeting', 'traffic', 'application_status', 'what_is_your_name', 'what_are_your_hobbies', 'taxes', 'income', 'do_you_have_pets', 'tire_pressure', 'mpg', 'oil_change_how', 'min_payment', 'mpg']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(3.9969, grad_fn=<DivBackward0>)

[3,    30] loss_total: 40.202 loss_supervised_contrasive:  9.307 loss_intent :2258.636 
skip_time: 0
total : 124
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [15]
current labels : tell_joke
pair class : ['tell_joke']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['tire_change', 'no', 'carry_on', 'play_music', 'ingredient_substitution', 'tell_joke', 'change_language', 'make_call', 'repeat', 'thank_you', 'plug_type', 'uber', 'how_busy', 'directions', 'travel_notification', 'tell_joke']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.7868, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [15]
current labels : time
pair class : ['time']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['time', 'what_are_your_hobbies', 'calculator', 'update_playlist', 'text', 'alarm', 'exchange_rate', 'income', 'gas', 'interest_rate', 'distance', 'report_fraud', 'transactions', 'last_maintenance', 'credit_score', 'time']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.1507, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [8]
current labels : goodbye
pair class : ['goodbye']
mask: [False False False False False False False False  True False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [15]
current labels : report_fraud
pair class : ['report_fraud']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [8, 15]
['goodbye', 'calories', 'whisper_mode', 'report_fraud', 'change_user_name', 'book_flight', 'taxes', 'account_blocked', 'goodbye', 'reminder_update', 'insurance', 'oil_change_how', 'are_you_a_bot', 'time', 'cancel_reservation', 'report_fraud']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0910, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [15]
current labels : freeze_account
pair class : ['freeze_account']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['improve_credit_score', 'goodbye', 'insurance_change', 'reset_settings', 'next_song', 'book_hotel', 'freeze_account', 'schedule_meeting', 'exchange_rate', 'order_status', 'rewards_balance', 'lost_luggage', 'card_declined', 'nutrition_info', 'change_ai_name', 'freeze_account']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.1936, grad_fn=<DivBackward0>)

----
masking label debug : ['restaurant_reviews' 'restaurant_reviews']
current labels  restaurant_reviews
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 4
current skips : [ 7 15]
current labels : restaurant_reviews
pair class : ['restaurant_reviews' 'restaurant_reviews']
mask: [False False False False False False False  True False False False False
 False False False  True]
count: 16
numbers of pairs one label : 2
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [7, 15]
['pto_request_status', 'ingredients_list', 'min_payment', 'current_location', 'restaurant_reviews', 'where_are_you_from', 'calendar', 'restaurant_reviews', 'replacement_card_duration', 'rewards_balance', 'thank_you', 'next_holiday', 'cook_time', 'schedule_maintenance', 'travel_alert', 'restaurant_reviews']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.4113, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : how_old_are_you
pair class : ['how_old_are_you']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['schedule_maintenance', 'weather', 'timezone', 'credit_limit_change', 'w2', 'lost_luggage', 'calculator', 'pto_request', 'uber', 'insurance_change', 'oil_change_when', 'how_old_are_you', 'next_holiday', 'traffic', 'who_made_you', 'how_old_are_you']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.2659, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : distance
pair class : ['distance']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['no', 'pto_used', 'next_song', 'pin_change', 'food_last', 'change_accent', 'oil_change_when', 'text', 'jump_start', 'vaccines', 'change_user_name', 'plug_type', 'pto_request', 'where_are_you_from', 'distance', 'distance']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(3.9839, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [15]
current labels : pto_used
pair class : ['pto_used']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['fun_fact', 'interest_rate', 'min_payment', 'income', 'maybe', 'pto_used', 'report_fraud', 'replacement_card_duration', 'roll_dice', 'change_ai_name', 'book_flight', 'expiration_date', 'what_can_i_ask_you', 'flip_coin', 'credit_limit', 'pto_used']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.9409, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [15]
current labels : measurement_conversion
pair class : ['measurement_conversion']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [8]
current labels : timezone
pair class : ['timezone']
mask: [False False False False False False False False  True False False False
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 8]
['spending_history', 'play_music', 'cook_time', 'meal_suggestion', 'who_made_you', 'measurement_conversion', 'timezone', 'tell_joke', 'timezone', 'maybe', 'cancel_reservation', 'share_location', 'todo_list_update', 'current_location', 'meeting_schedule', 'measurement_conversion']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.5055, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : restaurant_reservation
pair class : ['restaurant_reservation']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [11]
current labels : gas_type
pair class : ['gas_type']
mask: [False False False False False False False False False False False  True
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 11]
['meal_suggestion', 'min_payment', 'restaurant_reservation', 'who_made_you', 'gas_type', 'credit_score', 'calories', 'cook_time', 'accept_reservations', 'reset_settings', 'no', 'gas_type', 'international_visa', 'date', 'vaccines', 'restaurant_reservation']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.5763, grad_fn=<DivBackward0>)

[3,    40] loss_total: 41.129 loss_supervised_contrasive:  12.797 loss_intent :3011.409 
skip_time: 0
total : 134
----
masking label debug : ['what_can_i_ask_you' 'what_can_i_ask_you']
current labels  what_can_i_ask_you
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 1
current skips : [ 3 15]
current labels : what_can_i_ask_you
pair class : ['what_can_i_ask_you' 'what_can_i_ask_you']
mask: [False False False  True False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 2
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [3, 15]
['calories', 'what_can_i_ask_you', 'plug_type', 'what_can_i_ask_you', 'calendar', 'weather', 'are_you_a_bot', 'rollover_401k', 'play_music', 'vaccines', 'change_user_name', 'smart_home', 'direct_deposit', 'transactions', 'change_speed', 'what_can_i_ask_you']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.7233, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [15]
current labels : insurance
pair class : ['insurance']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['translate', 'shopping_list', 'rollover_401k', 'yes', 'spending_history', 'insurance', 'timer', 'restaurant_reservation', 'change_accent', 'transfer', 'report_lost_card', 'pto_balance', 'restaurant_reviews', 'taxes', 'apr', 'insurance']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.8380, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : what_is_your_name
pair class : ['what_is_your_name']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['improve_credit_score', 'uber', 'interest_rate', 'thank_you', 'fun_fact', 'gas', 'travel_alert', 'book_hotel', 'what_are_your_hobbies', 'do_you_have_pets', 'where_are_you_from', 'what_is_your_name', 'greeting', 'account_blocked', 'definition', 'what_is_your_name']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.1166, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [15]
current labels : gas
pair class : ['gas']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [9]
current labels : last_maintenance
pair class : ['last_maintenance']
mask: [False False False False False False False False False  True False False
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 9]
['transfer', 'calculator', 'ingredients_list', 'gas', 'ingredient_substitution', 'last_maintenance', 'application_status', 'gas_type', 'how_busy', 'last_maintenance', 'international_visa', 'recipe', 'what_can_i_ask_you', 'rewards_balance', 'pto_request_status', 'gas']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.8834, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [15]
current labels : measurement_conversion
pair class : ['measurement_conversion']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [6]
current labels : directions
pair class : ['directions']
mask: [False False False False False False  True False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 6]
['measurement_conversion', 'pto_balance', 'change_accent', 'how_busy', 'directions', 'insurance', 'directions', 'current_location', 'update_playlist', 'restaurant_reservation', 'uber', 'change_ai_name', 'accept_reservations', 'travel_suggestion', 'report_lost_card', 'measurement_conversion']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.7196, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [10]
current labels : order
pair class : ['order']
mask: [False False False False False False False False False False  True False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : tire_change
pair class : ['tire_change']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [10, 15]
['calendar', 'timer', 'schedule_meeting', 'order_status', 'do_you_have_pets', 'mpg', 'taxes', 'credit_limit', 'order', 'tire_change', 'order', 'vaccines', 'cancel', 'carry_on', 'flight_status', 'tire_change']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.8258, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [13]
current labels : travel_notification
pair class : ['travel_notification']
mask: [False False False False False False False False False False False False
 False  True]
count: 14
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [13]
['pto_used', 'order_checks', 'carry_on', 'no', 'todo_list', 'payday', 'what_song', 'shopping_list_update', 'change_ai_name', 'pto_request_status', 'ingredients_list', 'who_made_you', 'travel_notification', 'travel_notification']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([14, 14])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(3.8892, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : ingredients_list
pair class : ['ingredients_list']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['car_rental', 'uber', 'gas_type', 'credit_score', 'ingredients_list', 'definition', 'how_old_are_you', 'improve_credit_score', 'goodbye', 'calendar', 'pto_request', 'schedule_meeting', 'transfer', 'insurance_change', 'play_music', 'ingredients_list']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(3.9569, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : timer
pair class : ['timer']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['change_speed', 'traffic', 'make_call', 'weather', 'next_holiday', 'recipe', 'pto_used', 'accept_reservations', 'how_old_are_you', 'last_maintenance', 'find_phone', 'timer', 'schedule_meeting', 'cook_time', 'update_playlist', 'timer']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.6604, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [15]
current labels : time
pair class : ['time']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['jump_start', 'smart_home', 'improve_credit_score', 'shopping_list', 'reminder', 'schedule_maintenance', 'time', 'book_flight', 'exchange_rate', 'play_music', 'meeting_schedule', 'text', 'order', 'change_ai_name', 'insurance', 'time']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.8254, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [3]
current labels : pto_request_status
pair class : ['pto_request_status']
mask: [False False False  True False False False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : calendar
pair class : ['calendar']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [3, 15]
['order_checks', 'pto_request_status', 'restaurant_reservation', 'pto_request_status', 'todo_list_update', 'date', 'roll_dice', 'what_song', 'current_location', 'nutrition_info', 'sync_device', 'alarm', 'balance', 'report_lost_card', 'calendar', 'calendar']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.7688, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : insurance_change
pair class : ['insurance_change']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['international_fees', 'restaurant_reviews', 'do_you_have_pets', 'credit_limit_change', 'card_declined', 'cancel', 'jump_start', 'tell_joke', 'rollover_401k', 'meaning_of_life', 'flip_coin', 'insurance_change', 'text', 'interest_rate', 'ingredient_substitution', 'insurance_change']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.4769, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : card_declined
pair class : ['card_declined']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [12]
current labels : what_are_your_hobbies
pair class : ['what_are_your_hobbies']
mask: [False False False False False False False False False False False False
  True False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 12]
['travel_notification', 'expiration_date', 'card_declined', 'order_status', 'what_can_i_ask_you', 'change_ai_name', 'date', 'calories', 'how_busy', 'what_are_your_hobbies', 'reset_settings', 'shopping_list', 'what_are_your_hobbies', 'tire_pressure', 'vaccines', 'card_declined']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.2637, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [15]
current labels : sync_device
pair class : ['sync_device']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['what_can_i_ask_you', 'oil_change_how', 'pto_balance', 'spending_history', 'pto_used', 'gas_type', 'sync_device', 'restaurant_suggestion', 'taxes', 'account_blocked', 'find_phone', 'change_ai_name', 'income', 'shopping_list_update', 'insurance_change', 'sync_device']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.0190, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [11]
current labels : distance
pair class : ['distance']
mask: [False False False False False False False False False False False  True
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : shopping_list
pair class : ['shopping_list']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [11, 15]
['last_maintenance', 'food_last', 'translate', 'what_song', 'cook_time', 'goodbye', 'find_phone', 'interest_rate', 'ingredients_list', 'distance', 'time', 'distance', 'car_rental', 'where_are_you_from', 'shopping_list', 'shopping_list']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.3574, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : income
pair class : ['income']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['travel_alert', 'restaurant_reviews', 'fun_fact', 'travel_suggestion', 'confirm_reservation', 'pin_change', 'transfer', 'smart_home', 'account_blocked', 'international_visa', 'pto_used', 'income', 'damaged_card', 'change_accent', 'exchange_rate', 'income']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.9786, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [15]
current labels : confirm_reservation
pair class : ['confirm_reservation']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['play_music', 'order_checks', 'oil_change_how', 'calendar', 'w2', 'user_name', 'confirm_reservation', 'smart_home', 'redeem_rewards', 'mpg', 'definition', 'change_accent', 'credit_limit', 'rollover_401k', 'schedule_maintenance', 'confirm_reservation']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(3.9301, grad_fn=<DivBackward0>)

[4,    10] loss_total: 41.360 loss_supervised_contrasive:  3.724 loss_intent :752.729 
skip_time: 0
total : 151
----
masking label debug : ['user_name' 'user_name']
current labels  user_name
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 1
current skips : [ 4 15]
current labels : user_name
pair class : ['user_name' 'user_name']
mask: [False False False False  True False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 2
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [9]
current labels : meal_suggestion
pair class : ['meal_suggestion']
mask: [False False False False False False False False False  True False False
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [4, 15, 9]
['gas', 'user_name', 'traffic', 'maybe', 'user_name', 'lost_luggage', 'exchange_rate', 'w2', 'meal_suggestion', 'meal_suggestion', 'credit_limit_change', 'calculator', 'ingredient_substitution', 'reset_settings', 'meaning_of_life', 'user_name']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.0400, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [13]
current labels : make_call
pair class : ['make_call']
mask: [False False False False False False False False False False False False
 False  True False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : smart_home
pair class : ['smart_home']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [13, 15]
['make_call', 'smart_home', 'greeting', 'vaccines', 'who_do_you_work_for', 'change_language', 'meal_suggestion', 'do_you_have_pets', 'goodbye', 'where_are_you_from', 'what_are_your_hobbies', 'reminder_update', 'meeting_schedule', 'make_call', 'last_maintenance', 'smart_home']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.3879, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [11]
current labels : report_fraud
pair class : ['report_fraud']
mask: [False False False False False False False False False False False  True
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : international_fees
pair class : ['international_fees']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [11, 15]
['current_location', 'todo_list', 'change_volume', 'jump_start', 'income', 'book_flight', 'report_fraud', 'change_accent', 'international_fees', 'ingredient_substitution', 'tire_pressure', 'report_fraud', 'are_you_a_bot', 'meeting_schedule', 'time', 'international_fees']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.4677, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : shopping_list_update
pair class : ['shopping_list_update']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [6]
current labels : accept_reservations
pair class : ['accept_reservations']
mask: [False False False False False False  True False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [10]
current labels : oil_change_when
pair class : ['oil_change_when']
mask: [False False False False False False False False False False  True False
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [15, 6, 10]
['next_song', 'shopping_list_update', 'lost_luggage', 'accept_reservations', 'oil_change_when', 'travel_notification', 'accept_reservations', 'flight_status', 'report_lost_card', 'how_busy', 'oil_change_when', 'text', 'next_holiday', 'measurement_conversion', 'whisper_mode', 'shopping_list_update']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(0.8873, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : redeem_rewards
pair class : ['redeem_rewards']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [11]
current labels : tire_pressure
pair class : ['tire_pressure']
mask: [False False False False False False False False False False False  True
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 11]
['pto_request_status', 'min_payment', 'translate', 'carry_on', 'redeem_rewards', 'tire_pressure', 'yes', 'no', 'routing', 'credit_limit', 'change_volume', 'tire_pressure', 'calendar_update', 'maybe', 'insurance', 'redeem_rewards']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.5748, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : credit_limit_change
pair class : ['credit_limit_change']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [10]
current labels : what_is_your_name
pair class : ['what_is_your_name']
mask: [False False False False False False False False False False  True False
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 10]
['account_blocked', 'min_payment', 'spending_history', 'travel_alert', 'credit_limit_change', 'direct_deposit', 'calculator', 'plug_type', 'what_is_your_name', 'directions', 'what_is_your_name', 'calories', 'shopping_list_update', 'mpg', 'transfer', 'credit_limit_change']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.8397, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [15]
current labels : are_you_a_bot
pair class : ['are_you_a_bot']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['where_are_you_from', 'exchange_rate', 'redeem_rewards', 'balance', 'taxes', 'traffic', 'timer', 'pto_used', 'reminder_update', 'translate', 'are_you_a_bot', 'book_hotel', 'restaurant_reviews', 'report_fraud', 'rewards_balance', 'are_you_a_bot']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.0564, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [15]
current labels : gas
pair class : ['gas']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['balance', 'next_holiday', 'food_last', 'gas', 'transactions', 'redeem_rewards', 'rollover_401k', 'reminder_update', 'application_status', 'replacement_card_duration', 'report_lost_card', 'distance', 'order', 'uber', 'timezone', 'gas']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4327, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : restaurant_reviews
pair class : ['restaurant_reviews']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['new_card', 'spelling', 'find_phone', 'fun_fact', 'restaurant_suggestion', 'international_visa', 'reminder_update', 'pin_change', 'bill_balance', 'do_you_have_pets', 'share_location', 'credit_limit', 'ingredients_list', 'pto_request', 'restaurant_reviews', 'restaurant_reviews']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.1680, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [11]
current labels : pto_balance
pair class : ['pto_balance']
mask: [False False False False False False False False False False False  True
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 13
current skips : [15]
current labels : smart_home
pair class : ['smart_home']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [11, 15]
['distance', 'pto_balance', 'who_made_you', 'gas', 'shopping_list', 'uber', 'flip_coin', 'order_status', 'meaning_of_life', 'cancel', 'card_declined', 'pto_balance', 'update_playlist', 'smart_home', 'text', 'smart_home']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0788, grad_fn=<DivBackward0>)

[4,    20] loss_total: 40.225 loss_supervised_contrasive:  6.317 loss_intent :1505.363 
skip_time: 0
total : 161
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [14]
current labels : flip_coin
pair class : ['flip_coin']
mask: [False False False False False False False False False False False False
 False False  True False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 13
current skips : [15]
current labels : sync_device
pair class : ['sync_device']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [14, 15]
['no', 'insurance', 'taxes', 'cancel_reservation', 'change_volume', 'flip_coin', 'pay_bill', 'spending_history', 'ingredient_substitution', 'freeze_account', 'cancel', 'are_you_a_bot', 'confirm_reservation', 'sync_device', 'flip_coin', 'sync_device']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.3673, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [6]
current labels : current_location
pair class : ['current_location']
mask: [False False False False False False  True False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
----
masking label debug : ['what_is_your_name' 'what_is_your_name']
current labels  what_is_your_name
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 2
current skips : [ 3 15]
current labels : what_is_your_name
pair class : ['what_is_your_name' 'what_is_your_name']
mask: [False False False  True False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 2
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [6, 3, 15]
['current_location', 'definition', 'what_is_your_name', 'what_is_your_name', 'who_made_you', 'schedule_meeting', 'current_location', 'calendar', 'repeat', 'mpg', 'food_last', 'goodbye', 'carry_on', 'tire_change', 'new_card', 'what_is_your_name']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(0.9767, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [12]
current labels : schedule_maintenance
pair class : ['schedule_maintenance']
mask: [False False False False False False False False False False False False
  True False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : change_language
pair class : ['change_language']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [12, 15]
['replacement_card_duration', 'report_lost_card', 'schedule_maintenance', 'thank_you', 'change_user_name', 'travel_alert', 'repeat', 'pto_request', 'book_hotel', 'change_language', 'direct_deposit', 'restaurant_reviews', 'schedule_maintenance', 'freeze_account', 'recipe', 'change_language']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.5181, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [2]
current labels : restaurant_reservation
pair class : ['restaurant_reservation']
mask: [False False  True False False False False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [10]
current labels : interest_rate
pair class : ['interest_rate']
mask: [False False False False False False False False False False  True False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [15]
current labels : cook_time
pair class : ['cook_time']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [2, 10, 15]
['restaurant_reservation', 'interest_rate', 'restaurant_reservation', 'what_song', 'travel_suggestion', 'improve_credit_score', 'cook_time', 'credit_score', 'travel_notification', 'yes', 'interest_rate', 'vaccines', 'goodbye', 'reset_settings', 'oil_change_how', 'cook_time']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(0.7802, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [7]
current labels : travel_suggestion
pair class : ['travel_suggestion']
mask: [False False False False False False False  True False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [15]
current labels : distance
pair class : ['distance']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [9]
current labels : directions
pair class : ['directions']
mask: [False False False False False False False False False  True False False
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [7, 15, 9]
['travel_suggestion', 'insurance_change', 'who_do_you_work_for', 'distance', 'tire_change', 'cancel_reservation', 'directions', 'travel_suggestion', 'account_blocked', 'directions', 'change_speed', 'schedule_meeting', 'rollover_401k', 'spending_history', 'restaurant_reservation', 'distance']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(0.7498, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [15]
current labels : where_are_you_from
pair class : ['where_are_you_from']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['where_are_you_from', 'roll_dice', 'gas_type', 'change_language', 'uber', 'payday', 'exchange_rate', 'calendar_update', 'date', 'ingredients_list', 'repeat', 'tell_joke', 'order_checks', 'translate', 'play_music', 'where_are_you_from']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.9220, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [15]
current labels : update_playlist
pair class : ['update_playlist']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['international_fees', 'lost_luggage', 'international_visa', 'change_ai_name', 'meeting_schedule', 'update_playlist', 'pin_change', 'book_flight', 'fun_fact', 'date', 'pay_bill', 'cancel_reservation', 'improve_credit_score', 'transfer', 'car_rental', 'update_playlist']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.3255, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : restaurant_reservation
pair class : ['restaurant_reservation']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [14]
current labels : card_declined
pair class : ['card_declined']
mask: [False False False False False False False False False False False False
 False False  True False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 14]
['alarm', 'tire_change', 'carry_on', 'cancel', 'restaurant_reservation', 'card_declined', 'expiration_date', 'definition', 'oil_change_how', 'who_made_you', 'damaged_card', 'directions', 'credit_limit', 'no', 'card_declined', 'restaurant_reservation']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.6420, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : taxes
pair class : ['taxes']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [14]
current labels : todo_list
pair class : ['todo_list']
mask: [False False False False False False False False False False False False
 False False  True False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 14]
['travel_notification', 'damaged_card', 'report_fraud', 'yes', 'taxes', 'todo_list', 'make_call', 'w2', 'order', 'update_playlist', 'greeting', 'bill_due', 'cook_time', 'calories', 'todo_list', 'taxes']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.5399, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [12]
current labels : application_status
pair class : ['application_status']
mask: [False False False False False False False False False False False False
  True False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [15]
current labels : maybe
pair class : ['maybe']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [8]
current labels : oil_change_when
pair class : ['oil_change_when']
mask: [False False False False False False False False  True False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [14]
current labels : pay_bill
pair class : ['pay_bill']
mask: [False False False False False False False False False False False False
 False False  True False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 4
pairs see from labels :  4
All skippings : [12, 15, 8, 14]
['change_speed', 'bill_balance', 'application_status', 'meeting_schedule', 'replacement_card_duration', 'maybe', 'oil_change_when', 'update_playlist', 'oil_change_when', 'share_location', 'calculator', 'pay_bill', 'application_status', 'alarm', 'pay_bill', 'maybe']
---------------------------------------------
concatenate got h_i : torch.Size([4, 768])
concatenate got h_j :  torch.Size([4, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([4, 1])
pos_sim.shape : torch.Size([4])
len(neg) : 4
loss_s_cl: tensor(0.6164, grad_fn=<DivBackward0>)

[4,    30] loss_total: 39.577 loss_supervised_contrasive:  8.261 loss_intent :2258.024 
skip_time: 0
total : 171
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : book_hotel
pair class : ['book_hotel']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [14]
current labels : what_can_i_ask_you
pair class : ['what_can_i_ask_you']
mask: [False False False False False False False False False False False False
 False False  True False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 14]
['measurement_conversion', 'play_music', 'book_hotel', 'flight_status', 'what_can_i_ask_you', 'reset_settings', 'roll_dice', 'credit_score', 'last_maintenance', 'timezone', 'rewards_balance', 'mpg', 'no', 'min_payment', 'what_can_i_ask_you', 'book_hotel']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.7205, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : vaccines
pair class : ['vaccines']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['how_old_are_you', 'credit_limit_change', 'measurement_conversion', 'greeting', 'insurance', 'calories', 'payday', 'jump_start', 'food_last', 'change_speed', 'recipe', 'taxes', 'pto_balance', 'reminder', 'vaccines', 'vaccines']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.0589, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : what_are_your_hobbies
pair class : ['what_are_your_hobbies']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['reminder', 'weather', 'what_are_your_hobbies', 'replacement_card_duration', 'bill_balance', 'routing', 'recipe', 'change_volume', 'what_can_i_ask_you', 'whisper_mode', 'carry_on', 'nutrition_info', 'cancel', 'who_made_you', 'translate', 'what_are_your_hobbies']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.0346, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [15]
current labels : freeze_account
pair class : ['freeze_account']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['freeze_account', 'credit_limit_change', 'definition', 'international_visa', 'income', 'calendar_update', 'apr', 'thank_you', 'payday', 'gas', 'measurement_conversion', 'do_you_have_pets', 'todo_list', 'report_lost_card', 'sync_device', 'freeze_account']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(3.9490, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [15]
current labels : calendar_update
pair class : ['calendar_update']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['shopping_list', 'report_fraud', 'repeat', 'user_name', 'weather', 'spelling', 'share_location', 'calendar_update', 'order', 'next_song', 'pto_request', 'flight_status', 'plug_type', 'whisper_mode', 'interest_rate', 'calendar_update']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.0506, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : new_card
pair class : ['new_card']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [10]
current labels : greeting
pair class : ['greeting']
mask: [False False False False False False False False False False  True False
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 10]
['timer', 'new_card', 'bill_due', 'accept_reservations', 'damaged_card', 'make_call', 'are_you_a_bot', 'greeting', 'freeze_account', 'change_speed', 'greeting', 'fun_fact', 'rewards_balance', 'tell_joke', 'schedule_meeting', 'new_card']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.8822, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : uber
pair class : ['uber']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['tire_change', 'next_holiday', 'car_rental', 'pay_bill', 'uber', 'routing', 'order_status', 'traffic', 'no', 'weather', 'who_do_you_work_for', 'directions', 'timer', 'international_fees', 'confirm_reservation', 'uber']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.3664, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [15]
current labels : nutrition_info
pair class : ['nutrition_info']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['restaurant_suggestion', 'international_fees', 'calendar', 'nutrition_info', 'pin_change', 'carry_on', 'order_checks', 'alarm', 'payday', 'time', 'gas', 'shopping_list_update', 'balance', 'jump_start', 'apr', 'nutrition_info']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.2502, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : flip_coin
pair class : ['flip_coin']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['do_you_have_pets', 'application_status', 'change_accent', 'what_is_your_name', 'flip_coin', 'international_visa', 'todo_list', 'transactions', 'gas_type', 'maybe', 'how_busy', 'apr', 'cancel_reservation', 'timezone', 'what_song', 'flip_coin']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.0080, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : rollover_401k
pair class : ['rollover_401k']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['bill_balance', 'pto_request', 'rollover_401k', 'last_maintenance', 'oil_change_how', 'order_status', 'share_location', 'tell_joke', 'whisper_mode', 'thank_you', 'book_hotel', 'redeem_rewards', 'application_status', 'change_volume', 'nutrition_info', 'rollover_401k']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.4358, grad_fn=<DivBackward0>)

[4,    40] loss_total: 41.306 loss_supervised_contrasive:  11.936 loss_intent :3010.623 
skip_time: 0
total : 181
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 13
current skips : [15]
current labels : confirm_reservation
pair class : ['confirm_reservation']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['text', 'food_last', 'min_payment', 'bill_due', 'flight_status', 'sync_device', 'account_blocked', 'timezone', 'order', 'roll_dice', 'pto_balance', 'measurement_conversion', 'meaning_of_life', 'confirm_reservation', 'reminder_update', 'confirm_reservation']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(3.9172, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [15]
current labels : transactions
pair class : ['transactions']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['order_status', 'calendar_update', 'todo_list_update', 'cancel_reservation', 'new_card', 'calories', 'roll_dice', 'routing', 'nutrition_info', 'change_ai_name', 'transactions', 'next_song', 'share_location', 'travel_notification', 'who_made_you', 'transactions']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.2404, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : cook_time
pair class : ['cook_time']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['yes', 'transactions', 'travel_suggestion', 'pto_used', 'what_are_your_hobbies', 'spending_history', 'w2', 'timezone', 'cook_time', 'reminder', 'thank_you', 'change_user_name', 'book_hotel', 'order_checks', 'maybe', 'cook_time']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(3.9020, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [15]
current labels : whisper_mode
pair class : ['whisper_mode']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['weather', 'fun_fact', 'insurance_change', 'are_you_a_bot', 'how_busy', 'current_location', 'whisper_mode', 'pto_request_status', 'lost_luggage', 'direct_deposit', 'time', 'shopping_list_update', 'replacement_card_duration', 'payday', 'date', 'whisper_mode']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.0219, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : travel_alert
pair class : ['travel_alert']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['pto_request_status', 'lost_luggage', 'meal_suggestion', 'find_phone', 'bill_balance', 'restaurant_suggestion', 'accept_reservations', 'how_busy', 'travel_alert', 'traffic', 'plug_type', 'income', 'spelling', 'bill_due', 'who_do_you_work_for', 'travel_alert']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.8452, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : vaccines
pair class : ['vaccines']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [10]
current labels : change_user_name
pair class : ['change_user_name']
mask: [False False False False False False False False False False  True False
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 10]
['car_rental', 'rewards_balance', 'improve_credit_score', 'apr', 'vaccines', 'book_flight', 'change_user_name', 'freeze_account', 'reminder', 'thank_you', 'change_user_name', 'next_song', 'direct_deposit', 'w2', 'spelling', 'vaccines']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.3373, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [13]
current labels : how_old_are_you
pair class : ['how_old_are_you']
mask: [False False False False False False False False False False False False
 False  True]
count: 14
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [13]
['calculator', 'routing', 'change_user_name', 'timer', 'how_old_are_you', 'plug_type', 'insurance', 'direct_deposit', 'book_flight', 'tire_pressure', 'pin_change', 'where_are_you_from', 'spelling', 'how_old_are_you']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([14, 14])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.8769, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : maybe
pair class : ['maybe']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['definition', 'balance', 'account_blocked', 'schedule_meeting', 'repeat', 'jump_start', 'calculator', 'transactions', 'last_maintenance', 'reset_settings', 'accept_reservations', 'change_language', 'goodbye', 'greeting', 'maybe', 'maybe']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(3.9685, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [6]
current labels : change_accent
pair class : ['change_accent']
mask: [False False False False False False  True False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [15]
current labels : play_music
pair class : ['play_music']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [6, 15]
['rollover_401k', 'schedule_maintenance', 'gas', 'change_accent', 'next_song', 'play_music', 'change_accent', 'taxes', 'traffic', 'update_playlist', 'cancel', 'report_lost_card', 'how_old_are_you', 'timer', 'smart_home', 'play_music']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.5284, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : interest_rate
pair class : ['interest_rate']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['tell_joke', 'do_you_have_pets', 'flight_status', 'ingredients_list', 'interest_rate', 'roll_dice', 'reminder', 'text', 'order_checks', 'pto_used', 'traffic', 'recipe', 'what_is_your_name', 'greeting', 'calendar', 'interest_rate']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.3955, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [15]
current labels : payday
pair class : ['payday']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['damaged_card', 'reminder_update', 'change_speed', 'cancel_reservation', 'goodbye', 'restaurant_suggestion', 'payday', 'apr', 'meaning_of_life', 'whisper_mode', 'what_are_your_hobbies', 'current_location', 'restaurant_reviews', 'food_last', 'meeting_schedule', 'payday']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(6.0323, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [15]
current labels : text
pair class : ['text']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['reminder', 'shopping_list_update', 'restaurant_reservation', 'international_visa', 'replacement_card_duration', 'freeze_account', 'goodbye', 'text', 'pay_bill', 'flip_coin', 'credit_limit_change', 'income', 'taxes', 'rewards_balance', 'who_do_you_work_for', 'text']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.8912, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [5]
current labels : spelling
pair class : ['spelling']
mask: [False False False False False  True False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [14]
current labels : gas_type
pair class : ['gas_type']
mask: [False False False False False False False False False False False False
 False False  True False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : reminder_update
pair class : ['reminder_update']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [5, 14, 15]
['spelling', 'card_declined', 'share_location', 'reset_settings', 'international_visa', 'spelling', 'balance', 'repeat', 'confirm_reservation', 'gas_type', 'accept_reservations', 'reminder_update', 'how_old_are_you', 'taxes', 'gas_type', 'reminder_update']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(0.7198, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : rollover_401k
pair class : ['rollover_401k']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['direct_deposit', 'ingredient_substitution', 'sync_device', 'pto_used', 'rollover_401k', 'change_speed', 'min_payment', 'where_are_you_from', 'what_are_your_hobbies', 'alarm', 'insurance', 'maybe', 'todo_list', 'apr', 'schedule_meeting', 'rollover_401k']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.0396, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [15]
current labels : bill_due
pair class : ['bill_due']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['bill_due', 'todo_list_update', 'spending_history', 'spelling', 'how_busy', 'shopping_list', 'credit_limit', 'credit_score', 'next_holiday', 'what_is_your_name', 'pto_request', 'uber', 'international_visa', 'routing', 'who_do_you_work_for', 'bill_due']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.6661, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : who_made_you
pair class : ['who_made_you']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [11]
current labels : transactions
pair class : ['transactions']
mask: [False False False False False False False False False False False  True
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 11]
['reminder', 'redeem_rewards', 'who_made_you', 'order_checks', 'exchange_rate', 'ingredient_substitution', 'reset_settings', 'pto_balance', 'order_status', 'pto_used', 'transactions', 'transactions', 'what_is_your_name', 'todo_list_update', 'shopping_list_update', 'who_made_you']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0718, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [14]
current labels : change_volume
pair class : ['change_volume']
mask: [False False False False False False False False False False False False
 False False  True False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 13
current skips : [15]
current labels : plug_type
pair class : ['plug_type']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [14, 15]
['lost_luggage', 'calendar', 'calories', 'mpg', 'translate', 'payday', 'income', 'book_flight', 'where_are_you_from', 'account_blocked', 'change_volume', 'meal_suggestion', 'play_music', 'plug_type', 'change_volume', 'plug_type']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.4219, grad_fn=<DivBackward0>)

[5,    10] loss_total: 41.102 loss_supervised_contrasive:  3.474 loss_intent :752.574 
skip_time: 0
total : 198
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : change_language
pair class : ['change_language']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['user_name', 'taxes', 'rewards_balance', 'calories', 'pay_bill', 'new_card', 'directions', 'travel_alert', 'oil_change_when', 'order_checks', 'credit_limit', 'change_language', 'book_flight', 'pto_request', 'roll_dice', 'change_language']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.3568, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [6]
current labels : are_you_a_bot
pair class : ['are_you_a_bot']
mask: [False False False False False False  True False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [15]
current labels : improve_credit_score
pair class : ['improve_credit_score']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [6, 15]
['are_you_a_bot', 'freeze_account', 'food_last', 'change_speed', 'todo_list', 'improve_credit_score', 'are_you_a_bot', 'credit_score', 'transfer', 'cancel', 'make_call', 'shopping_list_update', 'tire_pressure', 'bill_due', 'tire_change', 'improve_credit_score']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.4066, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [15]
current labels : expiration_date
pair class : ['expiration_date']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [12]
current labels : goodbye
pair class : ['goodbye']
mask: [False False False False False False False False False False False False
  True False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 12]
['expiration_date', 'income', 'meaning_of_life', 'international_fees', 'distance', 'restaurant_reviews', 'meeting_schedule', 'goodbye', 'card_declined', 'time', 'tell_joke', 'maybe', 'goodbye', 'mpg', 'transfer', 'expiration_date']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.7052, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : nutrition_info
pair class : ['nutrition_info']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [8]
current labels : change_ai_name
pair class : ['change_ai_name']
mask: [False False False False False False False False  True False False False
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 8]
['whisper_mode', 'credit_limit', 'nutrition_info', 'insurance', 'calendar_update', 'replacement_card_duration', 'order', 'change_ai_name', 'change_ai_name', 'international_visa', 'international_fees', 'smart_home', 'jump_start', 'order_status', 'application_status', 'nutrition_info']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.5556, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : insurance_change
pair class : ['insurance_change']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['thank_you', 'bill_balance', 'jump_start', 'meaning_of_life', 'insurance_change', 'change_accent', 'recipe', 'time', 'flight_status', 'where_are_you_from', 'measurement_conversion', 'smart_home', 'travel_suggestion', 'play_music', 'freeze_account', 'insurance_change']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.9661, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [15]
current labels : recipe
pair class : ['recipe']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [8]
current labels : last_maintenance
pair class : ['last_maintenance']
mask: [False False False False False False False False  True False False False
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 8]
['recipe', 'last_maintenance', 'alarm', 'meal_suggestion', 'bill_due', 'fun_fact', 'ingredients_list', 'account_blocked', 'last_maintenance', 'accept_reservations', 'restaurant_reservation', 'play_music', 'what_are_your_hobbies', 'date', 'ingredient_substitution', 'recipe']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.6034, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : text
pair class : ['text']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [14]
current labels : tell_joke
pair class : ['tell_joke']
mask: [False False False False False False False False False False False False
 False False  True False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 14]
['restaurant_reservation', 'find_phone', 'direct_deposit', 'yes', 'text', 'flip_coin', 'meal_suggestion', 'roll_dice', 'pto_request_status', 'how_old_are_you', 'tell_joke', 'credit_limit', 'freeze_account', 'next_holiday', 'tell_joke', 'text']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.7005, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : flight_status
pair class : ['flight_status']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['confirm_reservation', 'credit_limit', 'flight_status', 'uber', 'book_hotel', 'direct_deposit', 'what_is_your_name', 'change_ai_name', 'oil_change_how', 'schedule_meeting', 'interest_rate', 'calendar_update', 'rewards_balance', 'transactions', 'oil_change_when', 'flight_status']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.6147, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : timezone
pair class : ['timezone']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['schedule_maintenance', 'measurement_conversion', 'no', 'balance', 'plug_type', 'reset_settings', 'how_busy', 'share_location', 'credit_limit_change', 'report_fraud', 'apr', 'timezone', 'insurance', 'change_language', 'uber', 'timezone']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.0204, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [13]
current labels : pay_bill
pair class : ['pay_bill']
mask: [False False False False False False False False False False False False
 False  True False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [15]
current labels : calculator
pair class : ['calculator']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [13, 15]
['todo_list', 'restaurant_reviews', 'book_hotel', 'order_status', 'food_last', 'pay_bill', 'insurance_change', 'calculator', 'pto_balance', 'plug_type', 'travel_suggestion', 'lost_luggage', 'rollover_401k', 'pay_bill', 'w2', 'calculator']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.8426, grad_fn=<DivBackward0>)

[5,    20] loss_total: 40.505 loss_supervised_contrasive:  6.351 loss_intent :1505.121 
skip_time: 0
total : 208
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : measurement_conversion
pair class : ['measurement_conversion']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['bill_balance', 'translate', 'redeem_rewards', 'shopping_list_update', 'who_do_you_work_for', 'share_location', 'mpg', 'what_is_your_name', 'exchange_rate', 'measurement_conversion', 'calendar', 'greeting', 'user_name', 'change_ai_name', 'ingredient_substitution', 'measurement_conversion']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.6329, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [8]
current labels : what_song
pair class : ['what_song']
mask: [False False False False False False False False  True False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [15]
current labels : how_busy
pair class : ['how_busy']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [8, 15]
['repeat', 'tire_pressure', 'report_lost_card', 'sync_device', 'pto_used', 'change_speed', 'carry_on', 'what_song', 'what_song', 'damaged_card', 'how_busy', 'meaning_of_life', 'travel_suggestion', 'mpg', 'do_you_have_pets', 'how_busy']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.7749, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [15]
current labels : how_busy
pair class : ['how_busy']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['how_old_are_you', 'uber', 'improve_credit_score', 'how_busy', 'find_phone', 'taxes', 'nutrition_info', 'smart_home', 'accept_reservations', 'application_status', 'damaged_card', 'gas_type', 'ingredient_substitution', 'next_song', 'text', 'how_busy']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.6043, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [13]
current labels : user_name
pair class : ['user_name']
mask: [False False False False False False False False False False False False
 False  True False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [15]
current labels : alarm
pair class : ['alarm']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [13, 15]
['user_name', 'travel_suggestion', 'order', 'redeem_rewards', 'spending_history', 'alarm', 'reset_settings', 'report_fraud', 'pto_request_status', 'flip_coin', 'distance', 'make_call', 'improve_credit_score', 'user_name', 'vaccines', 'alarm']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.4744, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : balance
pair class : ['balance']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['change_user_name', 'maybe', 'travel_notification', 'meeting_schedule', 'distance', 'where_are_you_from', 'pin_change', 'replacement_card_duration', 'balance', 'schedule_meeting', 'share_location', 'travel_suggestion', 'report_fraud', 'who_made_you', 'application_status', 'balance']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.6371, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [15]
current labels : restaurant_reviews
pair class : ['restaurant_reviews']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['redeem_rewards', 'carry_on', 'cook_time', 'rollover_401k', 'calendar', 'thank_you', 'calories', 'balance', 'bill_balance', 'vaccines', 'restaurant_reviews', 'uber', 'order', 'income', 'next_song', 'restaurant_reviews']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.0773, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [15]
current labels : shopping_list
pair class : ['shopping_list']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['account_blocked', 'update_playlist', 'traffic', 'shopping_list', 'application_status', 'vaccines', 'cook_time', 'todo_list', 'who_made_you', 'measurement_conversion', 'pto_request', 'rollover_401k', 'how_busy', 'what_can_i_ask_you', 'oil_change_how', 'shopping_list']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.6219, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [15]
current labels : car_rental
pair class : ['car_rental']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['todo_list_update', 'mpg', 'repeat', 'change_accent', 'who_do_you_work_for', 'car_rental', 'damaged_card', 'find_phone', 'meal_suggestion', 'reminder', 'restaurant_reservation', 'reminder_update', 'where_are_you_from', 'shopping_list', 'timer', 'car_rental']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.3390, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [11]
current labels : find_phone
pair class : ['find_phone']
mask: [False False False False False False False False False False False  True
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : apr
pair class : ['apr']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [11, 15]
['find_phone', 'meeting_schedule', 'weather', 'calendar_update', 'book_hotel', 'thank_you', 'yes', 'time', 'car_rental', 'apr', 'restaurant_suggestion', 'find_phone', 'who_made_you', 'interest_rate', 'change_accent', 'apr']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.4258, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [8]
current labels : translate
pair class : ['translate']
mask: [False False False False False False False False  True False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : change_volume
pair class : ['change_volume']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [8, 15]
['no', 'redeem_rewards', 'weather', 'report_fraud', 'cook_time', 'calories', 'translate', 'distance', 'translate', 'car_rental', 'what_can_i_ask_you', 'timer', 'change_volume', 'current_location', 'oil_change_when', 'change_volume']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.7044, grad_fn=<DivBackward0>)

[5,    30] loss_total: 41.148 loss_supervised_contrasive:  9.880 loss_intent :2257.504 
skip_time: 0
total : 218
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [15]
current labels : schedule_maintenance
pair class : ['schedule_maintenance']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['schedule_maintenance', 'food_last', 'exchange_rate', 'confirm_reservation', 'international_fees', 'plug_type', 'lost_luggage', 'insurance_change', 'time', 'flip_coin', 'transactions', 'routing', 'change_language', 'are_you_a_bot', 'w2', 'schedule_maintenance']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.8677, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [7]
current labels : fun_fact
pair class : ['fun_fact']
mask: [False False False False False False False  True False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : order_checks
pair class : ['order_checks']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [7, 15]
['timezone', 'lost_luggage', 'weather', 'car_rental', 'fun_fact', 'credit_limit_change', 'interest_rate', 'fun_fact', 'oil_change_when', 'time', 'distance', 'spending_history', 'order_checks', 'repeat', 'tire_change', 'order_checks']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.8674, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [4]
current labels : credit_score
pair class : ['credit_score']
mask: [False False False False  True False False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : oil_change_when
pair class : ['oil_change_when']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [4, 15]
['credit_score', 'ingredients_list', 'oil_change_when', 'change_speed', 'credit_score', 'directions', 'pto_request_status', 'carry_on', 'book_hotel', 'new_card', 'sync_device', 'min_payment', 'pin_change', 'credit_limit_change', 'reminder', 'oil_change_when']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.4129, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : tire_pressure
pair class : ['tire_pressure']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['spending_history', 'cancel_reservation', 'international_visa', 'thank_you', 'play_music', 'travel_notification', 'definition', 'change_language', 'tire_pressure', 'change_ai_name', 'account_blocked', 'make_call', 'min_payment', 'confirm_reservation', 'alarm', 'tire_pressure']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.0648, grad_fn=<DivBackward0>)

----
masking label debug : ['gas' 'gas']
current labels  gas
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 0
current skips : [ 5 15]
current labels : gas
pair class : ['gas' 'gas']
mask: [False False False False False  True False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 2
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [5, 15]
['gas', 'cook_time', 'alarm', 'book_hotel', 'calendar_update', 'gas', 'card_declined', 'reminder_update', 'cancel', 'pto_used', 'yes', 'todo_list_update', 'international_fees', 'lost_luggage', 'transfer', 'gas']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.8634, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [14]
current labels : calculator
pair class : ['calculator']
mask: [False False False False False False False False False False False False
 False False  True False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : flight_status
pair class : ['flight_status']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [14, 15]
['sync_device', 'cancel_reservation', 'restaurant_reservation', 'freeze_account', 'pto_request_status', 'expiration_date', 'shopping_list_update', 'pto_balance', 'restaurant_suggestion', 'smart_home', 'calculator', 'flight_status', 'bill_balance', 'roll_dice', 'calculator', 'flight_status']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.6598, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : carry_on
pair class : ['carry_on']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['next_song', 'are_you_a_bot', 'what_song', 'change_user_name', 'ingredients_list', 'restaurant_reviews', 'w2', 'expiration_date', 'pto_balance', 'insurance_change', 'yes', 'traffic', 'how_old_are_you', 'spelling', 'carry_on', 'carry_on']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(3.9896, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [12]
current labels : do_you_have_pets
pair class : ['do_you_have_pets']
mask: [False False False False False False False False False False False False
  True False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : definition
pair class : ['definition']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [12, 15]
['pin_change', 'cancel', 'restaurant_suggestion', 'travel_alert', 'meal_suggestion', 'no', 'report_lost_card', 'do_you_have_pets', 'oil_change_how', 'card_declined', 'insurance', 'definition', 'do_you_have_pets', 'cook_time', 'expiration_date', 'definition']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.6154, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : calendar_update
pair class : ['calendar_update']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['next_holiday', 'damaged_card', 'nutrition_info', 'pto_request', 'translate', 'timer', 'gas', 'rewards_balance', 'exchange_rate', 'calendar_update', 'what_song', 'replacement_card_duration', 'last_maintenance', 'bill_due', 'directions', 'calendar_update']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.1706, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : order
pair class : ['order']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['meeting_schedule', 'order', 'report_lost_card', 'fun_fact', 'what_can_i_ask_you', 'no', 'insurance_change', 'whisper_mode', 'exchange_rate', 'flight_status', 'change_user_name', 'reminder_update', 'report_fraud', 'confirm_reservation', 'yes', 'order']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4256, grad_fn=<DivBackward0>)

[5,    40] loss_total: 40.712 loss_supervised_contrasive:  12.974 loss_intent :3009.864 
skip_time: 0
total : 228
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [8]
current labels : payday
pair class : ['payday']
mask: [False False False False False False False False  True False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [15]
current labels : tire_change
pair class : ['tire_change']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [8, 15]
['make_call', 'payday', 'improve_credit_score', 'calories', 'spending_history', 'weather', 'tell_joke', 'new_card', 'payday', 'travel_notification', 'tire_change', 'fun_fact', 'user_name', 'rewards_balance', 'nutrition_info', 'tire_change']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.5433, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [5]
current labels : current_location
pair class : ['current_location']
mask: [False False False False False  True False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 13
current skips : [15]
current labels : change_user_name
pair class : ['change_user_name']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [5, 15]
['what_can_i_ask_you', 'current_location', 'measurement_conversion', 'nutrition_info', 'change_volume', 'current_location', 'order_status', 'tire_pressure', 'next_song', 'pay_bill', 'roll_dice', 'who_do_you_work_for', 'jump_start', 'change_user_name', 'definition', 'change_user_name']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.5736, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [15]
current labels : weather
pair class : ['weather']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['payday', 'traffic', 'accept_reservations', 'routing', 'do_you_have_pets', 'who_made_you', 'change_volume', 'travel_alert', 'interest_rate', 'spelling', 'weather', 'recipe', 'report_lost_card', 'restaurant_suggestion', 'update_playlist', 'weather']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.5951, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [15]
current labels : order_checks
pair class : ['order_checks']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['international_fees', 'card_declined', 'travel_alert', 'last_maintenance', 'update_playlist', 'date', 'order_checks', 'maybe', 'tire_change', 'improve_credit_score', 'book_flight', 'timer', 'oil_change_how', 'insurance', 'greeting', 'order_checks']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.0592, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : cancel_reservation
pair class : ['cancel_reservation']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['pto_request', 'expiration_date', 'cancel_reservation', 'shopping_list', 'w2', 'timezone', 'min_payment', 'pin_change', 'date', 'tire_pressure', 'direct_deposit', 'vaccines', 'schedule_meeting', 'tire_change', 'todo_list_update', 'cancel_reservation']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5588, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : current_location
pair class : ['current_location']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [14]
current labels : book_flight
pair class : ['book_flight']
mask: [False False False False False False False False False False False False
 False False  True False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 14]
['pto_request_status', 'w2', 'meaning_of_life', 'food_last', 'current_location', 'bill_due', 'directions', 'transfer', 'what_are_your_hobbies', 'book_flight', 'replacement_card_duration', 'what_can_i_ask_you', 'pin_change', 'text', 'book_flight', 'current_location']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.4526, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [13]
current labels : calendar
pair class : ['calendar']
mask: [False False False False False False False False False False False False
 False  True]
count: 14
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [13]
['min_payment', 'order', 'schedule_maintenance', 'order_status', 'whisper_mode', 'vaccines', 'what_are_your_hobbies', 'calendar', 'recipe', 'thank_you', 'travel_notification', 'directions', 'timezone', 'calendar']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([14, 14])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.4654, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [6]
current labels : do_you_have_pets
pair class : ['do_you_have_pets']
mask: [False False False False False False  True False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : measurement_conversion
pair class : ['measurement_conversion']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [6, 15]
['international_visa', 'cancel', 'do_you_have_pets', 'interest_rate', 'find_phone', 'yes', 'do_you_have_pets', 'reminder_update', 'meaning_of_life', 'measurement_conversion', 'tire_change', 'card_declined', 'gas_type', 'weather', 'apr', 'measurement_conversion']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.4660, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : definition
pair class : ['definition']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['what_song', 'damaged_card', 'transfer', 'what_is_your_name', 'todo_list_update', 'international_visa', 'change_user_name', 'travel_alert', 'min_payment', 'what_are_your_hobbies', 'transactions', 'international_fees', 'definition', 'book_flight', 'vaccines', 'definition']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.1685, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [11]
current labels : calendar_update
pair class : ['calendar_update']
mask: [False False False False False False False False False False False  True
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : accept_reservations
pair class : ['accept_reservations']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [11, 15]
['tell_joke', 'current_location', 'repeat', 'who_made_you', 'reminder', 'spending_history', 'reminder_update', 'calendar_update', 'meeting_schedule', 'accept_reservations', 'yes', 'calendar_update', 'text', 'cook_time', 'make_call', 'accept_reservations']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.7138, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 13
current skips : [15]
current labels : credit_limit_change
pair class : ['credit_limit_change']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['flight_status', 'timezone', 'pto_used', 'what_can_i_ask_you', 'new_card', 'improve_credit_score', 'flip_coin', 'next_holiday', 'oil_change_when', 'spending_history', 'find_phone', 'recipe', 'alarm', 'credit_limit_change', 'next_song', 'credit_limit_change']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.6864, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [8]
current labels : repeat
pair class : ['repeat']
mask: [False False False False False False False False  True False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : recipe
pair class : ['recipe']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [8, 15]
['whisper_mode', 'flip_coin', 'user_name', 'repeat', 'tire_change', 'balance', 'taxes', 'order_status', 'repeat', 'recipe', 'expiration_date', 'what_are_your_hobbies', 'text', 'rewards_balance', 'interest_rate', 'recipe']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.5660, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : calendar_update
pair class : ['calendar_update']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['change_language', 'expiration_date', 'calendar_update', 'confirm_reservation', 'insurance_change', 'rewards_balance', 'oil_change_when', 'freeze_account', 'directions', 'shopping_list', 'redeem_rewards', 'no', 'cancel_reservation', 'damaged_card', 'taxes', 'calendar_update']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.3919, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [8]
current labels : pto_balance
pair class : ['pto_balance']
mask: [False False False False False False False False  True False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : recipe
pair class : ['recipe']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [8, 15]
['pto_balance', 'order_checks', 'how_old_are_you', 'timer', 'exchange_rate', 'fun_fact', 'jump_start', 'roll_dice', 'pto_balance', 'recipe', 'definition', 'vaccines', 'confirm_reservation', 'rollover_401k', 'next_song', 'recipe']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.6819, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [6]
current labels : mpg
pair class : ['mpg']
mask: [False False False False False False  True False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [8]
current labels : order
pair class : ['order']
mask: [False False False False False False False False  True False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : min_payment
pair class : ['min_payment']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [6, 8, 15]
['mpg', 'directions', 'order', 'time', 'calculator', 'international_fees', 'mpg', 'payday', 'order', 'min_payment', 'card_declined', 'calendar', 'credit_score', 'change_speed', 'weather', 'min_payment']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(0.7880, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [7]
current labels : reminder
pair class : ['reminder']
mask: [False False False False False False False  True False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [15]
current labels : calendar
pair class : ['calendar']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [7, 15]
['bill_balance', 'book_hotel', 'reminder', 'calendar', 'oil_change_how', 'improve_credit_score', 'food_last', 'reminder', 'what_is_your_name', 'tire_pressure', 'lost_luggage', 'confirm_reservation', 'schedule_maintenance', 'whisper_mode', 'schedule_meeting', 'calendar']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.7484, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : vaccines
pair class : ['vaccines']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['how_old_are_you', 'play_music', 'vaccines', 'expiration_date', 'meeting_schedule', 'bill_balance', 'book_hotel', 'change_ai_name', 'pin_change', 'apr', 'accept_reservations', 'next_holiday', 'taxes', 'greeting', 'user_name', 'vaccines']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.5912, grad_fn=<DivBackward0>)

[6,    10] loss_total: 40.500 loss_supervised_contrasive:  2.880 loss_intent :752.386 
skip_time: 0
total : 245
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : change_speed
pair class : ['change_speed']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [11]
current labels : min_payment
pair class : ['min_payment']
mask: [False False False False False False False False False False False  True
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 11]
['credit_limit', 'change_speed', 'timer', 'calories', 'text', 'min_payment', 'todo_list_update', 'routing', 'sync_device', 'international_visa', 'time', 'min_payment', 'traffic', 'exchange_rate', 'shopping_list', 'change_speed']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.6395, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [15]
current labels : income
pair class : ['income']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['user_name', 'share_location', 'nutrition_info', 'improve_credit_score', 'rollover_401k', 'confirm_reservation', 'todo_list', 'order', 'redeem_rewards', 'order_checks', 'income', 'insurance', 'gas', 'change_speed', 'credit_limit', 'income']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.6875, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [15]
current labels : car_rental
pair class : ['car_rental']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['pto_request_status', 'rewards_balance', 'book_flight', 'what_song', 'play_music', 'measurement_conversion', 'replacement_card_duration', 'car_rental', 'report_fraud', 'who_made_you', 'expiration_date', 'card_declined', 'flight_status', 'nutrition_info', 'credit_score', 'car_rental']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.0136, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : cook_time
pair class : ['cook_time']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['flight_status', 'todo_list_update', 'cook_time', 'new_card', 'accept_reservations', 'greeting', 'oil_change_when', 'transfer', 'calories', 'find_phone', 'next_song', 'translate', 'last_maintenance', 'are_you_a_bot', 'who_made_you', 'cook_time']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.7871, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [12]
current labels : flip_coin
pair class : ['flip_coin']
mask: [False False False False False False False False False False False False
  True False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : do_you_have_pets
pair class : ['do_you_have_pets']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [12, 15]
['plug_type', 'carry_on', 'flip_coin', 'car_rental', 'goodbye', 'tire_change', 'pto_balance', 'last_maintenance', 'fun_fact', 'bill_due', 'current_location', 'do_you_have_pets', 'flip_coin', 'weather', 'who_made_you', 'do_you_have_pets']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.5078, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [15]
current labels : damaged_card
pair class : ['damaged_card']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['gas_type', 'weather', 'uber', 'damaged_card', 'account_blocked', 'yes', 'traffic', 'ingredient_substitution', 'definition', 'restaurant_reviews', 'spelling', 'what_song', 'no', 'how_busy', 'cancel', 'damaged_card']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.6937, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : find_phone
pair class : ['find_phone']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['what_song', 'thank_you', 'tire_pressure', 'ingredient_substitution', 'schedule_maintenance', 'greeting', 'direct_deposit', 'change_volume', 'meaning_of_life', 'todo_list', 'travel_suggestion', 'calendar', 'find_phone', 'restaurant_suggestion', 'payday', 'find_phone']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.8961, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [15]
current labels : restaurant_reservation
pair class : ['restaurant_reservation']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['directions', 'current_location', 'alarm', 'restaurant_reservation', 'tell_joke', 'tire_pressure', 'are_you_a_bot', 'ingredient_substitution', 'timezone', 'thank_you', 'report_fraud', 'restaurant_suggestion', 'definition', 'routing', 'calories', 'restaurant_reservation']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.5700, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [15]
current labels : find_phone
pair class : ['find_phone']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['transfer', 'freeze_account', 'pay_bill', 'change_speed', 'pin_change', 'find_phone', 'insurance', 'translate', 'roll_dice', 'jump_start', 'who_do_you_work_for', 'what_are_your_hobbies', 'plug_type', 'insurance_change', 'min_payment', 'find_phone']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.9065, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : credit_limit
pair class : ['credit_limit']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['tell_joke', 'credit_limit', 'play_music', 'goodbye', 'change_accent', 'pto_used', 'spending_history', 'where_are_you_from', 'tire_pressure', 'oil_change_how', 'are_you_a_bot', 'next_song', 'order_status', 'no', 'schedule_maintenance', 'credit_limit']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.2267, grad_fn=<DivBackward0>)

[6,    20] loss_total: 41.608 loss_supervised_contrasive:  6.873 loss_intent :1504.690 
skip_time: 0
total : 255
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : next_song
pair class : ['next_song']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['application_status', 'calories', 'how_busy', 'bill_balance', 'account_blocked', 'uber', 'mpg', 'plug_type', 'next_song', 'schedule_meeting', 'credit_limit_change', 'change_ai_name', 'redeem_rewards', 'direct_deposit', 'reminder_update', 'next_song']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.1193, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : sync_device
pair class : ['sync_device']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['order_checks', 'what_is_your_name', 'last_maintenance', 'gas_type', 'how_old_are_you', 'pto_request_status', 'book_hotel', 'shopping_list', 'alarm', 'measurement_conversion', 'change_accent', 'sync_device', 'smart_home', 'nutrition_info', 'order_status', 'sync_device']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.1595, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : thank_you
pair class : ['thank_you']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['expiration_date', 'thank_you', 'travel_alert', 'timer', 'calendar', 'transfer', 'credit_limit_change', 'interest_rate', 'carry_on', 'timezone', 'no', 'pto_request', 'maybe', 'what_are_your_hobbies', 'what_is_your_name', 'thank_you']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4590, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : meal_suggestion
pair class : ['meal_suggestion']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['change_user_name', 'pto_request_status', 'lost_luggage', 'what_are_your_hobbies', 'interest_rate', 'maybe', 'pto_balance', 'cancel', 'restaurant_suggestion', 'measurement_conversion', 'todo_list_update', 'meal_suggestion', 'change_language', 'goodbye', 'bill_due', 'meal_suggestion']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.4880, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : what_is_your_name
pair class : ['what_is_your_name']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['gas', 'update_playlist', 'improve_credit_score', 'insurance_change', 'sync_device', 'bill_due', 'share_location', 'timer', 'what_is_your_name', 'text', 'travel_notification', 'flight_status', 'make_call', 'ingredient_substitution', 'whisper_mode', 'what_is_your_name']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.6285, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [15]
current labels : cook_time
pair class : ['cook_time']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['carry_on', 'reset_settings', 'sync_device', 'report_lost_card', 'pto_used', 'cook_time', 'transactions', 'w2', 'traffic', 'play_music', 'how_old_are_you', 'todo_list_update', 'schedule_meeting', 'change_language', 'todo_list', 'cook_time']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.7990, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [15]
current labels : meaning_of_life
pair class : ['meaning_of_life']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['travel_suggestion', 'rewards_balance', 'credit_score', 'book_flight', 'uber', 'restaurant_reservation', 'roll_dice', 'meaning_of_life', 'change_user_name', 'bill_balance', 'plug_type', 'calculator', 'definition', 'flight_status', 'reset_settings', 'meaning_of_life']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.2597, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : spelling
pair class : ['spelling']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['directions', 'last_maintenance', 'exchange_rate', 'time', 'greeting', 'freeze_account', 'travel_notification', 'pin_change', 'cancel', 'sync_device', 'gas', 'meeting_schedule', 'spelling', 'meal_suggestion', 'repeat', 'spelling']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.2695, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : restaurant_reservation
pair class : ['restaurant_reservation']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['direct_deposit', 'restaurant_reservation', 'alarm', 'update_playlist', 'yes', 'smart_home', 'ingredients_list', 'travel_notification', 'oil_change_how', 'where_are_you_from', 'w2', 'thank_you', 'make_call', 'credit_limit', 'calculator', 'restaurant_reservation']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.0576, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : food_last
pair class : ['food_last']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['current_location', 'next_holiday', 'jump_start', 'direct_deposit', 'mpg', 'todo_list', 'tell_joke', 'pto_used', 'income', 'vaccines', 'whisper_mode', 'food_last', 'application_status', 'travel_alert', 'who_do_you_work_for', 'food_last']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.6033, grad_fn=<DivBackward0>)

[6,    30] loss_total: 42.294 loss_supervised_contrasive:  11.557 loss_intent :2256.883 
skip_time: 0
total : 265
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 13
current skips : [15]
current labels : change_ai_name
pair class : ['change_ai_name']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['rollover_401k', 'meaning_of_life', 'pin_change', 'calculator', 'who_made_you', 'pto_request', 'maybe', 'change_volume', 'cook_time', 'goodbye', 'damaged_card', 'roll_dice', 'travel_suggestion', 'change_ai_name', 'insurance_change', 'change_ai_name']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.6624, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [15]
current labels : spelling
pair class : ['spelling']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['replacement_card_duration', 'confirm_reservation', 'next_holiday', 'spelling', 'shopping_list_update', 'where_are_you_from', 'maybe', 'travel_notification', 'pay_bill', 'flip_coin', 'traffic', 'transactions', 'tire_change', 'application_status', 'freeze_account', 'spelling']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.5302, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [15]
current labels : balance
pair class : ['balance']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['travel_suggestion', 'restaurant_reservation', 'lost_luggage', 'schedule_meeting', 'weather', 'redeem_rewards', 'pto_request', 'balance', 'current_location', 'report_lost_card', 'international_fees', 'food_last', 'payday', 'routing', 'spending_history', 'balance']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.1225, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [15]
current labels : order
pair class : ['order']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['distance', 'international_fees', 'routing', 'order', 'text', 'travel_alert', 'food_last', 'change_speed', 'uber', 'spelling', 'application_status', 'yes', 'pto_balance', 'transactions', 'change_accent', 'order']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.3438, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [13]
current labels : ingredients_list
pair class : ['ingredients_list']
mask: [False False False False False False False False False False False False
 False  True False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [15]
current labels : date
pair class : ['date']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [11]
current labels : oil_change_when
pair class : ['oil_change_when']
mask: [False False False False False False False False False False False  True
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [13, 15, 11]
['bill_due', 'change_ai_name', 'cancel_reservation', 'repeat', 'ingredients_list', 'application_status', 'date', 'w2', 'oil_change_when', 'oil_change_how', 'book_hotel', 'oil_change_when', 'thank_you', 'ingredients_list', 'fun_fact', 'date']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(0.9205, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [6]
current labels : order_checks
pair class : ['order_checks']
mask: [False False False False False False  True False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
----
masking label debug : ['date' 'date']
current labels  date
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 10
current skips : [14 15]
current labels : date
pair class : ['date' 'date']
mask: [False False False False False False False False False False False False
 False False  True  True]
count: 16
numbers of pairs one label : 2
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [6, 14, 15]
['how_busy', 'whisper_mode', 'accept_reservations', 'who_do_you_work_for', 'what_can_i_ask_you', 'order_checks', 'order_checks', 'transfer', 'gas_type', 'apr', 'date', 'meeting_schedule', 'travel_alert', 'card_declined', 'date', 'date']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(0.8764, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [15]
current labels : income
pair class : ['income']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['credit_limit_change', 'roll_dice', 'gas', 'income', 'oil_change_how', 'travel_notification', 'what_song', 'share_location', 'vaccines', 'apr', 'restaurant_suggestion', 'improve_credit_score', 'restaurant_reviews', 'balance', 'damaged_card', 'income']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.6823, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [14]
current labels : report_lost_card
pair class : ['report_lost_card']
mask: [False False False False False False False False False False False False
 False False  True False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : how_busy
pair class : ['how_busy']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [14, 15]
['pin_change', 'make_call', 'replacement_card_duration', 'lost_luggage', 'report_lost_card', 'travel_suggestion', 'book_flight', 'last_maintenance', 'income', 'how_busy', 'spelling', 'what_can_i_ask_you', 'traffic', 'pay_bill', 'report_lost_card', 'how_busy']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.8590, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : income
pair class : ['income']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['change_language', 'account_blocked', 'change_accent', 'cancel_reservation', 'recipe', 'replacement_card_duration', 'pay_bill', 'reset_settings', 'restaurant_reviews', 'taxes', 'reminder_update', 'smart_home', 'pto_used', 'time', 'income', 'income']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.2041, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [15]
current labels : order
pair class : ['order']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['credit_score', 'new_card', 'restaurant_reservation', 'spending_history', 'change_volume', 'calculator', 'meal_suggestion', 'order', 'cancel_reservation', 'meaning_of_life', 'update_playlist', 'who_do_you_work_for', 'w2', 'greeting', 'do_you_have_pets', 'order']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.2492, grad_fn=<DivBackward0>)

[6,    40] loss_total: 41.454 loss_supervised_contrasive:  15.402 loss_intent :3009.062 
skip_time: 0
total : 275
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : cancel_reservation
pair class : ['cancel_reservation']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['play_music', 'account_blocked', 'pay_bill', 'insurance', 'cancel_reservation', 'pto_request_status', 'gas_type', 'next_holiday', 'timezone', 'todo_list', 'fun_fact', 'report_lost_card', 'car_rental', 'meal_suggestion', 'directions', 'cancel_reservation']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.1769, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : shopping_list_update
pair class : ['shopping_list_update']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['jump_start', 'plug_type', 'fun_fact', 'tell_joke', 'reset_settings', 'car_rental', 'change_user_name', 'routing', 'distance', 'shopping_list_update', 'uber', 'date', 'measurement_conversion', 'international_visa', 'reminder', 'shopping_list_update']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.4419, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : lost_luggage
pair class : ['lost_luggage']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['nutrition_info', 'reset_settings', 'report_fraud', 'carry_on', 'redeem_rewards', 'change_user_name', 'tire_pressure', 'change_ai_name', 'lost_luggage', 'order_status', 'restaurant_suggestion', 'user_name', 'share_location', 'mpg', 'insurance_change', 'lost_luggage']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4454, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : meeting_schedule
pair class : ['meeting_schedule']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['alarm', 'timezone', 'restaurant_reviews', 'distance', 'insurance', 'change_accent', 'card_declined', 'apr', 'bill_due', 'shopping_list_update', 'ingredient_substitution', 'reminder_update', 'meeting_schedule', 'nutrition_info', 'change_volume', 'meeting_schedule']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.7654, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [12]
current labels : pto_request
pair class : ['pto_request']
mask: [False False False False False False False False False False False False
  True False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : update_playlist
pair class : ['update_playlist']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [12, 15]
['what_can_i_ask_you', 'direct_deposit', 'balance', 'freeze_account', 'pto_request', 'distance', 'payday', 'translate', 'update_playlist', 'report_fraud', 'no', 'account_blocked', 'pto_request', 'calendar_update', 'international_fees', 'update_playlist']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.7102, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : are_you_a_bot
pair class : ['are_you_a_bot']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['change_volume', 'timer', 'where_are_you_from', 'meal_suggestion', 'jump_start', 'smart_home', 'what_can_i_ask_you', 'replacement_card_duration', 'are_you_a_bot', 'change_language', 'goodbye', 'food_last', 'share_location', 'car_rental', 'distance', 'are_you_a_bot']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.4101, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [8]
current labels : exchange_rate
pair class : ['exchange_rate']
mask: [False False False False False False False False  True False False False
 False False]
count: 14
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [13]
current labels : book_hotel
pair class : ['book_hotel']
mask: [False False False False False False False False False False False False
 False  True]
count: 14
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [8, 13]
['reminder', 'bill_balance', 'accept_reservations', 'exchange_rate', 'are_you_a_bot', 'recipe', 'international_visa', 'update_playlist', 'exchange_rate', 'rollover_401k', 'carry_on', 'report_fraud', 'book_hotel', 'book_hotel']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([14, 14])
after summing res.shape : torch.Size([])
res.shape : torch.Size([14, 14])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.7444, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [11]
current labels : no
pair class : ['no']
mask: [False False False False False False False False False False False  True
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [15]
current labels : oil_change_when
pair class : ['oil_change_when']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [11, 15]
['damaged_card', 'no', 'cook_time', 'balance', 'exchange_rate', 'restaurant_reservation', 'oil_change_when', 'tire_pressure', 'flip_coin', 'update_playlist', 'tire_change', 'no', 'account_blocked', 'translate', 'change_accent', 'oil_change_when']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.6525, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : traffic
pair class : ['traffic']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['w2', 'traffic', 'pto_balance', 'travel_alert', 'order_checks', 'freeze_account', 'ingredient_substitution', 'recipe', 'translate', 'vaccines', 'calories', 'measurement_conversion', 'restaurant_suggestion', 'definition', 'todo_list', 'traffic']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.2174, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : fun_fact
pair class : ['fun_fact']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [13]
current labels : timer
pair class : ['timer']
mask: [False False False False False False False False False False False False
 False  True False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 13]
['pto_used', 'fun_fact', 'goodbye', 'smart_home', 'travel_suggestion', 'tire_change', 'rewards_balance', 'timer', 'make_call', 'current_location', 'cancel', 'taxes', 'are_you_a_bot', 'timer', 'share_location', 'fun_fact']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.6028, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [11]
current labels : pto_request_status
pair class : ['pto_request_status']
mask: [False False False False False False False False False False False  True
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [15]
current labels : change_volume
pair class : ['change_volume']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [11, 15]
['maybe', 'cook_time', 'pto_request_status', 'yes', 'timer', 'change_volume', 'user_name', 'oil_change_how', 'mpg', 'reminder', 'what_can_i_ask_you', 'pto_request_status', 'insurance', 'share_location', 'w2', 'change_volume']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.7215, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [6]
current labels : pto_used
pair class : ['pto_used']
mask: [False False False False False False  True False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [15]
current labels : rollover_401k
pair class : ['rollover_401k']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [6, 15]
['pto_used', 'order', 'gas', 'are_you_a_bot', 'reminder_update', 'smart_home', 'pto_used', 'rollover_401k', 'restaurant_reservation', 'definition', 'goodbye', 'jump_start', 'timezone', 'taxes', 'who_made_you', 'rollover_401k']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.7816, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [10]
current labels : exchange_rate
pair class : ['exchange_rate']
mask: [False False False False False False False False False False  True False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : change_speed
pair class : ['change_speed']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [10, 15]
['roll_dice', 'damaged_card', 'pin_change', 'improve_credit_score', 'make_call', 'exchange_rate', 'travel_suggestion', 'vaccines', 'time', 'bill_balance', 'exchange_rate', 'no', 'income', 'schedule_meeting', 'change_speed', 'change_speed']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.7898, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [13]
current labels : application_status
pair class : ['application_status']
mask: [False False False False False False False False False False False False
 False  True False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : spending_history
pair class : ['spending_history']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [13, 15]
['next_song', 'order_status', 'book_hotel', 'application_status', 'definition', 'plug_type', 'restaurant_suggestion', 'book_flight', 'spending_history', 'calendar_update', 'pay_bill', 'pto_request', 'weather', 'application_status', 'current_location', 'spending_history']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.8438, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : reset_settings
pair class : ['reset_settings']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['where_are_you_from', 'cook_time', 'income', 'international_fees', 'credit_limit', 'roll_dice', 'reminder_update', 'book_flight', 'reset_settings', 'taxes', 'report_fraud', 'change_volume', 'thank_you', 'credit_limit_change', 'greeting', 'reset_settings']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.6811, grad_fn=<DivBackward0>)

----
masking label debug : ['change_ai_name' 'change_ai_name']
current labels  change_ai_name
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 2
current skips : [ 8 15]
current labels : change_ai_name
pair class : ['change_ai_name' 'change_ai_name']
mask: [False False False False False False False False  True False False False
 False False False  True]
count: 16
numbers of pairs one label : 2
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [9]
current labels : find_phone
pair class : ['find_phone']
mask: [False False False False False False False False False  True False False
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [8, 15, 9]
['greeting', 'routing', 'change_ai_name', 'definition', 'what_is_your_name', 'find_phone', 'book_hotel', 'credit_score', 'change_ai_name', 'find_phone', 'flip_coin', 'last_maintenance', 'reset_settings', 'insurance_change', 'what_can_i_ask_you', 'change_ai_name']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(0.9117, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [15]
current labels : routing
pair class : ['routing']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['fun_fact', 'plug_type', 'report_fraud', 'oil_change_when', 'how_busy', 'vaccines', 'routing', 'calculator', 'find_phone', 'direct_deposit', 'schedule_maintenance', 'oil_change_how', 'sync_device', 'international_visa', 'lost_luggage', 'routing']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.7836, grad_fn=<DivBackward0>)

[7,    10] loss_total: 40.107 loss_supervised_contrasive:  2.499 loss_intent :752.160 
skip_time: 0
total : 292
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [15]
current labels : application_status
pair class : ['application_status']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['make_call', 'pto_request_status', 'play_music', 'credit_score', 'mpg', 'uber', 'change_language', 'pin_change', 'insurance', 'transactions', 'application_status', 'cancel', 'calculator', 'who_do_you_work_for', 'payday', 'application_status']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.6314, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : credit_limit_change
pair class : ['credit_limit_change']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [8]
current labels : last_maintenance
pair class : ['last_maintenance']
mask: [False False False False False False False False  True False False False
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 8]
['spending_history', 'reminder_update', 'credit_limit_change', 'cancel', 'last_maintenance', 'calendar_update', 'ingredients_list', 'change_language', 'last_maintenance', 'next_song', 'spelling', 'book_flight', 'date', 'schedule_maintenance', 'lost_luggage', 'credit_limit_change']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.7550, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [9]
current labels : do_you_have_pets
pair class : ['do_you_have_pets']
mask: [False False False False False False False False False  True False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : next_holiday
pair class : ['next_holiday']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [9, 15]
['redeem_rewards', 'date', 'greeting', 'distance', 'update_playlist', 'how_old_are_you', 'uber', 'order_checks', 'do_you_have_pets', 'do_you_have_pets', 'shopping_list', 'carry_on', 'next_holiday', 'ingredients_list', 'change_volume', 'next_holiday']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.8372, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [14]
current labels : text
pair class : ['text']
mask: [False False False False False False False False False False False False
 False False  True False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [15]
current labels : alarm
pair class : ['alarm']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [14, 15]
['food_last', 'whisper_mode', 'text', 'alarm', 'sync_device', 'smart_home', 'insurance', 'directions', 'translate', 'credit_limit', 'credit_limit_change', 'gas', 'insurance_change', 'travel_suggestion', 'text', 'alarm']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.7155, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [15]
current labels : who_do_you_work_for
pair class : ['who_do_you_work_for']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['shopping_list_update', 'do_you_have_pets', 'car_rental', 'bill_due', 'account_blocked', 'make_call', 'rollover_401k', 'who_do_you_work_for', 'what_is_your_name', 'restaurant_reviews', 'current_location', 'no', 'payday', 'damaged_card', 'confirm_reservation', 'who_do_you_work_for']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.9162, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [15]
current labels : roll_dice
pair class : ['roll_dice']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['roll_dice', 'shopping_list_update', 'whisper_mode', 'translate', 'damaged_card', 'recipe', 'date', 'restaurant_reviews', 'schedule_maintenance', 'pto_balance', 'book_hotel', 'alarm', 'pto_request', 'international_fees', 'gas', 'roll_dice']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.5925, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [15]
current labels : sync_device
pair class : ['sync_device']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['ingredients_list', 'oil_change_when', 'order_checks', 'sync_device', 'food_last', 'flip_coin', 'change_accent', 'jump_start', 'reminder_update', 'share_location', 'payday', 'credit_limit', 'order', 'international_visa', 'where_are_you_from', 'sync_device']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.9668, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [7]
current labels : tell_joke
pair class : ['tell_joke']
mask: [False False False False False False False  True False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : tire_change
pair class : ['tire_change']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [7, 15]
['tell_joke', 'nutrition_info', 'pto_used', 'smart_home', 'what_are_your_hobbies', 'oil_change_when', 'change_user_name', 'tell_joke', 'order_status', 'tire_pressure', 'user_name', 'meal_suggestion', 'credit_limit_change', 'are_you_a_bot', 'tire_change', 'tire_change']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.8776, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [6]
current labels : apr
pair class : ['apr']
mask: [False False False False False False  True False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [15]
current labels : card_declined
pair class : ['card_declined']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [6, 15]
['flip_coin', 'direct_deposit', 'apr', 'card_declined', 'expiration_date', 'freeze_account', 'apr', 'measurement_conversion', 'shopping_list', 'lost_luggage', 'cancel_reservation', 'accept_reservations', 'car_rental', 'meal_suggestion', 'update_playlist', 'card_declined']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.9126, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : change_speed
pair class : ['change_speed']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['meal_suggestion', 'change_speed', 'weather', 'ingredient_substitution', 'apr', 'exchange_rate', 'timer', 'todo_list', 'alarm', 'calendar', 'bill_due', 'play_music', 'are_you_a_bot', 'next_song', 'how_busy', 'change_speed']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.1792, grad_fn=<DivBackward0>)

[7,    20] loss_total: 40.941 loss_supervised_contrasive:  5.837 loss_intent :1504.205 
skip_time: 0
total : 302
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [14]
current labels : change_volume
pair class : ['change_volume']
mask: [False False False False False False False False False False False False
 False False  True False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : replacement_card_duration
pair class : ['replacement_card_duration']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [14, 15]
['shopping_list', 'change_volume', 'order', 'bill_balance', 'tire_pressure', 'todo_list_update', 'order_status', 'measurement_conversion', 'transactions', 'how_old_are_you', 'meaning_of_life', 'timer', 'replacement_card_duration', 'meal_suggestion', 'change_volume', 'replacement_card_duration']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.6474, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : redeem_rewards
pair class : ['redeem_rewards']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['expiration_date', 'who_do_you_work_for', 'reminder', 'gas_type', 'redeem_rewards', 'tell_joke', 'meeting_schedule', 'recipe', 'apr', 'yes', 'play_music', 'restaurant_suggestion', 'new_card', 'schedule_maintenance', 'text', 'redeem_rewards']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.0246, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [2]
current labels : timezone
pair class : ['timezone']
mask: [False False  True False False False False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : calendar
pair class : ['calendar']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [2, 15]
['timezone', 'measurement_conversion', 'timezone', 'share_location', 'direct_deposit', 'gas_type', 'what_can_i_ask_you', 'repeat', 'restaurant_reservation', 'carry_on', 'directions', 'transfer', 'calendar', 'rewards_balance', 'income', 'calendar']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.7473, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [15]
current labels : order_checks
pair class : ['order_checks']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [13]
current labels : calendar_update
pair class : ['calendar_update']
mask: [False False False False False False False False False False False False
 False  True False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 13]
['new_card', 'jump_start', 'calculator', 'order_checks', 'make_call', 'nutrition_info', 'freeze_account', 'next_song', 'calendar_update', 'spelling', 'what_are_your_hobbies', 'shopping_list_update', 'traffic', 'calendar_update', 'alarm', 'order_checks']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.9729, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [13]
current labels : min_payment
pair class : ['min_payment']
mask: [False False False False False False False False False False False False
 False  True False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [10]
current labels : next_holiday
pair class : ['next_holiday']
mask: [False False False False False False False False False False  True False
 False False False False]
count: 16
numbers of pairs one label : 1
----
masking label debug : ['international_visa' 'international_visa']
current labels  international_visa
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 8
current skips : [11 15]
current labels : international_visa
pair class : ['international_visa' 'international_visa']
mask: [False False False False False False False False False False False  True
 False False False  True]
count: 16
numbers of pairs one label : 2
the number of pairs for entire batch: 4
pairs see from labels :  4
All skippings : [13, 10, 11, 15]
['what_are_your_hobbies', 'pin_change', 'bill_balance', 'change_user_name', 'min_payment', 'next_holiday', 'alarm', 'user_name', 'international_visa', 'goodbye', 'next_holiday', 'international_visa', 'pto_request_status', 'min_payment', 'ingredient_substitution', 'international_visa']
---------------------------------------------
concatenate got h_i : torch.Size([4, 768])
concatenate got h_j :  torch.Size([4, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([4, 1])
pos_sim.shape : torch.Size([4])
len(neg) : 4
loss_s_cl: tensor(0.5574, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [15]
current labels : repeat
pair class : ['repeat']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['carry_on', 'reset_settings', 'car_rental', 'restaurant_reservation', 'uber', 'gas', 'improve_credit_score', 'repeat', 'mpg', 'cancel', 'tell_joke', 'play_music', 'fun_fact', 'text', 'distance', 'repeat']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.0729, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : distance
pair class : ['distance']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['w2', 'book_flight', 'last_maintenance', 'interest_rate', 'flight_status', 'pin_change', 'change_user_name', 'travel_notification', 'report_lost_card', 'plug_type', 'weather', 'tell_joke', 'distance', 'pto_balance', 'rollover_401k', 'distance']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.6633, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : todo_list
pair class : ['todo_list']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['greeting', 'improve_credit_score', 'ingredient_substitution', 'change_speed', 'todo_list_update', 'application_status', 'reminder', 'repeat', 'change_accent', 'todo_list', 'update_playlist', 'accept_reservations', 'shopping_list_update', 'sync_device', 'travel_notification', 'todo_list']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.8350, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [8]
current labels : routing
pair class : ['routing']
mask: [False False False False False False False False  True False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [15]
current labels : international_fees
pair class : ['international_fees']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [8, 15]
['direct_deposit', 'time', 'income', 'spending_history', 'calories', 'meeting_schedule', 'routing', 'international_fees', 'routing', 'bill_balance', 'min_payment', 'order_status', 'thank_you', 'current_location', 'spelling', 'international_fees']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.8019, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [7]
current labels : card_declined
pair class : ['card_declined']
mask: [False False False False False False False  True False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : pto_request_status
pair class : ['pto_request_status']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [7, 15]
['traffic', 'what_are_your_hobbies', 'cook_time', 'restaurant_reviews', 'card_declined', 'report_lost_card', 'how_old_are_you', 'card_declined', 'pto_request_status', 'interest_rate', 'ingredient_substitution', 'whisper_mode', 'find_phone', 'who_do_you_work_for', 'distance', 'pto_request_status']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.9269, grad_fn=<DivBackward0>)

[7,    30] loss_total: 40.526 loss_supervised_contrasive:  8.762 loss_intent :2256.225 
skip_time: 0
total : 312
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : credit_score
pair class : ['credit_score']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['change_language', 'are_you_a_bot', 'do_you_have_pets', 'how_busy', 'bill_balance', 'calendar', 'international_fees', 'tire_pressure', 'where_are_you_from', 'what_is_your_name', 'w2', 'rewards_balance', 'food_last', 'taxes', 'credit_score', 'credit_score']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.9583, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [15]
current labels : transfer
pair class : ['transfer']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['report_fraud', 'time', 'card_declined', 'todo_list_update', 'replacement_card_duration', 'balance', 'maybe', 'transfer', 'calculator', 'transactions', 'meaning_of_life', 'how_old_are_you', 'bill_due', 'exchange_rate', 'insurance', 'transfer']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.6528, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [15]
current labels : bill_due
pair class : ['bill_due']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['todo_list_update', 'shopping_list', 'report_lost_card', 'gas', 'distance', 'bill_due', 'directions', 'where_are_you_from', 'ingredients_list', 'new_card', 'schedule_meeting', 'credit_limit', 'lost_luggage', 'flight_status', 'timezone', 'bill_due']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.0130, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : meeting_schedule
pair class : ['meeting_schedule']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['text', 'meeting_schedule', 'car_rental', 'nutrition_info', 'order', 'transactions', 'jump_start', 'time', 'card_declined', 'spending_history', 'traffic', 'report_lost_card', 'thank_you', 'cook_time', 'sync_device', 'meeting_schedule']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.2343, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [15]
current labels : vaccines
pair class : ['vaccines']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [14]
current labels : who_made_you
pair class : ['who_made_you']
mask: [False False False False False False False False False False False False
 False False  True False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 14]
['greeting', 'directions', 'international_visa', 'transfer', 'pay_bill', 'car_rental', 'vaccines', 'credit_score', 'improve_credit_score', 'who_made_you', 'current_location', 'date', 'measurement_conversion', 'balance', 'who_made_you', 'vaccines']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.7931, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : jump_start
pair class : ['jump_start']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['mpg', 'jump_start', 'damaged_card', 'reset_settings', 'tire_change', 'new_card', 'repeat', 'cancel_reservation', 'change_language', 'timezone', 'pay_bill', 'international_fees', 'order_checks', 'uber', 'next_holiday', 'jump_start']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.7960, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [10]
current labels : meaning_of_life
pair class : ['meaning_of_life']
mask: [False False False False False False False False False False  True False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : translate
pair class : ['translate']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [10, 15]
['meaning_of_life', 'translate', 'what_is_your_name', 'pay_bill', 'meeting_schedule', 'spelling', 'reset_settings', 'change_speed', 'share_location', 'what_can_i_ask_you', 'meaning_of_life', 'travel_alert', 'oil_change_how', 'replacement_card_duration', 'redeem_rewards', 'translate']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.7461, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [15]
current labels : reminder
pair class : ['reminder']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['reminder', 'change_user_name', 'gas_type', 'improve_credit_score', 'confirm_reservation', 'roll_dice', 'how_busy', 'routing', 'thank_you', 'plug_type', 'shopping_list', 'shopping_list_update', 'insurance_change', 'calendar', 'definition', 'reminder']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.2533, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : credit_limit
pair class : ['credit_limit']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['who_do_you_work_for', 'confirm_reservation', 'cancel_reservation', 'min_payment', 'credit_limit', 'book_hotel', 'do_you_have_pets', 'expiration_date', 'flight_status', 'change_accent', 'meal_suggestion', 'recipe', 'food_last', 'restaurant_reviews', 'pto_request', 'credit_limit']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.8527, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : pto_request
pair class : ['pto_request']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [7]
current labels : redeem_rewards
pair class : ['redeem_rewards']
mask: [False False False False False False False  True False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 7]
['transfer', 'pto_request', 'redeem_rewards', 'restaurant_suggestion', 'direct_deposit', 'time', 'carry_on', 'redeem_rewards', 'change_ai_name', 'payday', 'travel_notification', 'schedule_meeting', 'oil_change_how', 'change_user_name', 'who_made_you', 'pto_request']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.8902, grad_fn=<DivBackward0>)

[7,    40] loss_total: 41.620 loss_supervised_contrasive:  12.781 loss_intent :3008.237 
skip_time: 0
total : 322
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : user_name
pair class : ['user_name']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [14]
current labels : flight_status
pair class : ['flight_status']
mask: [False False False False False False False False False False False False
 False False  True False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [7]
current labels : rewards_balance
pair class : ['rewards_balance']
mask: [False False False False False False False  True False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [15, 14, 7]
['new_card', 'user_name', 'flight_status', 'todo_list_update', 'travel_alert', 'rewards_balance', 'next_song', 'rewards_balance', 'report_lost_card', 'pto_used', 'mpg', 'book_flight', 'find_phone', 'how_old_are_you', 'flight_status', 'user_name']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(0.9825, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [15]
current labels : goodbye
pair class : ['goodbye']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [13]
current labels : insurance_change
pair class : ['insurance_change']
mask: [False False False False False False False False False False False False
 False  True False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 13]
['goodbye', 'roll_dice', 'todo_list', 'next_holiday', 'accept_reservations', 'insurance_change', 'directions', 'what_is_your_name', 'pto_request', 'reminder_update', 'date', 'application_status', 'traffic', 'insurance_change', 'expiration_date', 'goodbye']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.8274, grad_fn=<DivBackward0>)

----
masking label debug : ['what_song' 'what_song']
current labels  what_song
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 0
current skips : [ 4 15]
current labels : what_song
pair class : ['what_song' 'what_song']
mask: [False False False False  True False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 2
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [4, 15]
['what_song', 'user_name', 'food_last', 'yes', 'what_song', 'calories', 'book_hotel', 'replacement_card_duration', 'maybe', 'transfer', 'travel_alert', 'recipe', 'bill_due', 'update_playlist', 'travel_notification', 'what_song']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.7909, grad_fn=<DivBackward0>)

----
masking label debug : ['whisper_mode' 'whisper_mode']
current labels  whisper_mode
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 3
current skips : [13 15]
current labels : whisper_mode
pair class : ['whisper_mode' 'whisper_mode']
mask: [False False False False False False False False False False False False
 False  True False  True]
count: 16
numbers of pairs one label : 2
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [9]
current labels : accept_reservations
pair class : ['accept_reservations']
mask: [False False False False False False False False False  True False False
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [13, 15, 9]
['nutrition_info', 'what_can_i_ask_you', 'play_music', 'whisper_mode', 'spending_history', 'cancel', 'w2', 'accept_reservations', 'meaning_of_life', 'accept_reservations', 'last_maintenance', 'rollover_401k', 'no', 'whisper_mode', 'order_status', 'whisper_mode']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.0325, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : repeat
pair class : ['repeat']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['spelling', 'report_fraud', 'todo_list', 'account_blocked', 'maybe', 'cancel_reservation', 'freeze_account', 'oil_change_when', 'repeat', 'taxes', 'calculator', 'transactions', 'travel_suggestion', 'tire_pressure', 'change_language', 'repeat']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.1804, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [7]
current labels : calories
pair class : ['calories']
mask: [False False False False False False False  True False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [10]
current labels : what_song
pair class : ['what_song']
mask: [False False False False False False False False False False  True False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [15]
current labels : account_blocked
pair class : ['account_blocked']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [7, 10, 15]
['calories', 'pto_balance', 'what_song', 'rollover_401k', 'change_speed', 'replacement_card_duration', 'account_blocked', 'calories', 'cancel_reservation', 'payday', 'what_song', 'confirm_reservation', 'schedule_meeting', 'reminder', 'balance', 'account_blocked']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(0.9381, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [13]
current labels : goodbye
pair class : ['goodbye']
mask: [False False False False False False False False False False False False
 False  True]
count: 14
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [13]
['restaurant_reviews', 'goodbye', 'min_payment', 'weather', 'order', 'where_are_you_from', 'pin_change', 'lost_luggage', 'who_made_you', 'what_are_your_hobbies', 'insurance', 'yes', 'carry_on', 'goodbye']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([14, 14])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.9435, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 13
current skips : [15]
current labels : where_are_you_from
pair class : ['where_are_you_from']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['repeat', 'ingredients_list', 'roll_dice', 'greeting', 'confirm_reservation', 'schedule_maintenance', 'bill_due', 'routing', 'smart_home', 'calendar_update', 'change_user_name', 'schedule_meeting', 'application_status', 'where_are_you_from', 'meaning_of_life', 'where_are_you_from']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.1789, grad_fn=<DivBackward0>)

----
masking label debug : ['pto_request' 'pto_request']
current labels  pto_request
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 1
current skips : [11 15]
current labels : pto_request
pair class : ['pto_request' 'pto_request']
mask: [False False False False False False False False False False False  True
 False False False  True]
count: 16
numbers of pairs one label : 2
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [11, 15]
['what_song', 'pto_request', 'calendar', 'whisper_mode', 'what_is_your_name', 'spelling', 'flip_coin', 'restaurant_reservation', 'exchange_rate', 'book_flight', 'who_do_you_work_for', 'pto_request', 'restaurant_reviews', 'roll_dice', 'last_maintenance', 'pto_request']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.9460, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : expiration_date
pair class : ['expiration_date']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [10]
current labels : thank_you
pair class : ['thank_you']
mask: [False False False False False False False False False False  True False
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 10]
['restaurant_reviews', 'expiration_date', 'carry_on', 'cook_time', 'pto_request_status', 'current_location', 'taxes', 'goodbye', 'thank_you', 'update_playlist', 'thank_you', 'greeting', 'international_fees', 'calendar_update', 'bill_due', 'expiration_date']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.8884, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [15]
current labels : restaurant_suggestion
pair class : ['restaurant_suggestion']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['change_volume', 'travel_notification', 'measurement_conversion', 'translate', 'shopping_list_update', 'international_fees', 'definition', 'food_last', 'travel_alert', 'do_you_have_pets', 'restaurant_suggestion', 'gas', 'repeat', 'thank_you', 'restaurant_reviews', 'restaurant_suggestion']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4477, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [13]
current labels : cancel
pair class : ['cancel']
mask: [False False False False False False False False False False False False
 False  True False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [15]
current labels : oil_change_when
pair class : ['oil_change_when']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [13, 15]
['report_fraud', 'cancel', 'shopping_list', 'user_name', 'pto_balance', 'todo_list', 'how_old_are_you', 'change_speed', 'last_maintenance', 'timezone', 'oil_change_when', 'travel_notification', 'min_payment', 'cancel', 'what_is_your_name', 'oil_change_when']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.9391, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : car_rental
pair class : ['car_rental']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['who_do_you_work_for', 'car_rental', 'uber', 'cancel_reservation', 'exchange_rate', 'rewards_balance', 'whisper_mode', 'what_is_your_name', 'how_busy', 'replacement_card_duration', 'taxes', 'confirm_reservation', 'current_location', 'distance', 'meaning_of_life', 'car_rental']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.0803, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : tell_joke
pair class : ['tell_joke']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['change_language', 'smart_home', 'tell_joke', 'redeem_rewards', 'distance', 'meaning_of_life', 'routing', 'carry_on', 'what_are_your_hobbies', 'reminder_update', 'how_busy', 'new_card', 'roll_dice', 'restaurant_reservation', 'shopping_list_update', 'tell_joke']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.1772, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [3]
current labels : order
pair class : ['order']
mask: [False False False  True False False False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : international_visa
pair class : ['international_visa']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [3, 15]
['application_status', 'who_made_you', 'order', 'order', 'alarm', 'accept_reservations', 'oil_change_when', 'timer', 'report_fraud', 'gas', 'traffic', 'international_visa', 'find_phone', 'pin_change', 'text', 'international_visa']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.9077, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [9]
current labels : change_volume
pair class : ['change_volume']
mask: [False False False False False False False False False  True False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : order_status
pair class : ['order_status']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [9, 15]
['do_you_have_pets', 'nutrition_info', 'meal_suggestion', 'last_maintenance', 'plug_type', 'transfer', 'direct_deposit', 'change_volume', 'payday', 'change_volume', 'change_language', 'pto_request', 'schedule_maintenance', 'vaccines', 'order_status', 'order_status']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.8874, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : yes
pair class : ['yes']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['pto_request_status', 'translate', 'replacement_card_duration', 'oil_change_when', 'yes', 'directions', 'ingredient_substitution', 'report_lost_card', 'travel_notification', 'reminder_update', 'balance', 'how_busy', 'what_can_i_ask_you', 'book_hotel', 'tire_pressure', 'yes']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.2212, grad_fn=<DivBackward0>)

[8,    10] loss_total: 41.166 loss_supervised_contrasive:  3.567 loss_intent :751.976 
skip_time: 0
total : 339
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [6]
current labels : w2
pair class : ['w2']
mask: [False False False False False False  True False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : income
pair class : ['income']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [6, 15]
['confirm_reservation', 'lost_luggage', 'mpg', 'w2', 'meeting_schedule', 'measurement_conversion', 'w2', 'time', 'income', 'travel_suggestion', 'damaged_card', 'balance', 'change_accent', 'how_busy', 'interest_rate', 'income']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.8738, grad_fn=<DivBackward0>)

----
masking label debug : ['apr' 'apr']
current labels  apr
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 0
current skips : [6 7]
current labels : apr
pair class : ['apr' 'apr']
mask: [False False False False False False  True  True False False False False
 False False False False]
count: 16
numbers of pairs one label : 2
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 13
current skips : [15]
current labels : change_ai_name
pair class : ['change_ai_name']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [6, 7, 15]
['apr', 'credit_score', 'redeem_rewards', 'smart_home', 'jump_start', 'rollover_401k', 'apr', 'apr', 'balance', 'whisper_mode', 'transfer', 'interest_rate', 'car_rental', 'change_ai_name', 'cook_time', 'change_ai_name']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.0518, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [8]
current labels : no
pair class : ['no']
mask: [False False False False False False False False  True False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [14]
current labels : bill_balance
pair class : ['bill_balance']
mask: [False False False False False False False False False False False False
 False False  True False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [11]
current labels : direct_deposit
pair class : ['direct_deposit']
mask: [False False False False False False False False False False False  True
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [15]
current labels : date
pair class : ['date']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 4
pairs see from labels :  4
All skippings : [8, 14, 11, 15]
['no', 'fun_fact', 'improve_credit_score', 'bill_balance', 'mpg', 'nutrition_info', 'restaurant_suggestion', 'direct_deposit', 'no', 'bill_due', 'date', 'direct_deposit', 'pto_balance', 'ingredients_list', 'bill_balance', 'date']
---------------------------------------------
concatenate got h_i : torch.Size([4, 768])
concatenate got h_j :  torch.Size([4, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([4, 1])
pos_sim.shape : torch.Size([4])
len(neg) : 4
loss_s_cl: tensor(0.5883, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : definition
pair class : ['definition']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['plug_type', 'find_phone', 'ingredient_substitution', 'interest_rate', 'definition', 'where_are_you_from', 'weather', 'oil_change_how', 'directions', 'credit_limit', 'what_are_your_hobbies', 'pay_bill', 'vaccines', 'replacement_card_duration', 'maybe', 'definition']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.0504, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : restaurant_reservation
pair class : ['restaurant_reservation']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['change_volume', 'accept_reservations', 'ingredient_substitution', 'recipe', 'traffic', 'report_lost_card', 'change_ai_name', 'book_flight', 'restaurant_reservation', 'freeze_account', 'alarm', 'what_can_i_ask_you', 'next_song', 'what_are_your_hobbies', 'order_status', 'restaurant_reservation']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(4.8592, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [11]
current labels : flight_status
pair class : ['flight_status']
mask: [False False False False False False False False False False False  True
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : share_location
pair class : ['share_location']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [11, 15]
['spelling', 'reset_settings', 'flight_status', 'application_status', 'share_location', 'meeting_schedule', 'no', 'yes', 'change_speed', 'bill_balance', 'restaurant_reservation', 'flight_status', 'improve_credit_score', 'last_maintenance', 'uber', 'share_location']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.9725, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [15]
current labels : update_playlist
pair class : ['update_playlist']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['rewards_balance', 'income', 'travel_alert', 'pto_balance', 'shopping_list', 'traffic', 'update_playlist', 'book_hotel', 'travel_notification', 'redeem_rewards', 'flip_coin', 'calories', 'calendar', 'what_song', 'date', 'update_playlist']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.1742, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [15]
current labels : reset_settings
pair class : ['reset_settings']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['reminder_update', 'gas_type', 'cook_time', 'thank_you', 'sync_device', 'shopping_list_update', 'vaccines', 'reset_settings', 'change_speed', 'recipe', 'account_blocked', 'schedule_maintenance', 'pay_bill', 'maybe', 'food_last', 'reset_settings']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.0458, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : maybe
pair class : ['maybe']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['ingredients_list', 'nutrition_info', 'next_holiday', 'sync_device', 'balance', 'pin_change', 'new_card', 'timer', 'accept_reservations', 'maybe', 'who_made_you', 'repeat', 'share_location', 'income', 'user_name', 'maybe']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4643, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [15]
current labels : share_location
pair class : ['share_location']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [10]
current labels : meal_suggestion
pair class : ['meal_suggestion']
mask: [False False False False False False False False False False  True False
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 10]
['international_visa', 'calendar_update', 'roll_dice', 'share_location', 'meal_suggestion', 'calories', 'transfer', 'vaccines', 'taxes', 'current_location', 'meal_suggestion', 'replacement_card_duration', 'payday', 'reminder_update', 'restaurant_suggestion', 'share_location']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.8387, grad_fn=<DivBackward0>)

[8,    20] loss_total: 40.887 loss_supervised_contrasive:  6.859 loss_intent :1503.883 
skip_time: 0
total : 349
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 13
current skips : [15]
current labels : rollover_401k
pair class : ['rollover_401k']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['insurance_change', 'goodbye', 'pto_used', 'who_made_you', 'are_you_a_bot', 'order_status', 'book_flight', 'translate', 'insurance', 'timer', 'gas_type', 'tire_change', 'credit_score', 'rollover_401k', 'freeze_account', 'rollover_401k']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.0129, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [14]
current labels : food_last
pair class : ['food_last']
mask: [False False False False False False False False False False False False
 False False  True False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : current_location
pair class : ['current_location']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [14, 15]
['ingredients_list', 'how_old_are_you', 'play_music', 'directions', 'fun_fact', 'expiration_date', 'sync_device', 'change_user_name', 'food_last', 'timezone', 'what_song', 'current_location', 'spending_history', 'meeting_schedule', 'food_last', 'current_location']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.9269, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [15]
current labels : find_phone
pair class : ['find_phone']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [14]
current labels : maybe
pair class : ['maybe']
mask: [False False False False False False False False False False False False
 False False  True False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 14]
['bill_due', 'insurance_change', 'calculator', 'travel_alert', 'change_ai_name', 'accept_reservations', 'make_call', 'freeze_account', 'calendar_update', 'payday', 'find_phone', 'cook_time', 'maybe', 'direct_deposit', 'maybe', 'find_phone']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.9754, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : schedule_meeting
pair class : ['schedule_meeting']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['direct_deposit', 'todo_list', 'gas_type', 'damaged_card', 'reminder', 'book_flight', 'shopping_list_update', 'pto_request', 'alarm', 'oil_change_how', 'order_checks', 'schedule_meeting', 'oil_change_when', 'report_fraud', 'uber', 'schedule_meeting']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.1617, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [15]
current labels : what_song
pair class : ['what_song']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [8]
current labels : time
pair class : ['time']
mask: [False False False False False False False False  True False False False
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 8]
['play_music', 'how_old_are_you', 'where_are_you_from', 'what_song', 'goodbye', 'time', 'confirm_reservation', 'todo_list_update', 'time', 'car_rental', 'improve_credit_score', 'cancel', 'international_visa', 'transactions', 'tell_joke', 'what_song']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.9323, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : cook_time
pair class : ['cook_time']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['no', 'lost_luggage', 'tell_joke', 'schedule_meeting', 'travel_suggestion', 'international_fees', 'share_location', 'current_location', 'plug_type', 'report_fraud', 'card_declined', 'who_do_you_work_for', 'change_language', 'travel_notification', 'cook_time', 'cook_time']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.0932, grad_fn=<DivBackward0>)

----
masking label debug : ['are_you_a_bot' 'are_you_a_bot']
current labels  are_you_a_bot
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 3
current skips : [10 15]
current labels : are_you_a_bot
pair class : ['are_you_a_bot' 'are_you_a_bot']
mask: [False False False False False False False False False False  True False
 False False False  True]
count: 16
numbers of pairs one label : 2
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [8]
current labels : oil_change_how
pair class : ['oil_change_how']
mask: [False False False False False False False False  True False False False
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [10, 15, 8]
['how_busy', 'credit_limit_change', 'calendar', 'are_you_a_bot', 'todo_list', 'who_made_you', 'definition', 'oil_change_how', 'oil_change_how', 'what_is_your_name', 'are_you_a_bot', 'translate', 'tell_joke', 'bill_due', 'what_are_your_hobbies', 'are_you_a_bot']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(0.9916, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [5]
current labels : tire_pressure
pair class : ['tire_pressure']
mask: [False False False False False  True False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 13
current skips : [15]
current labels : calendar
pair class : ['calendar']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [5, 15]
['change_ai_name', 'cancel_reservation', 'tire_pressure', 'what_can_i_ask_you', 'play_music', 'tire_pressure', 'replacement_card_duration', 'transfer', 'order', 'directions', 'timezone', 'next_song', 'text', 'calendar', 'schedule_maintenance', 'calendar']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.8939, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : what_are_your_hobbies
pair class : ['what_are_your_hobbies']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['change_user_name', 'pin_change', 'min_payment', 'what_is_your_name', 'taxes', 'tire_pressure', 'timezone', 'insurance_change', 'new_card', 'what_are_your_hobbies', 'credit_limit', 'do_you_have_pets', 'reminder', 'card_declined', 'what_can_i_ask_you', 'what_are_your_hobbies']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5683, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 13
current skips : [15]
current labels : payday
pair class : ['payday']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['report_fraud', 'change_speed', 'damaged_card', 'fun_fact', 'weather', 'pto_balance', 'pay_bill', 'pto_used', 'date', 'do_you_have_pets', 'food_last', 'alarm', 'report_lost_card', 'payday', 'tell_joke', 'payday']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5569, grad_fn=<DivBackward0>)

[8,    30] loss_total: 41.104 loss_supervised_contrasive:  10.371 loss_intent :2255.736 
skip_time: 0
total : 359
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : sync_device
pair class : ['sync_device']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['transactions', 'reset_settings', 'how_old_are_you', 'shopping_list_update', 'thank_you', 'tire_change', 'last_maintenance', 'calories', 'uber', 'sync_device', 'spending_history', 'exchange_rate', 'pto_used', 'balance', 'transfer', 'sync_device']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.1826, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [15]
current labels : credit_limit_change
pair class : ['credit_limit_change']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [8]
current labels : calories
pair class : ['calories']
mask: [False False False False False False False False  True False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [14]
current labels : damaged_card
pair class : ['damaged_card']
mask: [False False False False False False False False False False False False
 False False  True False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [15, 8, 14]
['credit_limit_change', 'international_fees', 'calories', 'car_rental', 'expiration_date', 'nutrition_info', 'damaged_card', 'date', 'calories', 'mpg', 'yes', 'calendar_update', 'pay_bill', 'tire_change', 'damaged_card', 'credit_limit_change']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.0552, grad_fn=<DivBackward0>)

----
masking label debug : ['next_holiday' 'next_holiday']
current labels  next_holiday
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 5
current skips : [ 7 15]
current labels : next_holiday
pair class : ['next_holiday' 'next_holiday']
mask: [False False False False False False False  True False False False False
 False False False  True]
count: 16
numbers of pairs one label : 2
----
masking label debug : ['change_accent' 'change_accent']
current labels  change_accent
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 10
current skips : [13 14]
current labels : change_accent
pair class : ['change_accent' 'change_accent']
mask: [False False False False False False False False False False False False
 False  True  True False]
count: 16
numbers of pairs one label : 2
the number of pairs for entire batch: 4
pairs see from labels :  4
All skippings : [7, 15, 13, 14]
['bill_balance', 'rewards_balance', 'account_blocked', 'distance', 'jump_start', 'next_holiday', 'taxes', 'next_holiday', 'timezone', 'shopping_list', 'change_accent', 'travel_suggestion', 'where_are_you_from', 'change_accent', 'change_accent', 'next_holiday']
---------------------------------------------
concatenate got h_i : torch.Size([4, 768])
concatenate got h_j :  torch.Size([4, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([4, 1])
pos_sim.shape : torch.Size([4])
len(neg) : 4
loss_s_cl: tensor(0.6292, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [15]
current labels : measurement_conversion
pair class : ['measurement_conversion']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['pto_request_status', 'car_rental', 'min_payment', 'measurement_conversion', 'greeting', 'reminder', 'what_can_i_ask_you', 'share_location', 'income', 'order_checks', 'reset_settings', 'schedule_meeting', 'text', 'cancel', 'goodbye', 'measurement_conversion']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.2496, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : cancel
pair class : ['cancel']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['rewards_balance', 'order_checks', 'oil_change_when', 'pin_change', 'plug_type', 'goodbye', 'who_do_you_work_for', 'text', 'order_status', 'calculator', 'credit_score', 'meaning_of_life', 'bill_balance', 'translate', 'cancel', 'cancel']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.1801, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [15]
current labels : next_song
pair class : ['next_song']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [14]
current labels : update_playlist
pair class : ['update_playlist']
mask: [False False False False False False False False False False False False
 False False  True False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 14]
['reminder', 'account_blocked', 'play_music', 'credit_limit', 'oil_change_how', 'credit_score', 'next_song', 'update_playlist', 'recipe', 'meeting_schedule', 'weather', 'expiration_date', 'pto_request_status', 'yes', 'update_playlist', 'next_song']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.9404, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : card_declined
pair class : ['card_declined']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['flight_status', 'new_card', 'meeting_schedule', 'smart_home', 'schedule_meeting', 'user_name', 'greeting', 'restaurant_suggestion', 'improve_credit_score', 'schedule_maintenance', 'freeze_account', 'todo_list_update', 'change_speed', 'meal_suggestion', 'card_declined', 'card_declined']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.1672, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : jump_start
pair class : ['jump_start']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['tire_change', 'who_made_you', 'mpg', 'change_ai_name', 'jump_start', 'credit_limit_change', 'transactions', 'timer', 'min_payment', 'travel_suggestion', 'interest_rate', 'what_song', 'fun_fact', 'change_accent', 'rollover_401k', 'jump_start']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.1874, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [8]
current labels : flip_coin
pair class : ['flip_coin']
mask: [False False False False False False False False  True False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [13]
current labels : routing
pair class : ['routing']
mask: [False False False False False False False False False False False False
 False  True False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : whisper_mode
pair class : ['whisper_mode']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [8, 13, 15]
['flip_coin', 'w2', 'weather', 'flight_status', 'update_playlist', 'gas_type', 'calculator', 'routing', 'flip_coin', 'how_old_are_you', 'credit_limit', 'make_call', 'whisper_mode', 'routing', 'time', 'whisper_mode']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(0.9927, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : meal_suggestion
pair class : ['meal_suggestion']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['whisper_mode', 'restaurant_suggestion', 'exchange_rate', 'credit_limit', 'calendar', 'freeze_account', 'improve_credit_score', 'order_checks', 'expiration_date', 'jump_start', 'shopping_list', 'insurance_change', 'order_status', 'plug_type', 'meal_suggestion', 'meal_suggestion']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5319, grad_fn=<DivBackward0>)

[8,    40] loss_total: 41.205 loss_supervised_contrasive:  13.982 loss_intent :3007.612 
skip_time: 0
total : 369
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : redeem_rewards
pair class : ['redeem_rewards']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [10]
current labels : find_phone
pair class : ['find_phone']
mask: [False False False False False False False False False False  True False
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 10]
['rollover_401k', 'new_card', 'redeem_rewards', 'pto_balance', 'reset_settings', 'user_name', 'cancel_reservation', 'find_phone', 'international_visa', 'restaurant_reservation', 'find_phone', 'pay_bill', 'confirm_reservation', 'make_call', 'pin_change', 'redeem_rewards']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0281, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : flip_coin
pair class : ['flip_coin']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['are_you_a_bot', 'routing', 'card_declined', 'account_blocked', 'flip_coin', 'weather', 'date', 'calculator', 'next_song', 'fun_fact', 'accept_reservations', 'spelling', 'timer', 'ingredient_substitution', 'traffic', 'flip_coin']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.0875, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : international_visa
pair class : ['international_visa']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['reminder_update', 'directions', 'cancel_reservation', 'next_holiday', 'pto_used', 'international_fees', 'where_are_you_from', 'greeting', 'uber', 'report_lost_card', 'account_blocked', 'international_visa', 'text', 'carry_on', 'spelling', 'international_visa']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.2596, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : payday
pair class : ['payday']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['rollover_401k', 'book_hotel', 'income', 'lost_luggage', 'payday', 'ingredient_substitution', 'change_language', 'time', 'definition', 'do_you_have_pets', 'transactions', 'distance', 'user_name', 'travel_alert', 'change_volume', 'payday']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.1166, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [9]
current labels : gas
pair class : ['gas']
mask: [False False False False False False False False False  True False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [15]
current labels : carry_on
pair class : ['carry_on']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [9, 15]
['gas', 'w2', 'credit_limit_change', 'tire_change', 'todo_list_update', 'gas_type', 'spending_history', 'make_call', 'are_you_a_bot', 'gas', 'carry_on', 'change_user_name', 'insurance', 'restaurant_reviews', 'jump_start', 'carry_on']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.9184, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [15]
current labels : order_checks
pair class : ['order_checks']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['apr', 'ingredients_list', 'flight_status', 'card_declined', 'todo_list_update', 'recipe', 'todo_list', 'order_checks', 'rewards_balance', 'exchange_rate', 'lost_luggage', 'traffic', 'smart_home', 'yes', 'mpg', 'order_checks']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.3988, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [11]
current labels : spending_history
pair class : ['spending_history']
mask: [False False False False False False False False False False False  True
 False False]
count: 14
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [13]
current labels : carry_on
pair class : ['carry_on']
mask: [False False False False False False False False False False False False
 False  True]
count: 14
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [11, 13]
['book_flight', 'tire_pressure', 'transactions', 'spending_history', 'report_lost_card', 'next_holiday', 'book_hotel', 'gas', 'nutrition_info', 'carry_on', 'credit_score', 'spending_history', 'cancel_reservation', 'carry_on']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([14, 14])
after summing res.shape : torch.Size([])
res.shape : torch.Size([14, 14])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.8223, grad_fn=<DivBackward0>)

----
masking label debug : ['calendar' 'calendar']
current labels  calendar
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 0
current skips : [11 15]
current labels : calendar
pair class : ['calendar' 'calendar']
mask: [False False False False False False False False False False False  True
 False False False  True]
count: 16
numbers of pairs one label : 2
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [11, 15]
['calendar', 'insurance_change', 'what_are_your_hobbies', 'play_music', 'application_status', 'rewards_balance', 'lost_luggage', 'thank_you', 'restaurant_suggestion', 'flight_status', 'make_call', 'calendar', 'maybe', 'car_rental', 'insurance', 'calendar']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.9486, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [15]
current labels : international_visa
pair class : ['international_visa']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['international_visa', 'update_playlist', 'travel_suggestion', 'smart_home', 'last_maintenance', 'no', 'cancel_reservation', 'change_accent', 'bill_due', 'calculator', 'routing', 'travel_alert', 'where_are_you_from', 'change_language', 'order_checks', 'international_visa']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.1329, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [15]
current labels : shopping_list
pair class : ['shopping_list']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['shopping_list', 'oil_change_when', 'car_rental', 'pto_balance', 'ingredient_substitution', 'change_speed', 'freeze_account', 'what_song', 'distance', 'fun_fact', 'schedule_meeting', 'definition', 'play_music', 'traffic', 'how_old_are_you', 'shopping_list']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.2042, grad_fn=<DivBackward0>)

----
masking label debug : ['improve_credit_score' 'improve_credit_score']
current labels  improve_credit_score
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 1
current skips : [ 3 15]
current labels : improve_credit_score
pair class : ['improve_credit_score' 'improve_credit_score']
mask: [False False False  True False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 2
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [11]
current labels : confirm_reservation
pair class : ['confirm_reservation']
mask: [False False False False False False False False False False False  True
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [3, 15, 11]
['travel_suggestion', 'improve_credit_score', 'pto_used', 'improve_credit_score', 'who_do_you_work_for', 'oil_change_how', 'what_is_your_name', 'confirm_reservation', 'sync_device', 'share_location', 'shopping_list', 'confirm_reservation', 'restaurant_suggestion', 'accept_reservations', 'repeat', 'improve_credit_score']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.0483, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : no
pair class : ['no']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['international_fees', 'lost_luggage', 'meal_suggestion', 'translate', 'no', 'what_is_your_name', 'fun_fact', 'insurance', 'ingredient_substitution', 'current_location', 'income', 'recipe', 'cancel_reservation', 'card_declined', 'expiration_date', 'no']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4451, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [11]
current labels : shopping_list_update
pair class : ['shopping_list_update']
mask: [False False False False False False False False False False False  True
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [15]
current labels : do_you_have_pets
pair class : ['do_you_have_pets']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [11, 15]
['whisper_mode', 'current_location', 'book_flight', 'no', 'shopping_list_update', 'application_status', 'restaurant_reviews', 'car_rental', 'reset_settings', 'international_fees', 'do_you_have_pets', 'shopping_list_update', 'payday', 'measurement_conversion', 'smart_home', 'do_you_have_pets']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.8712, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : pto_request_status
pair class : ['pto_request_status']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['confirm_reservation', 'roll_dice', 'international_visa', 'timezone', 'reminder_update', 'balance', 'tell_joke', 'expiration_date', 'pto_request_status', 'goodbye', 'change_volume', 'oil_change_how', 'direct_deposit', 'freeze_account', 'interest_rate', 'pto_request_status']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.2376, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 13
current skips : [15]
current labels : goodbye
pair class : ['goodbye']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['calendar_update', 'reset_settings', 'report_lost_card', 'roll_dice', 'mpg', 'order', 'order_checks', 'ingredients_list', 'yes', 'meal_suggestion', 'calendar', 'cancel', 'report_fraud', 'goodbye', 'change_ai_name', 'goodbye']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.3834, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : text
pair class : ['text']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['income', 'interest_rate', 'fun_fact', 'mpg', 'greeting', 'car_rental', 'nutrition_info', 'recipe', 'insurance', 'text', 'change_ai_name', 'international_fees', 'current_location', 'change_speed', 'bill_balance', 'text']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.2203, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 13
current skips : [15]
current labels : calendar
pair class : ['calendar']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['pto_balance', 'pin_change', 'who_do_you_work_for', 'meeting_schedule', 'vaccines', 'report_fraud', 'lost_luggage', 'tell_joke', 'directions', 'reset_settings', 'gas', 'repeat', 'bill_balance', 'calendar', 'carry_on', 'calendar']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.3439, grad_fn=<DivBackward0>)

[9,    10] loss_total: 41.777 loss_supervised_contrasive:  4.184 loss_intent :751.863 
skip_time: 0
total : 386
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [3]
current labels : pay_bill
pair class : ['pay_bill']
mask: [False False False  True False False False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : oil_change_how
pair class : ['oil_change_how']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [3, 15]
['jump_start', 'oil_change_when', 'pay_bill', 'pay_bill', 'bill_due', 'vaccines', 'distance', 'change_language', 'greeting', 'oil_change_how', 'transfer', 'order_checks', 'exchange_rate', 'update_playlist', 'date', 'oil_change_how']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.9123, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [15]
current labels : freeze_account
pair class : ['freeze_account']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['order', 'bill_due', 'accept_reservations', 'shopping_list', 'transfer', 'freeze_account', 'thank_you', 'cook_time', 'international_visa', 'user_name', 'tire_change', 'meeting_schedule', 'roll_dice', 'new_card', 'are_you_a_bot', 'freeze_account']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.1337, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [5]
current labels : min_payment
pair class : ['min_payment']
mask: [False False False False False  True False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [13]
current labels : what_song
pair class : ['what_song']
mask: [False False False False False False False False False False False False
 False  True False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : pay_bill
pair class : ['pay_bill']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [5, 13, 15]
['time', 'min_payment', 'make_call', 'smart_home', 'what_song', 'min_payment', 'meal_suggestion', 'nutrition_info', 'damaged_card', 'change_speed', 'restaurant_suggestion', 'new_card', 'pay_bill', 'what_song', 'schedule_maintenance', 'pay_bill']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.0486, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : update_playlist
pair class : ['update_playlist']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['ingredient_substitution', 'reminder_update', 'alarm', 'current_location', 'pto_request', 'last_maintenance', 'taxes', 'pto_balance', 'rollover_401k', 'update_playlist', 'calories', 'food_last', 'change_language', 'cancel_reservation', 'schedule_meeting', 'update_playlist']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.1986, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 13
current skips : [15]
current labels : whisper_mode
pair class : ['whisper_mode']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['where_are_you_from', 'bill_balance', 'improve_credit_score', 'smart_home', 'calories', 'exchange_rate', 'accept_reservations', 'transactions', 'ingredient_substitution', 'credit_limit_change', 'uber', 'direct_deposit', 'change_volume', 'whisper_mode', 'oil_change_when', 'whisper_mode']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4028, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : spelling
pair class : ['spelling']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['interest_rate', 'play_music', 'carry_on', 'report_lost_card', 'food_last', 'what_can_i_ask_you', 'whisper_mode', 'how_busy', 'flip_coin', 'spelling', 'apr', 'alarm', 'pto_used', 'redeem_rewards', 'restaurant_suggestion', 'spelling']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4390, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : order
pair class : ['order']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['interest_rate', 'order', 'flight_status', 'calculator', 'date', 'timer', 'mpg', 'shopping_list_update', 'tire_pressure', 'expiration_date', 'pto_request_status', 'sync_device', 'order_status', 'exchange_rate', 'last_maintenance', 'order']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.0212, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : confirm_reservation
pair class : ['confirm_reservation']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['yes', 'card_declined', 'what_can_i_ask_you', 'what_are_your_hobbies', 'todo_list_update', 'calendar_update', 'whisper_mode', 'user_name', 'plug_type', 'travel_alert', 'replacement_card_duration', 'confirm_reservation', 'meaning_of_life', 'calculator', 'current_location', 'confirm_reservation']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4485, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : time
pair class : ['time']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['transactions', 'tell_joke', 'pto_request', 'gas', 'spelling', 'calendar_update', 'yes', 'who_made_you', 'direct_deposit', 'insurance_change', 'recipe', 'pin_change', 'time', 'repeat', 'carry_on', 'time']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.3256, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [15]
current labels : damaged_card
pair class : ['damaged_card']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['insurance_change', 'book_flight', 'insurance', 'next_song', 'share_location', 'damaged_card', 'uber', 'change_accent', 'calories', 'time', 'direct_deposit', 'carry_on', 'meaning_of_life', 'what_is_your_name', 'what_can_i_ask_you', 'damaged_card']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4671, grad_fn=<DivBackward0>)

[9,    20] loss_total: 42.131 loss_supervised_contrasive:  8.723 loss_intent :1503.692 
skip_time: 0
total : 396
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [15]
current labels : maybe
pair class : ['maybe']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['damaged_card', 'timer', 'restaurant_reviews', 'what_song', 'change_speed', 'pto_request_status', 'improve_credit_score', 'maybe', 'goodbye', 'timezone', 'flip_coin', 'next_song', 'do_you_have_pets', 'spelling', 'oil_change_when', 'maybe']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.3113, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [15]
current labels : sync_device
pair class : ['sync_device']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['transactions', 'credit_score', 'pto_request', 'sync_device', 'make_call', 'reminder', 'weather', 'vaccines', 'what_song', 'credit_limit_change', 'greeting', 'redeem_rewards', 'book_flight', 'meeting_schedule', 'play_music', 'sync_device']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.3981, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : travel_suggestion
pair class : ['travel_suggestion']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['spelling', 'travel_suggestion', 'find_phone', 'shopping_list_update', 'next_song', 'what_are_your_hobbies', 'text', 'tell_joke', 'who_made_you', 'new_card', 'insurance_change', 'replacement_card_duration', 'routing', 'todo_list_update', 'measurement_conversion', 'travel_suggestion']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.3616, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : schedule_meeting
pair class : ['schedule_meeting']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['timer', 'what_is_your_name', 'thank_you', 'cook_time', 'rewards_balance', 'weather', 'ingredient_substitution', 'balance', 'travel_alert', 'order_checks', 'payday', 'schedule_meeting', 'reminder_update', 'uber', 'taxes', 'schedule_meeting']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.3187, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [15]
current labels : w2
pair class : ['w2']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['w2', 'timezone', 'travel_notification', 'application_status', 'nutrition_info', 'reminder', 'calendar', 'are_you_a_bot', 'cook_time', 'change_volume', 'book_flight', 'calories', 'make_call', 'meeting_schedule', 'schedule_maintenance', 'w2']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.2562, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [15]
current labels : who_do_you_work_for
pair class : ['who_do_you_work_for']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['freeze_account', 'todo_list', 'calculator', 'application_status', 'share_location', 'last_maintenance', 'translate', 'schedule_maintenance', 'roll_dice', 'restaurant_reservation', 'who_do_you_work_for', 'transactions', 'restaurant_reviews', 'apr', 'reminder', 'who_do_you_work_for']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.3681, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [10]
current labels : next_holiday
pair class : ['next_holiday']
mask: [False False False False False False False False False False  True False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : distance
pair class : ['distance']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [10, 15]
['play_music', 'roll_dice', 'taxes', 'measurement_conversion', 'spending_history', 'timezone', 'travel_notification', 'report_lost_card', 'todo_list', 'next_holiday', 'next_holiday', 'order', 'lost_luggage', 'rewards_balance', 'distance', 'distance']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.9402, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : insurance
pair class : ['insurance']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [11]
current labels : book_hotel
pair class : ['book_hotel']
mask: [False False False False False False False False False False False  True
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 11]
['pin_change', 'sync_device', 'insurance', 'time', 'traffic', 'cancel', 'credit_limit', 'expiration_date', 'book_hotel', 'find_phone', 'change_speed', 'book_hotel', 'share_location', 'weather', 'account_blocked', 'insurance']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.8762, grad_fn=<DivBackward0>)

----
masking label debug : ['balance' 'balance']
current labels  balance
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 1
current skips : [10 12]
current labels : balance
pair class : ['balance' 'balance']
mask: [False False False False False False False False False False  True False
  True False False False]
count: 16
numbers of pairs one label : 2
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [15]
current labels : are_you_a_bot
pair class : ['are_you_a_bot']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [10, 12, 15]
['report_lost_card', 'balance', 'restaurant_reviews', 'change_volume', 'yes', 'date', 'are_you_a_bot', 'new_card', 'greeting', 'min_payment', 'balance', 'gas_type', 'balance', 'pto_used', 'what_can_i_ask_you', 'are_you_a_bot']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.0552, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [15]
current labels : what_are_your_hobbies
pair class : ['what_are_your_hobbies']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['how_old_are_you', 'gas_type', 'account_blocked', 'who_do_you_work_for', 'international_fees', 'what_are_your_hobbies', 'tire_pressure', 'accept_reservations', 'damaged_card', 'restaurant_suggestion', 'schedule_maintenance', 'traffic', 'credit_score', 'book_hotel', 'who_made_you', 'what_are_your_hobbies']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.3530, grad_fn=<DivBackward0>)

[9,    30] loss_total: 41.818 loss_supervised_contrasive:  12.947 loss_intent :2255.571 
skip_time: 0
total : 406
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [2]
current labels : traffic
pair class : ['traffic']
mask: [False False  True False False False False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [11]
current labels : plug_type
pair class : ['plug_type']
mask: [False False False False False False False False False False False  True
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : w2
pair class : ['w2']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [2, 11, 15]
['traffic', 'who_made_you', 'traffic', 'meaning_of_life', 'plug_type', 'greeting', 'timer', 'rollover_401k', 'credit_limit_change', 'w2', 'repeat', 'plug_type', 'restaurant_reservation', 'change_ai_name', 'reminder', 'w2']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.0185, grad_fn=<DivBackward0>)

----
masking label debug : ['directions' 'directions']
current labels  directions
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 7
current skips : [ 8 15]
current labels : directions
pair class : ['directions' 'directions']
mask: [False False False False False False False False  True False False False
 False False False  True]
count: 16
numbers of pairs one label : 2
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [8, 15]
['reminder_update', 'vaccines', 'expiration_date', 'redeem_rewards', 'next_holiday', 'change_ai_name', 'date', 'directions', 'directions', 'meaning_of_life', 'alarm', 'definition', 'update_playlist', 'no', 'order_checks', 'directions']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0232, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [15]
current labels : pin_change
pair class : ['pin_change']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['pin_change', 'change_user_name', 'order', 'time', 'measurement_conversion', 'book_hotel', 'card_declined', 'travel_suggestion', 'payday', 'do_you_have_pets', 'todo_list', 'routing', 'text', 'pto_balance', 'how_old_are_you', 'pin_change']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.3811, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [4]
current labels : gas_type
pair class : ['gas_type']
mask: [False False False False  True False False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : schedule_meeting
pair class : ['schedule_meeting']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [4, 15]
['spelling', 'restaurant_reservation', 'gas_type', 'are_you_a_bot', 'gas_type', 'pto_used', 'ingredients_list', 'todo_list', 'cancel_reservation', 'credit_limit', 'pto_request', 'schedule_meeting', 'taxes', 'plug_type', 'routing', 'schedule_meeting']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0029, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [4]
current labels : translate
pair class : ['translate']
mask: [False False False False  True False False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : travel_alert
pair class : ['travel_alert']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [4, 15]
['redeem_rewards', 'translate', 'apr', 'international_visa', 'translate', 'book_flight', 'account_blocked', 'change_language', 'what_can_i_ask_you', 'flip_coin', 'who_do_you_work_for', 'freeze_account', 'travel_alert', 'find_phone', 'travel_suggestion', 'travel_alert']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.9818, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : find_phone
pair class : ['find_phone']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['report_lost_card', 'shopping_list', 'user_name', 'restaurant_reviews', 'next_holiday', 'uber', 'measurement_conversion', 'spending_history', 'find_phone', 'apr', 'exchange_rate', 'rollover_401k', 'pin_change', 'bill_due', 'reset_settings', 'find_phone']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.3964, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : order_status
pair class : ['order_status']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [6]
current labels : how_busy
pair class : ['how_busy']
mask: [False False False False False False  True False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [13]
current labels : credit_score
pair class : ['credit_score']
mask: [False False False False False False False False False False False False
 False  True False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [15, 6, 13]
['goodbye', 'whisper_mode', 'order_status', 'no', 'definition', 'how_busy', 'how_busy', 'credit_score', 'application_status', 'smart_home', 'flight_status', 'todo_list_update', 'jump_start', 'credit_score', 'redeem_rewards', 'order_status']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.0789, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [6]
current labels : meal_suggestion
pair class : ['meal_suggestion']
mask: [False False False False False False  True False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : update_playlist
pair class : ['update_playlist']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [6, 15]
['insurance_change', 'spending_history', 'meal_suggestion', 'oil_change_how', 'what_is_your_name', 'next_song', 'meal_suggestion', 'pay_bill', 'confirm_reservation', 'apr', 'international_visa', 'recipe', 'update_playlist', 'date', 'ingredients_list', 'update_playlist']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.9623, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [15]
current labels : reminder
pair class : ['reminder']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['change_ai_name', 'credit_limit', 'weather', 'todo_list_update', 'change_accent', 'reminder', 'user_name', 'flip_coin', 'alarm', 'yes', 'food_last', 'tire_pressure', 'plug_type', 'how_busy', 'replacement_card_duration', 'reminder']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.3857, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [15]
current labels : travel_alert
pair class : ['travel_alert']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['text', 'directions', 'sync_device', 'bill_balance', 'meaning_of_life', 'reminder_update', 'travel_alert', 'account_blocked', 'find_phone', 'cook_time', 'change_volume', 'flip_coin', 'alarm', 'maybe', 'oil_change_how', 'travel_alert']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4516, grad_fn=<DivBackward0>)

[9,    40] loss_total: 40.762 loss_supervised_contrasive:  16.115 loss_intent :3007.438 
skip_time: 0
total : 416
----
masking label debug : ['restaurant_reservation' 'restaurant_reservation']
current labels  restaurant_reservation
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 10
current skips : [12 15]
current labels : restaurant_reservation
pair class : ['restaurant_reservation' 'restaurant_reservation']
mask: [False False False False False False False False False False False False
  True False False  True]
count: 16
numbers of pairs one label : 2
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [12, 15]
['income', 'gas', 'last_maintenance', 'spending_history', 'nutrition_info', 'new_card', 'report_fraud', 'translate', 'pto_request_status', 'are_you_a_bot', 'restaurant_reservation', 'interest_rate', 'restaurant_reservation', 'jump_start', 'credit_limit_change', 'restaurant_reservation']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.9294, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : timer
pair class : ['timer']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['next_holiday', 'credit_score', 'travel_notification', 'cancel_reservation', 'timer', 'damaged_card', 'bill_balance', 'order_status', 'how_old_are_you', 'credit_limit', 'reset_settings', 'cook_time', 'direct_deposit', 'change_accent', 'carry_on', 'timer']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.3916, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [15]
current labels : schedule_maintenance
pair class : ['schedule_maintenance']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['schedule_maintenance', 'mpg', 'change_language', 'order_status', 'travel_notification', 'user_name', 'credit_limit', 'where_are_you_from', 'change_user_name', 'account_blocked', 'replacement_card_duration', 'min_payment', 'pto_balance', 'improve_credit_score', 'jump_start', 'schedule_maintenance']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4189, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : todo_list_update
pair class : ['todo_list_update']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['routing', 'thank_you', 'do_you_have_pets', 'calories', 'transactions', 'ingredients_list', 'food_last', 'report_fraud', 'income', 'repeat', 'tell_joke', 'uber', 'todo_list_update', 'order_status', 'cancel', 'todo_list_update']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.2397, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : transfer
pair class : ['transfer']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['pto_used', 'travel_notification', 'transfer', 'lost_luggage', 'fun_fact', 'oil_change_when', 'spending_history', 'rewards_balance', 'text', 'timezone', 'gas', 'ingredients_list', 'calendar_update', 'change_user_name', 'replacement_card_duration', 'transfer']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4959, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [15]
current labels : tire_change
pair class : ['tire_change']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [8]
current labels : payday
pair class : ['payday']
mask: [False False False False False False False False  True False False False
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 8]
['distance', 'do_you_have_pets', 'pay_bill', 'cancel', 'calendar_update', 'what_are_your_hobbies', 'tire_change', 'payday', 'payday', 'how_busy', 'report_fraud', 'maybe', 'vaccines', 'nutrition_info', 'definition', 'tire_change']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0054, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [13]
current labels : card_declined
pair class : ['card_declined']
mask: [False False False False False False False False False False False False
 False  True]
count: 14
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [11]
current labels : tire_change
pair class : ['tire_change']
mask: [False False False False False False False False False False False  True
 False False]
count: 14
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [13, 11]
['change_user_name', 'jump_start', 'gas_type', 'flight_status', 'definition', 'card_declined', 'calculator', 'weather', 'tire_change', 'directions', 'where_are_you_from', 'tire_change', 'next_song', 'card_declined']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([14, 14])
after summing res.shape : torch.Size([])
res.shape : torch.Size([14, 14])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.8468, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : international_visa
pair class : ['international_visa']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['flip_coin', 'international_visa', 'are_you_a_bot', 'tire_pressure', 'timer', 'next_holiday', 'fun_fact', 'measurement_conversion', 'roll_dice', 'definition', 'change_volume', 'bill_balance', 'improve_credit_score', 'repeat', 'balance', 'international_visa']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.3407, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : calories
pair class : ['calories']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['vaccines', 'order_checks', 'timer', 'expiration_date', 'income', 'what_song', 'text', 'how_busy', 'timezone', 'reminder_update', 'taxes', 'calories', 'maybe', 'transfer', 'exchange_rate', 'calories']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.1819, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : w2
pair class : ['w2']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['insurance_change', 'w2', 'min_payment', 'time', 'no', 'uber', 'routing', 'greeting', 'cancel', 'schedule_meeting', 'fun_fact', 'pto_request_status', 'goodbye', 'what_can_i_ask_you', 'bill_due', 'w2']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5545, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : new_card
pair class : ['new_card']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['timezone', 'time', 'book_hotel', 'make_call', 'expiration_date', 'measurement_conversion', 'traffic', 'change_ai_name', 'new_card', 'ingredient_substitution', 'distance', 'balance', 'travel_suggestion', 'vaccines', 'min_payment', 'new_card']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4052, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : mpg
pair class : ['mpg']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['share_location', 'income', 'rollover_401k', 'oil_change_when', 'meal_suggestion', 'lost_luggage', 'transactions', 'accept_reservations', 'international_visa', 'no', 'last_maintenance', 'date', 'alarm', 'how_busy', 'mpg', 'mpg']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.3079, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [8]
current labels : insurance
pair class : ['insurance']
mask: [False False False False False False False False  True False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [15]
current labels : where_are_you_from
pair class : ['where_are_you_from']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [14]
current labels : who_do_you_work_for
pair class : ['who_do_you_work_for']
mask: [False False False False False False False False False False False False
 False False  True False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [8, 15, 14]
['insurance', 'replacement_card_duration', 'report_lost_card', 'where_are_you_from', 'reminder_update', 'change_ai_name', 'roll_dice', 'do_you_have_pets', 'insurance', 'who_do_you_work_for', 'restaurant_reviews', 'ingredient_substitution', 'pin_change', 'reset_settings', 'who_do_you_work_for', 'where_are_you_from']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.0583, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : interest_rate
pair class : ['interest_rate']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['car_rental', 'replacement_card_duration', 'interest_rate', 'do_you_have_pets', 'no', 'spelling', 'user_name', 'cook_time', 'roll_dice', 'bill_balance', 'pto_balance', 'improve_credit_score', 'update_playlist', 'distance', 'credit_score', 'interest_rate']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4316, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 13
current skips : [15]
current labels : ingredient_substitution
pair class : ['ingredient_substitution']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['pin_change', 'translate', 'expiration_date', 'car_rental', 'calendar_update', 'cancel', 'travel_alert', 'tell_joke', 'vaccines', 'what_are_your_hobbies', 'sync_device', 'interest_rate', 'exchange_rate', 'ingredient_substitution', 'spending_history', 'ingredient_substitution']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.1818, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : schedule_meeting
pair class : ['schedule_meeting']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['routing', 'schedule_meeting', 'share_location', 'account_blocked', 'food_last', 'carry_on', 'thank_you', 'credit_limit_change', 'min_payment', 'who_made_you', 'book_hotel', 'income', 'calculator', 'international_visa', 'timer', 'schedule_meeting']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4700, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : accept_reservations
pair class : ['accept_reservations']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['reset_settings', 'nutrition_info', 'travel_suggestion', 'do_you_have_pets', 'pto_balance', 'tire_pressure', 'shopping_list_update', 'what_can_i_ask_you', 'play_music', 'gas', 'recipe', 'account_blocked', 'report_lost_card', 'timezone', 'accept_reservations', 'accept_reservations']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4210, grad_fn=<DivBackward0>)

[10,    10] loss_total: 42.526 loss_supervised_contrasive:  4.935 loss_intent :751.815 
skip_time: 0
total : 433
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : recipe
pair class : ['recipe']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [11]
current labels : how_old_are_you
pair class : ['how_old_are_you']
mask: [False False False False False False False False False False False  True
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [12]
current labels : change_user_name
pair class : ['change_user_name']
mask: [False False False False False False False False False False False False
  True False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [15, 11, 12]
['mpg', 'accept_reservations', 'recipe', 'goodbye', 'how_old_are_you', 'schedule_maintenance', 'fun_fact', 'gas_type', 'change_speed', 'car_rental', 'change_user_name', 'how_old_are_you', 'change_user_name', 'what_are_your_hobbies', 'card_declined', 'recipe']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.0527, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : rewards_balance
pair class : ['rewards_balance']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['international_visa', 'book_flight', 'credit_score', 'damaged_card', 'reminder_update', 'play_music', 'schedule_meeting', 'account_blocked', 'insurance', 'pay_bill', 'ingredient_substitution', 'calories', 'rewards_balance', 'redeem_rewards', 'flip_coin', 'rewards_balance']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.3884, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : schedule_meeting
pair class : ['schedule_meeting']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['thank_you', 'change_ai_name', 'alarm', 'spelling', 'flip_coin', 'order_checks', 'change_accent', 'rewards_balance', 'application_status', 'schedule_meeting', 'weather', 'shopping_list_update', 'damaged_card', 'update_playlist', 'what_is_your_name', 'schedule_meeting']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.3723, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : thank_you
pair class : ['thank_you']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['todo_list', 'meaning_of_life', 'travel_alert', 'payday', 'confirm_reservation', 'spelling', 'change_volume', 'restaurant_reservation', 'thank_you', 'change_speed', 'insurance_change', 'insurance', 'oil_change_when', 'calendar_update', 'who_made_you', 'thank_you']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4226, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [14]
current labels : tire_change
pair class : ['tire_change']
mask: [False False False False False False False False False False False False
 False False  True False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [15]
current labels : book_flight
pair class : ['book_flight']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [14, 15]
['taxes', 'tire_change', 'gas', 'shopping_list_update', 'bill_due', 'book_flight', 'plug_type', 'ingredients_list', 'todo_list', 'are_you_a_bot', 'jump_start', 'food_last', 'change_language', 'what_are_your_hobbies', 'tire_change', 'book_flight']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0070, grad_fn=<DivBackward0>)

----
masking label debug : ['yes' 'yes']
current labels  yes
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 4
current skips : [ 8 15]
current labels : yes
pair class : ['yes' 'yes']
mask: [False False False False False False False False  True False False False
 False False False  True]
count: 16
numbers of pairs one label : 2
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [8, 15]
['transactions', 'international_fees', 'date', 'user_name', 'yes', 'payday', 'no', 'pto_used', 'yes', 'who_do_you_work_for', 'confirm_reservation', 'balance', 'change_accent', 'jump_start', 'translate', 'yes']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0673, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : carry_on
pair class : ['carry_on']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['sync_device', 'carry_on', 'mpg', 'what_song', 'credit_score', 'smart_home', 'recipe', 'ingredient_substitution', 'oil_change_how', 'account_blocked', 'ingredients_list', 'restaurant_suggestion', 'calendar', 'maybe', 'exchange_rate', 'carry_on']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.3961, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [9]
current labels : goodbye
pair class : ['goodbye']
mask: [False False False False False False False False False  True False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [15]
current labels : roll_dice
pair class : ['roll_dice']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [9, 15]
['meeting_schedule', 'travel_notification', 'smart_home', 'maybe', 'yes', 'goodbye', 'are_you_a_bot', 'roll_dice', 'reminder', 'goodbye', 'travel_alert', 'date', 'thank_you', 'todo_list_update', 'pto_request', 'roll_dice']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0221, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : calendar
pair class : ['calendar']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [10]
current labels : where_are_you_from
pair class : ['where_are_you_from']
mask: [False False False False False False False False False False  True False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [11]
current labels : transactions
pair class : ['transactions']
mask: [False False False False False False False False False False False  True
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [15, 10, 11]
['direct_deposit', 'report_fraud', 'calendar', 'where_are_you_from', 'credit_limit_change', 'todo_list', 'jump_start', 'apr', 'freeze_account', 'transactions', 'where_are_you_from', 'transactions', 'lost_luggage', 'calories', 'make_call', 'calendar']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.0732, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [6]
current labels : whisper_mode
pair class : ['whisper_mode']
mask: [False False False False False False  True False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
----
masking label debug : ['nutrition_info' 'nutrition_info']
current labels  nutrition_info
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 8
current skips : [10 12]
current labels : nutrition_info
pair class : ['nutrition_info' 'nutrition_info']
mask: [False False False False False False False False False False  True False
  True False False False]
count: 16
numbers of pairs one label : 2
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : rollover_401k
pair class : ['rollover_401k']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 4
pairs see from labels :  4
All skippings : [6, 10, 12, 15]
['direct_deposit', 'timer', 'whisper_mode', 'how_old_are_you', 'greeting', 'replacement_card_duration', 'whisper_mode', 'update_playlist', 'nutrition_info', 'reminder', 'nutrition_info', 'rollover_401k', 'nutrition_info', 'tire_pressure', 'next_holiday', 'rollover_401k']
---------------------------------------------
concatenate got h_i : torch.Size([4, 768])
concatenate got h_j :  torch.Size([4, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([4, 1])
pos_sim.shape : torch.Size([4])
len(neg) : 4
loss_s_cl: tensor(0.6560, grad_fn=<DivBackward0>)

[10,    20] loss_total: 40.638 loss_supervised_contrasive:  7.981 loss_intent :1503.651 
skip_time: 0
total : 443
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : next_song
pair class : ['next_song']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['what_can_i_ask_you', 'interest_rate', 'change_speed', 'book_flight', 'meal_suggestion', 'book_hotel', 'maybe', 'income', 'w2', 'reminder', 'jump_start', 'accept_reservations', 'next_song', 'insurance', 'oil_change_when', 'next_song']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.3511, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [9]
current labels : alarm
pair class : ['alarm']
mask: [False False False False False False False False False  True False False
 False False False False]
count: 16
numbers of pairs one label : 1
----
masking label debug : ['credit_limit' 'credit_limit']
current labels  credit_limit
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 7
current skips : [13 15]
current labels : credit_limit
pair class : ['credit_limit' 'credit_limit']
mask: [False False False False False False False False False False False False
 False  True False  True]
count: 16
numbers of pairs one label : 2
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [9, 13, 15]
['alarm', 'no', 'book_hotel', 'recipe', 'change_volume', 'sync_device', 'international_fees', 'credit_limit', 'plug_type', 'alarm', 'credit_limit_change', 'what_is_your_name', 'maybe', 'credit_limit', 'order_status', 'credit_limit']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.0625, grad_fn=<DivBackward0>)

----
masking label debug : ['how_busy' 'how_busy']
current labels  how_busy
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 1
current skips : [14 15]
current labels : how_busy
pair class : ['how_busy' 'how_busy']
mask: [False False False False False False False False False False False False
 False False  True  True]
count: 16
numbers of pairs one label : 2
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [6]
current labels : flight_status
pair class : ['flight_status']
mask: [False False False False False False  True False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [14, 15, 6]
['credit_limit_change', 'how_busy', 'time', 'change_speed', 'flight_status', 'order_status', 'flight_status', 'pto_used', 'apr', 'direct_deposit', 'change_accent', 'card_declined', 'min_payment', 'improve_credit_score', 'how_busy', 'how_busy']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.1017, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : find_phone
pair class : ['find_phone']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['smart_home', 'find_phone', 'distance', 'user_name', 'shopping_list_update', 'text', 'pto_request', 'transfer', 'restaurant_suggestion', 'cancel', 'shopping_list', 'gas', 'report_fraud', 'how_old_are_you', 'ingredients_list', 'find_phone']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4322, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : mpg
pair class : ['mpg']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['change_user_name', 'flight_status', 'oil_change_how', 'exchange_rate', 'food_last', 'what_song', 'calories', 'weather', 'pin_change', 'mpg', 'definition', 'measurement_conversion', 'spending_history', 'travel_suggestion', 'who_made_you', 'mpg']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.2675, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : mpg
pair class : ['mpg']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['cook_time', 'tire_change', 'order_status', 'whisper_mode', 'pto_request_status', 'goodbye', 'cancel_reservation', 'translate', 'pto_request', 'change_volume', 'calendar_update', 'mpg', 'payday', 'meal_suggestion', 'flip_coin', 'mpg']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.3011, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : change_volume
pair class : ['change_volume']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['reset_settings', 'find_phone', 'cook_time', 'credit_limit', 'smart_home', 'flight_status', 'carry_on', 'min_payment', 'timer', 'change_volume', 'change_user_name', 'last_maintenance', 'bill_due', 'ingredients_list', 'timezone', 'change_volume']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.2672, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [15]
current labels : car_rental
pair class : ['car_rental']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['vaccines', 'improve_credit_score', 'uber', 'car_rental', 'gas_type', 'todo_list_update', 'time', 'order_status', 'cancel', 'pto_used', 'cancel_reservation', 'routing', 'travel_notification', 'exchange_rate', 'whisper_mode', 'car_rental']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.2881, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : confirm_reservation
pair class : ['confirm_reservation']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [8]
current labels : travel_notification
pair class : ['travel_notification']
mask: [False False False False False False False False  True False False False
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 8]
['international_fees', 'calories', 'confirm_reservation', 'credit_score', 'find_phone', 'travel_notification', 'apr', 'order', 'travel_notification', 'who_do_you_work_for', 'insurance_change', 'text', 'greeting', 'book_flight', 'pay_bill', 'confirm_reservation']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0567, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [10]
current labels : order_checks
pair class : ['order_checks']
mask: [False False False False False False False False False False  True False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : share_location
pair class : ['share_location']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [10, 15]
['damaged_card', 'what_song', 'meaning_of_life', 'insurance_change', 'travel_alert', 'order_checks', 'new_card', 'calendar', 'shopping_list', 'current_location', 'order_checks', 'share_location', 'redeem_rewards', 'reset_settings', 'rollover_401k', 'share_location']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0255, grad_fn=<DivBackward0>)

[10,    30] loss_total: 41.407 loss_supervised_contrasive:  11.796 loss_intent :2255.477 
skip_time: 0
total : 453
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [15]
current labels : bill_balance
pair class : ['bill_balance']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['bill_due', 'schedule_maintenance', 'pto_used', 'credit_score', 'tire_change', 'bill_balance', 'weather', 'uber', 'text', 'measurement_conversion', 'calendar', 'replacement_card_duration', 'international_visa', 'next_song', 'gas', 'bill_balance']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4729, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [10]
current labels : meeting_schedule
pair class : ['meeting_schedule']
mask: [False False False False False False False False False False  True False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : traffic
pair class : ['traffic']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [10, 15]
['calculator', 'plug_type', 'what_are_your_hobbies', 'meeting_schedule', 'oil_change_how', 'meaning_of_life', 'pto_used', 'restaurant_reservation', 'lost_luggage', 'replacement_card_duration', 'meeting_schedule', 'w2', 'calendar_update', 'make_call', 'traffic', 'traffic']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.9967, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 13
current skips : [15]
current labels : todo_list_update
pair class : ['todo_list_update']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['share_location', 'balance', 'direct_deposit', 'taxes', 'calendar_update', 'current_location', 'damaged_card', 'where_are_you_from', 'sync_device', 'weather', 'gas_type', 'last_maintenance', 'change_accent', 'todo_list_update', 'bill_balance', 'todo_list_update']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.2977, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [9]
current labels : card_declined
pair class : ['card_declined']
mask: [False False False False False False False False False  True False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [15]
current labels : damaged_card
pair class : ['damaged_card']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [9, 15]
['measurement_conversion', 'travel_alert', 'are_you_a_bot', 'how_old_are_you', 'card_declined', 'directions', 'w2', 'damaged_card', 'food_last', 'card_declined', 'meal_suggestion', 'lost_luggage', 'update_playlist', 'book_flight', 'jump_start', 'damaged_card']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0455, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [15]
current labels : payday
pair class : ['payday']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['freeze_account', 'cancel', 'translate', 'report_lost_card', 'user_name', 'payday', 'smart_home', 'routing', 'last_maintenance', 'rewards_balance', 'pin_change', 'confirm_reservation', 'who_do_you_work_for', 'reminder', 'meaning_of_life', 'payday']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.3794, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : tire_pressure
pair class : ['tire_pressure']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['current_location', 'directions', 'carry_on', 'restaurant_reviews', 'tire_pressure', 'flight_status', 'improve_credit_score', 'routing', 'meeting_schedule', 'meal_suggestion', 'rollover_401k', 'last_maintenance', 'what_is_your_name', 'international_fees', 'uber', 'tire_pressure']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5309, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [15]
current labels : transfer
pair class : ['transfer']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['transfer', 'what_is_your_name', 'fun_fact', 'next_song', 'directions', 'gas_type', 'pto_balance', 'food_last', 'card_declined', 'traffic', 'taxes', 'ingredients_list', 'report_fraud', 'restaurant_suggestion', 'change_ai_name', 'transfer']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.3931, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : schedule_maintenance
pair class : ['schedule_maintenance']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['nutrition_info', 'plug_type', 'interest_rate', 'calculator', 'timezone', 'pay_bill', 'travel_suggestion', 'definition', 'freeze_account', 'cancel_reservation', 'reset_settings', 'what_can_i_ask_you', 'what_is_your_name', 'pin_change', 'schedule_maintenance', 'schedule_maintenance']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5821, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [15]
current labels : credit_limit
pair class : ['credit_limit']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [8]
current labels : change_language
pair class : ['change_language']
mask: [False False False False False False False False  True False False False
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 8]
['credit_limit', 'restaurant_reservation', 'change_language', 'what_song', 'next_holiday', 'translate', 'tell_joke', 'pto_request_status', 'change_language', 'user_name', 'uber', 'order', 'sync_device', 'income', 'play_music', 'credit_limit']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0365, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : change_language
pair class : ['change_language']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['carry_on', 'direct_deposit', 'transfer', 'application_status', 'directions', 'order_status', 'reminder_update', 'what_are_your_hobbies', 'gas', 'bill_due', 'spending_history', 'next_holiday', 'change_language', 'shopping_list', 'make_call', 'change_language']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4261, grad_fn=<DivBackward0>)

[10,    40] loss_total: 42.006 loss_supervised_contrasive:  16.213 loss_intent :3007.273 
skip_time: 0
total : 463
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [11]
current labels : spending_history
pair class : ['spending_history']
mask: [False False False False False False False False False False False  True
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : repeat
pair class : ['repeat']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [11, 15]
['change_speed', 'spending_history', 'oil_change_when', 'directions', 'flip_coin', 'text', 'tell_joke', 'greeting', 'freeze_account', 'traffic', 'redeem_rewards', 'spending_history', 'restaurant_suggestion', 'make_call', 'repeat', 'repeat']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0206, grad_fn=<DivBackward0>)

----
masking label debug : ['current_location' 'current_location']
current labels  current_location
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 5
current skips : [14 15]
current labels : current_location
pair class : ['current_location' 'current_location']
mask: [False False False False False False False False False False False False
 False False  True  True]
count: 16
numbers of pairs one label : 2
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [10]
current labels : find_phone
pair class : ['find_phone']
mask: [False False False False False False False False False False  True False
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [14, 15, 10]
['yes', 'oil_change_when', 'traffic', 'greeting', 'meeting_schedule', 'current_location', 'thank_you', 'gas_type', 'shopping_list', 'find_phone', 'find_phone', 'do_you_have_pets', 'repeat', 'order', 'current_location', 'current_location']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.0993, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : restaurant_reviews
pair class : ['restaurant_reviews']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['pto_request', 'restaurant_reviews', 'oil_change_how', 'bill_balance', 'report_fraud', 'travel_notification', 'spelling', 'tell_joke', 'order_checks', 'what_can_i_ask_you', 'transactions', 'redeem_rewards', 'freeze_account', 'pto_request_status', 'are_you_a_bot', 'restaurant_reviews']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4491, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 13
current skips : [15]
current labels : todo_list
pair class : ['todo_list']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['account_blocked', 'play_music', 'how_busy', 'new_card', 'time', 'accept_reservations', 'vaccines', 'order', 'spelling', 'who_made_you', 'restaurant_reviews', 'shopping_list', 'where_are_you_from', 'todo_list', 'transfer', 'todo_list']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4140, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [15]
current labels : reminder_update
pair class : ['reminder_update']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['yes', 'tire_pressure', 'application_status', 'calculator', 'expiration_date', 'reminder_update', 'restaurant_suggestion', 'meaning_of_life', 'change_user_name', 'fun_fact', 'pto_request_status', 'shopping_list_update', 'reminder', 'cook_time', 'new_card', 'reminder_update']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4292, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : tire_change
pair class : ['tire_change']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['report_lost_card', 'restaurant_reservation', 'car_rental', 'book_hotel', 'pay_bill', 'calendar', 'redeem_rewards', 'whisper_mode', 'who_made_you', 'travel_suggestion', 'roll_dice', 'oil_change_how', 'tire_change', 'rewards_balance', 'expiration_date', 'tire_change']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4557, grad_fn=<DivBackward0>)

----
masking label debug : ['distance' 'distance']
current labels  distance
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 4
current skips : [ 9 13]
current labels : distance
pair class : ['distance' 'distance']
mask: [False False False False False False False False False  True False False
 False  True]
count: 14
numbers of pairs one label : 2
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [9, 13]
['pto_request', 'todo_list_update', 'share_location', 'update_playlist', 'distance', 'report_lost_card', 'schedule_maintenance', 'alarm', 'date', 'distance', 'calculator', 'taxes', 'next_song', 'distance']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([14, 14])
after summing res.shape : torch.Size([])
res.shape : torch.Size([14, 14])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.8593, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [15]
current labels : new_card
pair class : ['new_card']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['exchange_rate', 'travel_notification', 'what_are_your_hobbies', 'new_card', 'pin_change', 'freeze_account', 'interest_rate', 'date', 'meal_suggestion', 'current_location', 'credit_limit_change', 'lost_luggage', 'bill_due', 'spelling', 'order_status', 'new_card']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4696, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : what_song
pair class : ['what_song']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['travel_alert', 'what_song', 'tire_change', 'whisper_mode', 'sync_device', 'pto_used', 'reset_settings', 'user_name', 'share_location', 'meaning_of_life', 'expiration_date', 'calories', 'meal_suggestion', 'book_flight', 'order', 'what_song']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.6061, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : min_payment
pair class : ['min_payment']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['user_name', 'schedule_meeting', 'oil_change_when', 'ingredient_substitution', 'min_payment', 'shopping_list', 'pto_balance', 'rollover_401k', 'change_ai_name', 'calendar_update', 'carry_on', 'card_declined', 'update_playlist', 'where_are_you_from', 'replacement_card_duration', 'min_payment']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4728, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : thank_you
pair class : ['thank_you']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['what_song', 'shopping_list_update', 'what_can_i_ask_you', 'nutrition_info', 'payday', 'pto_request_status', 'change_language', 'card_declined', 'who_made_you', 'no', 'make_call', 'thank_you', 'recipe', 'tire_pressure', 'bill_due', 'thank_you']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4328, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [15]
current labels : timezone
pair class : ['timezone']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['greeting', 'traffic', 'what_song', 'no', 'calories', 'mpg', 'uber', 'timezone', 'restaurant_reservation', 'change_user_name', 'travel_alert', 'pto_balance', 'balance', 'reset_settings', 'change_volume', 'timezone']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4233, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : pto_used
pair class : ['pto_used']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['payday', 'pto_used', 'pin_change', 'pay_bill', 'flight_status', 'what_are_your_hobbies', 'order', 'rewards_balance', 'car_rental', 'shopping_list_update', 'repeat', 'measurement_conversion', 'flip_coin', 'exchange_rate', 'meal_suggestion', 'pto_used']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4404, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : how_busy
pair class : ['how_busy']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['pto_used', 'calendar_update', 'cancel', 'ingredients_list', 'rewards_balance', 'measurement_conversion', 'credit_limit_change', 'flip_coin', 'oil_change_when', 'how_busy', 'restaurant_suggestion', 'what_song', 'how_old_are_you', 'gas_type', 'routing', 'how_busy']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5294, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [4]
current labels : car_rental
pair class : ['car_rental']
mask: [False False False False  True False False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : who_do_you_work_for
pair class : ['who_do_you_work_for']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [13]
current labels : rollover_401k
pair class : ['rollover_401k']
mask: [False False False False False False False False False False False False
 False  True False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [4, 15, 13]
['interest_rate', 'car_rental', 'who_do_you_work_for', 'play_music', 'car_rental', 'goodbye', 'rollover_401k', 'uber', 'restaurant_suggestion', 'payday', 'balance', 'repeat', 'pto_request', 'rollover_401k', 'calendar', 'who_do_you_work_for']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.0834, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [13]
current labels : book_flight
pair class : ['book_flight']
mask: [False False False False False False False False False False False False
 False  True False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [15]
current labels : pin_change
pair class : ['pin_change']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [13, 15]
['recipe', 'mpg', 'shopping_list', 'calculator', 'book_flight', 'pin_change', 'whisper_mode', 'bill_balance', 'ingredients_list', 'bill_due', 'report_lost_card', 'pto_balance', 'damaged_card', 'book_flight', 'change_language', 'pin_change']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0075, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [10]
current labels : report_fraud
pair class : ['report_fraud']
mask: [False False False False False False False False False False  True False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : restaurant_suggestion
pair class : ['restaurant_suggestion']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [10, 15]
['thank_you', 'ingredients_list', 'direct_deposit', 'rewards_balance', 'make_call', 'yes', 'reminder_update', 'do_you_have_pets', 'report_fraud', 'fun_fact', 'report_fraud', 'what_is_your_name', 'restaurant_suggestion', 'bill_balance', 'travel_alert', 'restaurant_suggestion']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0847, grad_fn=<DivBackward0>)

[11,    10] loss_total: 41.946 loss_supervised_contrasive:  4.355 loss_intent :751.825 
skip_time: 0
total : 480
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [9]
current labels : measurement_conversion
pair class : ['measurement_conversion']
mask: [False False False False False False False False False  True False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : credit_score
pair class : ['credit_score']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [9, 15]
['measurement_conversion', 'taxes', 'confirm_reservation', 'make_call', 'credit_score', 'timer', 'routing', 'damaged_card', 'lost_luggage', 'measurement_conversion', 'text', 'plug_type', 'distance', 'calendar_update', 'exchange_rate', 'credit_score']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0229, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [15]
current labels : damaged_card
pair class : ['damaged_card']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [10]
current labels : freeze_account
pair class : ['freeze_account']
mask: [False False False False False False False False False False  True False
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 10]
['last_maintenance', 'shopping_list', 'account_blocked', 'damaged_card', 'tell_joke', 'freeze_account', 'oil_change_when', 'replacement_card_duration', 'gas_type', 'insurance_change', 'freeze_account', 'how_busy', 'todo_list', 'oil_change_how', 'pto_balance', 'damaged_card']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0393, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : flight_status
pair class : ['flight_status']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['yes', 'apr', 'flight_status', 'oil_change_when', 'spending_history', 'change_speed', 'current_location', 'schedule_meeting', 'what_song', 'last_maintenance', 'restaurant_reviews', 'play_music', 'whisper_mode', 'vaccines', 'credit_limit_change', 'flight_status']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4737, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [10]
current labels : timer
pair class : ['timer']
mask: [False False False False False False False False False False  True False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [11]
current labels : taxes
pair class : ['taxes']
mask: [False False False False False False False False False False False  True
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : change_ai_name
pair class : ['change_ai_name']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [10, 11, 15]
['flip_coin', 'ingredients_list', 'timer', 'taxes', 'user_name', 'calculator', 'weather', 'definition', 'accept_reservations', 'change_ai_name', 'timer', 'taxes', 'roll_dice', 'change_speed', 'food_last', 'change_ai_name']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.0735, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [11]
current labels : next_holiday
pair class : ['next_holiday']
mask: [False False False False False False False False False False False  True
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : recipe
pair class : ['recipe']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [11, 15]
['share_location', 'what_is_your_name', 'travel_suggestion', 'insurance_change', 'order_status', 'reset_settings', 'gas', 'next_holiday', 'user_name', 'recipe', 'calendar', 'next_holiday', 'bill_due', 'translate', 'rewards_balance', 'recipe']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0294, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [9]
current labels : pto_request
pair class : ['pto_request']
mask: [False False False False False False False False False  True False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : how_old_are_you
pair class : ['how_old_are_you']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [9, 15]
['thank_you', 'pto_request', 'how_old_are_you', 'who_do_you_work_for', 'balance', 'todo_list_update', 'rollover_401k', 'spelling', 'book_hotel', 'pto_request', 'text', 'international_visa', 'do_you_have_pets', 'apr', 'report_lost_card', 'how_old_are_you']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0521, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [15]
current labels : report_fraud
pair class : ['report_fraud']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['gas_type', 'meeting_schedule', 'are_you_a_bot', 'w2', 'application_status', 'income', 'bill_balance', 'meaning_of_life', 'travel_alert', 'ingredient_substitution', 'report_fraud', 'next_song', 'insurance', 'oil_change_how', 'tell_joke', 'report_fraud']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.3679, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : credit_score
pair class : ['credit_score']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['flight_status', 'transfer', 'restaurant_reviews', 'reminder_update', 'new_card', 'replacement_card_duration', 'last_maintenance', 'pto_request_status', 'credit_score', 'uber', 'yes', 'calendar', 'change_accent', 'pay_bill', 'flip_coin', 'credit_score']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4665, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 13
current skips : [15]
current labels : change_accent
pair class : ['change_accent']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['gas_type', 'expiration_date', 'credit_limit_change', 'next_holiday', 'lost_luggage', 'book_hotel', 'spending_history', 'meaning_of_life', 'order_checks', 'income', 'alarm', 'international_fees', 'taxes', 'change_accent', 'are_you_a_bot', 'change_accent']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.3601, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [9]
current labels : todo_list_update
pair class : ['todo_list_update']
mask: [False False False False False False False False False  True False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [15]
current labels : report_fraud
pair class : ['report_fraud']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [9, 15]
['current_location', 'schedule_maintenance', 'rewards_balance', 'todo_list_update', 'redeem_rewards', 'accept_reservations', 'user_name', 'report_fraud', 'sync_device', 'todo_list_update', 'tell_joke', 'card_declined', 'credit_score', 'travel_notification', 'find_phone', 'report_fraud']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0038, grad_fn=<DivBackward0>)

[11,    20] loss_total: 40.880 loss_supervised_contrasive:  7.644 loss_intent :1503.643 
skip_time: 0
total : 490
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : spending_history
pair class : ['spending_history']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['change_accent', 'where_are_you_from', 'ingredient_substitution', 'book_hotel', 'definition', 'international_fees', 'fun_fact', 'w2', 'improve_credit_score', 'carry_on', 'restaurant_reviews', 'cook_time', 'spending_history', 'meal_suggestion', 'yes', 'spending_history']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4082, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [4]
current labels : calories
pair class : ['calories']
mask: [False False False False  True False False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [15]
current labels : goodbye
pair class : ['goodbye']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [4, 15]
['calories', 'fun_fact', 'last_maintenance', 'order_status', 'calories', 'goodbye', 'traffic', 'calculator', 'timezone', 'date', 'change_language', 'credit_limit', 'report_lost_card', 'account_blocked', 'meaning_of_life', 'goodbye']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0360, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [8]
current labels : restaurant_suggestion
pair class : ['restaurant_suggestion']
mask: [False False False False False False False False  True False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [15]
current labels : flip_coin
pair class : ['flip_coin']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [8, 15]
['jump_start', 'next_song', 'food_last', 'new_card', 'income', 'traffic', 'restaurant_suggestion', 'flip_coin', 'restaurant_suggestion', 'apr', 'make_call', 'update_playlist', 'interest_rate', 'travel_notification', 'directions', 'flip_coin']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0234, grad_fn=<DivBackward0>)

----
masking label debug : ['smart_home' 'smart_home']
current labels  smart_home
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 1
current skips : [ 6 13]
current labels : smart_home
pair class : ['smart_home' 'smart_home']
mask: [False False False False False False  True False False False False False
 False  True False False]
count: 16
numbers of pairs one label : 2
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [15]
current labels : bill_due
pair class : ['bill_due']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [6, 13, 15]
['credit_limit', 'smart_home', 'min_payment', 'bill_due', 'maybe', 'order_checks', 'smart_home', 'share_location', 'whisper_mode', 'jump_start', 'calendar_update', 'cancel', 'meeting_schedule', 'smart_home', 'cancel_reservation', 'bill_due']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.1100, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : weather
pair class : ['weather']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['transactions', 'apr', 'report_lost_card', 'timezone', 'weather', 'reminder_update', 'who_made_you', 'insurance_change', 'gas', 'spending_history', 'are_you_a_bot', 'new_card', 'book_hotel', 'order_checks', 'flight_status', 'weather']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.3661, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [10]
current labels : oil_change_how
pair class : ['oil_change_how']
mask: [False False False False False False False False False False  True False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : how_busy
pair class : ['how_busy']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [10, 15]
['cancel_reservation', 'oil_change_how', 'tire_change', 'transactions', 'meal_suggestion', 'tell_joke', 'insurance', 'directions', 'change_language', 'shopping_list', 'oil_change_how', 'calendar', 'how_busy', 'plug_type', 'uber', 'how_busy']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0452, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : next_holiday
pair class : ['next_holiday']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['date', 'pay_bill', 'where_are_you_from', 'plug_type', 'cancel', 'goodbye', 'gas_type', 'international_fees', 'transfer', 'timer', 'meeting_schedule', 'insurance_change', 'next_holiday', 'translate', 'cook_time', 'next_holiday']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.3885, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [9]
current labels : sync_device
pair class : ['sync_device']
mask: [False False False False False False False False False  True False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : restaurant_reviews
pair class : ['restaurant_reviews']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [9, 15]
['confirm_reservation', 'cancel_reservation', 'gas', 'whisper_mode', 'reminder', 'vaccines', 'sync_device', 'schedule_meeting', 'find_phone', 'sync_device', 'improve_credit_score', 'income', 'restaurant_reviews', 'definition', 'time', 'restaurant_reviews']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0397, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : todo_list
pair class : ['todo_list']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['how_old_are_you', 'calories', 'insurance', 'insurance_change', 'routing', 'find_phone', 'food_last', 'travel_suggestion', 'pto_request', 'change_user_name', 'text', 'todo_list', 'accept_reservations', 'alarm', 'are_you_a_bot', 'todo_list']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.3225, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : international_visa
pair class : ['international_visa']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['ingredients_list', 'reset_settings', 'where_are_you_from', 'what_are_your_hobbies', 'international_visa', 'reminder', 'lost_luggage', 'pto_used', 'travel_notification', 'find_phone', 'pto_balance', 'income', 'confirm_reservation', 'bill_balance', 'definition', 'international_visa']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4000, grad_fn=<DivBackward0>)

[11,    30] loss_total: 41.204 loss_supervised_contrasive:  11.258 loss_intent :2255.449 
skip_time: 0
total : 500
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [13]
current labels : cook_time
pair class : ['cook_time']
mask: [False False False False False False False False False False False False
 False  True False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [12]
current labels : expiration_date
pair class : ['expiration_date']
mask: [False False False False False False False False False False False False
  True False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : fun_fact
pair class : ['fun_fact']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [13, 12, 15]
['cook_time', 'improve_credit_score', 'find_phone', 'where_are_you_from', 'transfer', 'tire_pressure', 'cancel_reservation', 'expiration_date', 'thank_you', 'book_flight', 'weather', 'fun_fact', 'expiration_date', 'cook_time', 'application_status', 'fun_fact']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.0865, grad_fn=<DivBackward0>)

----
masking label debug : ['translate' 'translate']
current labels  translate
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 4
current skips : [ 9 15]
current labels : translate
pair class : ['translate' 'translate']
mask: [False False False False False False False False False  True False False
 False False False  True]
count: 16
numbers of pairs one label : 2
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [9, 15]
['play_music', 'rollover_401k', 'apr', 'carry_on', 'translate', 'nutrition_info', 'transactions', 'schedule_maintenance', 'date', 'translate', 'accept_reservations', 'no', 'pin_change', 'vaccines', 'exchange_rate', 'translate']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0355, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [11]
current labels : maybe
pair class : ['maybe']
mask: [False False False False False False False False False False False  True
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [13]
current labels : repeat
pair class : ['repeat']
mask: [False False False False False False False False False False False False
 False  True False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : jump_start
pair class : ['jump_start']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [11, 13, 15]
['maybe', 'repeat', 'change_ai_name', 'alarm', 'date', 'redeem_rewards', 'todo_list', 'pto_request', 'jump_start', 'w2', 'travel_suggestion', 'maybe', 'who_made_you', 'repeat', 'distance', 'jump_start']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.1043, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [15]
current labels : todo_list_update
pair class : ['todo_list_update']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['accept_reservations', 'jump_start', 'order_checks', 'todo_list_update', 'thank_you', 'freeze_account', 'plug_type', 'make_call', 'who_made_you', 'change_volume', 'update_playlist', 'alarm', 'transfer', 'how_busy', 'distance', 'todo_list_update']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4827, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : uber
pair class : ['uber']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['car_rental', 'repeat', 'who_do_you_work_for', 'pay_bill', 'timer', 'nutrition_info', 'direct_deposit', 'change_speed', 'time', 'play_music', 'how_old_are_you', 'credit_limit', 'uber', 'credit_score', 'international_fees', 'uber']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.3851, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [8]
current labels : traffic
pair class : ['traffic']
mask: [False False False False False False False False  True False False False
 False False False False]
count: 16
numbers of pairs one label : 1
----
masking label debug : ['greeting' 'greeting']
current labels  greeting
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 2
current skips : [ 5 15]
current labels : greeting
pair class : ['greeting' 'greeting']
mask: [False False False False False  True False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 2
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [8, 5, 15]
['order', 'traffic', 'greeting', 'food_last', 'todo_list', 'greeting', 'exchange_rate', 'play_music', 'traffic', 'insurance', 'carry_on', 'replacement_card_duration', 'update_playlist', 'oil_change_when', 'change_accent', 'greeting']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.0924, grad_fn=<DivBackward0>)

----
masking label debug : ['do_you_have_pets' 'do_you_have_pets']
current labels  do_you_have_pets
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 2
current skips : [5 8]
current labels : do_you_have_pets
pair class : ['do_you_have_pets' 'do_you_have_pets']
mask: [False False False False False  True False False  True False False False
 False False False False]
count: 16
numbers of pairs one label : 2
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [15]
current labels : calendar
pair class : ['calendar']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [12]
current labels : ingredient_substitution
pair class : ['ingredient_substitution']
mask: [False False False False False False False False False False False False
  True False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 4
pairs see from labels :  4
All skippings : [5, 8, 15, 12]
['todo_list_update', 'new_card', 'do_you_have_pets', 'greeting', 'restaurant_reviews', 'do_you_have_pets', 'calendar', 'spelling', 'do_you_have_pets', 'ingredient_substitution', 'gas', 'international_visa', 'ingredient_substitution', 'cancel', 'pto_used', 'calendar']
---------------------------------------------
concatenate got h_i : torch.Size([4, 768])
concatenate got h_j :  torch.Size([4, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([4, 1])
pos_sim.shape : torch.Size([4])
len(neg) : 4
loss_s_cl: tensor(0.6699, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : are_you_a_bot
pair class : ['are_you_a_bot']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['smart_home', 'translate', 'payday', 'damaged_card', 'who_made_you', 'redeem_rewards', 'next_song', 'min_payment', 'timezone', 'tire_pressure', 'calculator', 'are_you_a_bot', 'time', 'what_is_your_name', 'tire_change', 'are_you_a_bot']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.2754, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [15]
current labels : share_location
pair class : ['share_location']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['book_flight', 'book_hotel', 'change_ai_name', 'next_holiday', 'w2', 'share_location', 'time', 'flight_status', 'smart_home', 'improve_credit_score', 'transactions', 'recipe', 'payday', 'vaccines', 'pay_bill', 'share_location']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4617, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [15]
current labels : what_are_your_hobbies
pair class : ['what_are_your_hobbies']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [9]
current labels : roll_dice
pair class : ['roll_dice']
mask: [False False False False False False False False False  True False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [13]
current labels : restaurant_reservation
pair class : ['restaurant_reservation']
mask: [False False False False False False False False False False False False
 False  True False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [15, 9, 13]
['update_playlist', 'text', 'mpg', 'what_are_your_hobbies', 'nutrition_info', 'insurance', 'schedule_meeting', 'roll_dice', 'min_payment', 'roll_dice', 'restaurant_reservation', 'pin_change', 'change_volume', 'restaurant_reservation', 'credit_limit_change', 'what_are_your_hobbies']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.0734, grad_fn=<DivBackward0>)

[11,    40] loss_total: 40.456 loss_supervised_contrasive:  14.125 loss_intent :3007.237 
skip_time: 0
total : 510
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : tire_change
pair class : ['tire_change']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['travel_notification', 'change_volume', 'credit_score', 'freeze_account', 'todo_list', 'calculator', 'restaurant_reservation', 'change_accent', 'gas', 'schedule_maintenance', 'report_lost_card', 'improve_credit_score', 'shopping_list', 'meeting_schedule', 'tire_change', 'tire_change']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4841, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : order_checks
pair class : ['order_checks']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['maybe', 'meeting_schedule', 'interest_rate', 'car_rental', 'balance', 'damaged_card', 'change_language', 'what_is_your_name', 'order_checks', 'change_user_name', 'who_do_you_work_for', 'min_payment', 'cancel', 'shopping_list_update', 'international_visa', 'order_checks']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5392, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [15]
current labels : what_can_i_ask_you
pair class : ['what_can_i_ask_you']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['definition', 'plug_type', 'order_status', 'what_can_i_ask_you', 'current_location', 'goodbye', 'timezone', 'directions', 'mpg', 'jump_start', 'transactions', 'w2', 'change_speed', 'next_song', 'direct_deposit', 'what_can_i_ask_you']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4635, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : food_last
pair class : ['food_last']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['reminder', 'directions', 'tell_joke', 'replacement_card_duration', 'tire_pressure', 'yes', 'application_status', 'order', 'text', 'weather', 'international_visa', 'change_speed', 'food_last', 'restaurant_reservation', 'what_can_i_ask_you', 'food_last']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4056, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : change_ai_name
pair class : ['change_ai_name']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [12]
current labels : what_can_i_ask_you
pair class : ['what_can_i_ask_you']
mask: [False False False False False False False False False False False False
  True False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 12]
['expiration_date', 'card_declined', 'change_ai_name', 'what_can_i_ask_you', 'no', 'last_maintenance', 'redeem_rewards', 'what_are_your_hobbies', 'change_volume', 'spelling', 'order', 'share_location', 'what_can_i_ask_you', 'reset_settings', 'routing', 'change_ai_name']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0578, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : directions
pair class : ['directions']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['roll_dice', 'schedule_maintenance', 'cancel_reservation', 'pto_request_status', 'directions', 'how_busy', 'tire_change', 'distance', 'application_status', 'reminder', 'direct_deposit', 'card_declined', 'oil_change_how', 'what_is_your_name', 'nutrition_info', 'directions']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4460, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [13]
current labels : travel_alert
pair class : ['travel_alert']
mask: [False False False False False False False False False False False False
 False  True]
count: 14
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [13]
['change_user_name', 'bill_balance', 'no', 'application_status', 'roll_dice', 'taxes', 'maybe', 'direct_deposit', 'vaccines', 'confirm_reservation', 'shopping_list_update', 'travel_alert', 'next_song', 'travel_alert']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([14, 14])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.1447, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [8]
current labels : spelling
pair class : ['spelling']
mask: [False False False False False False False False  True False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : meaning_of_life
pair class : ['meaning_of_life']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [8, 15]
['pto_request_status', 'cancel_reservation', 'uber', 'change_ai_name', 'routing', 'smart_home', 'definition', 'spelling', 'spelling', 'meaning_of_life', 'greeting', 'traffic', 'reset_settings', 'international_fees', 'goodbye', 'meaning_of_life']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0515, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : calories
pair class : ['calories']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['distance', 'change_language', 'sync_device', 'ingredient_substitution', 'do_you_have_pets', 'what_song', 'yes', 'income', 'confirm_reservation', 'timer', 'car_rental', 'lost_luggage', 'calendar_update', 'whisper_mode', 'calories', 'calories']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.3850, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [6]
current labels : order_status
pair class : ['order_status']
mask: [False False False False False False  True False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : calendar
pair class : ['calendar']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [6, 15]
['order_status', 'interest_rate', 'calendar', 'next_holiday', 'w2', 'bill_balance', 'order_status', 'accept_reservations', 'alarm', 'play_music', 'international_visa', 'next_song', 'insurance', 'oil_change_how', 'who_do_you_work_for', 'calendar']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0297, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : what_are_your_hobbies
pair class : ['what_are_your_hobbies']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [11]
current labels : credit_score
pair class : ['credit_score']
mask: [False False False False False False False False False False False  True
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 11]
['greeting', 'application_status', 'what_are_your_hobbies', 'credit_score', 'pin_change', 'smart_home', 'last_maintenance', 'credit_limit_change', 'meeting_schedule', 'what_is_your_name', 'damaged_card', 'credit_score', 'how_old_are_you', 'pto_request', 'restaurant_suggestion', 'what_are_your_hobbies']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0266, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [15]
current labels : flight_status
pair class : ['flight_status']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['what_is_your_name', 'w2', 'credit_limit_change', 'roll_dice', 'traffic', 'text', 'timer', 'who_made_you', 'calendar_update', 'date', 'flight_status', 'vaccines', 'weather', 'shopping_list_update', 'flip_coin', 'flight_status']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4217, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [15]
current labels : greeting
pair class : ['greeting']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [14]
current labels : book_flight
pair class : ['book_flight']
mask: [False False False False False False False False False False False False
 False False  True False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 14]
['are_you_a_bot', 'meal_suggestion', 'travel_alert', 'text', 'find_phone', 'greeting', 'expiration_date', 'travel_notification', 'book_flight', 'lost_luggage', 'repeat', 'tire_change', 'direct_deposit', 'what_can_i_ask_you', 'book_flight', 'greeting']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0329, grad_fn=<DivBackward0>)

----
masking label debug : ['date' 'date']
current labels  date
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 4
current skips : [ 9 15]
current labels : date
pair class : ['date' 'date']
mask: [False False False False False False False False False  True False False
 False False False  True]
count: 16
numbers of pairs one label : 2
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [9, 15]
['new_card', 'pto_balance', 'card_declined', 'bill_balance', 'date', 'bill_due', 'transfer', 'do_you_have_pets', 'definition', 'date', 'update_playlist', 'car_rental', 'flip_coin', 'change_volume', 'rollover_401k', 'date']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0599, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [15]
current labels : tire_change
pair class : ['tire_change']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['tire_pressure', 'cancel_reservation', 'travel_notification', 'balance', 'change_ai_name', 'income', 'tire_change', 'food_last', 'ingredients_list', 'order', 'accept_reservations', 'gas', 'text', 'reminder_update', 'transactions', 'tire_change']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4566, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : calculator
pair class : ['calculator']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['transactions', 'calculator', 'how_old_are_you', 'jump_start', 'translate', 'mpg', 'fun_fact', 'confirm_reservation', 'make_call', 'freeze_account', 'lost_luggage', 'update_playlist', 'schedule_meeting', 'oil_change_how', 'recipe', 'calculator']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4643, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : maybe
pair class : ['maybe']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['what_song', 'last_maintenance', 'payday', 'min_payment', 'directions', 'current_location', 'lost_luggage', 'how_old_are_you', 'change_speed', 'improve_credit_score', 'credit_score', 'change_volume', 'todo_list_update', 'carry_on', 'maybe', 'maybe']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5090, grad_fn=<DivBackward0>)

[12,    10] loss_total: 41.333 loss_supervised_contrasive:  3.744 loss_intent :751.787 
skip_time: 0
total : 527
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [15]
current labels : interest_rate
pair class : ['interest_rate']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['interest_rate', 'confirm_reservation', 'pto_used', 'rewards_balance', 'ingredients_list', 'change_ai_name', 'what_is_your_name', 'mpg', 'translate', 'travel_notification', 'text', 'user_name', 'routing', 'calculator', 'pto_request_status', 'interest_rate']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4477, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : cancel
pair class : ['cancel']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['vaccines', 'change_accent', 'thank_you', 'gas_type', 'directions', 'alarm', 'gas', 'exchange_rate', 'redeem_rewards', 'bill_due', 'flight_status', 'restaurant_reviews', 'cancel', 'meaning_of_life', 'who_do_you_work_for', 'cancel']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4446, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : measurement_conversion
pair class : ['measurement_conversion']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['taxes', 'tell_joke', 'schedule_maintenance', 'oil_change_when', 'update_playlist', 'travel_alert', 'bill_due', 'alarm', 'routing', 'share_location', 'book_hotel', 'travel_suggestion', 'measurement_conversion', 'what_is_your_name', 'car_rental', 'measurement_conversion']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.3055, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [8]
current labels : smart_home
pair class : ['smart_home']
mask: [False False False False False False False False  True False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : uber
pair class : ['uber']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [8, 15]
['smart_home', 'uber', 'reset_settings', 'pto_request', 'user_name', 'oil_change_how', 'ingredient_substitution', 'todo_list', 'smart_home', 'replacement_card_duration', 'measurement_conversion', 'calories', 'income', 'sync_device', 'damaged_card', 'uber']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0450, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : timezone
pair class : ['timezone']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['do_you_have_pets', 'alarm', 'thank_you', 'change_speed', 'account_blocked', 'restaurant_reviews', 'order', 'restaurant_suggestion', 'credit_limit', 'todo_list', 'current_location', 'what_song', 'timezone', 'balance', 'rewards_balance', 'timezone']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4208, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [15]
current labels : what_can_i_ask_you
pair class : ['what_can_i_ask_you']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['definition', 'expiration_date', 'book_hotel', 'shopping_list', 'next_song', 'what_can_i_ask_you', 'damaged_card', 'pin_change', 'transactions', 'next_holiday', 'greeting', 'food_last', 'cancel_reservation', 'travel_alert', 'roll_dice', 'what_can_i_ask_you']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4483, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : freeze_account
pair class : ['freeze_account']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['who_made_you', 'schedule_meeting', 'insurance_change', 'credit_limit', 'recipe', 'gas', 'distance', 'redeem_rewards', 'no', 'restaurant_reviews', 'maybe', 'freeze_account', 'application_status', 'sync_device', 'direct_deposit', 'freeze_account']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4390, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : todo_list_update
pair class : ['todo_list_update']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['redeem_rewards', 'tire_pressure', 'who_made_you', 'shopping_list', 'todo_list_update', 'meeting_schedule', 'reminder', 'nutrition_info', 'directions', 'change_language', 'next_holiday', 'payday', 'card_declined', 'weather', 'damaged_card', 'todo_list_update']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4884, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [10]
current labels : report_lost_card
pair class : ['report_lost_card']
mask: [False False False False False False False False False False  True False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [15]
current labels : cook_time
pair class : ['cook_time']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [10, 15]
['order', 'report_lost_card', 'flight_status', 'transfer', 'jump_start', 'meal_suggestion', 'meeting_schedule', 'cook_time', 'shopping_list', 'calories', 'report_lost_card', 'spending_history', 'what_can_i_ask_you', 'card_declined', 'direct_deposit', 'cook_time']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0232, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [12]
current labels : shopping_list_update
pair class : ['shopping_list_update']
mask: [False False False False False False False False False False False False
  True False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [15]
current labels : insurance
pair class : ['insurance']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [12, 15]
['pto_used', 'where_are_you_from', 'yes', 'calculator', 'nutrition_info', 'carry_on', 'todo_list', 'cancel_reservation', 'shopping_list_update', 'user_name', 'insurance', 'account_blocked', 'shopping_list_update', 'spending_history', 'apr', 'insurance']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0031, grad_fn=<DivBackward0>)

[12,    20] loss_total: 41.996 loss_supervised_contrasive:  8.150 loss_intent :1503.571 
skip_time: 0
total : 537
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : travel_alert
pair class : ['travel_alert']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['schedule_maintenance', 'timer', 'do_you_have_pets', 'reminder_update', 'travel_alert', 'are_you_a_bot', 'order_checks', 'spelling', 'reminder', 'balance', 'gas', 'gas_type', 'play_music', 'international_visa', 'timezone', 'travel_alert']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4889, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [11]
current labels : report_fraud
pair class : ['report_fraud']
mask: [False False False False False False False False False False False  True
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 13
current skips : [15]
current labels : application_status
pair class : ['application_status']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [11, 15]
['order', 'meal_suggestion', 'flight_status', 'last_maintenance', 'pto_request', 'oil_change_how', 'report_fraud', 'account_blocked', 'translate', 'directions', 'accept_reservations', 'report_fraud', 'share_location', 'application_status', 'update_playlist', 'application_status']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0501, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [15]
current labels : todo_list_update
pair class : ['todo_list_update']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [9]
current labels : next_song
pair class : ['next_song']
mask: [False False False False False False False False False  True False False
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 9]
['whisper_mode', 'freeze_account', 'min_payment', 'rollover_401k', 'calories', 'mpg', 'ingredient_substitution', 'todo_list_update', 'next_song', 'next_song', 'pto_balance', 'roll_dice', 'how_busy', 'insurance_change', 'travel_notification', 'todo_list_update']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0117, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [15]
current labels : find_phone
pair class : ['find_phone']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['tire_pressure', 'reminder', 'improve_credit_score', 'find_phone', 'repeat', 'credit_limit', 'ingredients_list', 'insurance', 'bill_due', 'yes', 'goodbye', 'order_status', 'time', 'maybe', 'reminder_update', 'find_phone']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5342, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [15]
current labels : make_call
pair class : ['make_call']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [7]
current labels : pin_change
pair class : ['pin_change']
mask: [False False False False False False False  True False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 7]
['reset_settings', 'cook_time', 'pto_request_status', 'make_call', 'travel_suggestion', 'apr', 'pin_change', 'pin_change', 'how_old_are_you', 'schedule_maintenance', 'whisper_mode', 'expiration_date', 'order_status', 'weather', 'rollover_401k', 'make_call']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0461, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [15]
current labels : application_status
pair class : ['application_status']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['taxes', 'accept_reservations', 'share_location', 'account_blocked', 'update_playlist', 'replacement_card_duration', 'shopping_list_update', 'application_status', 'change_volume', 'restaurant_reviews', 'calculator', 'date', 'carry_on', 'improve_credit_score', 'freeze_account', 'application_status']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4419, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [15]
current labels : maybe
pair class : ['maybe']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['maybe', 'todo_list_update', 'find_phone', 'freeze_account', 'change_user_name', 'expiration_date', 'how_busy', 'pay_bill', 'payday', 'restaurant_suggestion', 'flight_status', 'transfer', 'new_card', 'gas_type', 'application_status', 'maybe']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4449, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [10]
current labels : spending_history
pair class : ['spending_history']
mask: [False False False False False False False False False False  True False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [15]
current labels : cancel_reservation
pair class : ['cancel_reservation']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [7]
current labels : pay_bill
pair class : ['pay_bill']
mask: [False False False False False False False  True False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [10, 15, 7]
['mpg', 'spending_history', 'what_are_your_hobbies', 'cancel_reservation', 'pay_bill', 'goodbye', 'traffic', 'pay_bill', 'play_music', 'oil_change_how', 'spending_history', 'meeting_schedule', 'find_phone', 'travel_alert', 'card_declined', 'cancel_reservation']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.1127, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [10]
current labels : vaccines
pair class : ['vaccines']
mask: [False False False False False False False False False False  True False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [5]
current labels : fun_fact
pair class : ['fun_fact']
mask: [False False False False False  True False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [15]
current labels : are_you_a_bot
pair class : ['are_you_a_bot']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [10, 5, 15]
['bill_balance', 'vaccines', 'change_language', 'gas', 'fun_fact', 'fun_fact', 'book_flight', 'are_you_a_bot', 'schedule_maintenance', 'what_song', 'vaccines', 'what_can_i_ask_you', 'user_name', 'routing', 'where_are_you_from', 'are_you_a_bot']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.0862, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [15]
current labels : change_user_name
pair class : ['change_user_name']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['report_lost_card', 'tire_pressure', 'plug_type', 'roll_dice', 'no', 'payday', 'change_user_name', 'shopping_list_update', 'apr', 'restaurant_reservation', 'calendar', 'goodbye', 'time', 'distance', 'pto_request', 'change_user_name']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4293, grad_fn=<DivBackward0>)

[12,    30] loss_total: 41.153 loss_supervised_contrasive:  11.715 loss_intent :2255.349 
skip_time: 0
total : 547
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [13]
current labels : what_are_your_hobbies
pair class : ['what_are_your_hobbies']
mask: [False False False False False False False False False False False False
 False  True False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : car_rental
pair class : ['car_rental']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [13, 15]
['travel_notification', 'travel_suggestion', 'next_song', 'insurance_change', 'confirm_reservation', 'share_location', 'pto_request', 'cancel', 'what_are_your_hobbies', 'change_accent', 'redeem_rewards', 'shopping_list', 'ingredients_list', 'what_are_your_hobbies', 'car_rental', 'car_rental']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0546, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : todo_list
pair class : ['todo_list']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['new_card', 'pto_request_status', 'spelling', 'gas_type', 'oil_change_when', 'schedule_meeting', 'restaurant_reservation', 'whisper_mode', 'jump_start', 'roll_dice', 'order_checks', 'bill_balance', 'todo_list', 'min_payment', 'time', 'todo_list']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4495, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [12]
current labels : tire_change
pair class : ['tire_change']
mask: [False False False False False False False False False False False False
  True False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : apr
pair class : ['apr']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [13]
current labels : current_location
pair class : ['current_location']
mask: [False False False False False False False False False False False False
 False  True False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [12, 15, 13]
['rewards_balance', 'tire_change', 'book_hotel', 'cook_time', 'uber', 'last_maintenance', 'calendar_update', 'pto_balance', 'report_fraud', 'apr', 'current_location', 'calculator', 'tire_change', 'current_location', 'who_made_you', 'apr']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.1011, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : change_speed
pair class : ['change_speed']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['transfer', 'accept_reservations', 'share_location', 'credit_limit', 'current_location', 'make_call', 'tell_joke', 'where_are_you_from', 'no', 'change_speed', 'card_declined', 'car_rental', 'rollover_401k', 'sync_device', 'translate', 'change_speed']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5152, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [12]
current labels : interest_rate
pair class : ['interest_rate']
mask: [False False False False False False False False False False False False
  True False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [11]
current labels : meaning_of_life
pair class : ['meaning_of_life']
mask: [False False False False False False False False False False False  True
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [15]
current labels : reminder_update
pair class : ['reminder_update']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [12, 11, 15]
['pto_used', 'interest_rate', 'meaning_of_life', 'shopping_list', 'measurement_conversion', 'pto_request_status', 'reminder_update', 'change_ai_name', 'pay_bill', 'what_can_i_ask_you', 'where_are_you_from', 'meaning_of_life', 'interest_rate', 'restaurant_reservation', 'plug_type', 'reminder_update']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.0992, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [15]
current labels : replacement_card_duration
pair class : ['replacement_card_duration']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['book_flight', 'travel_suggestion', 'jump_start', 'w2', 'how_busy', 'lost_luggage', 'balance', 'replacement_card_duration', 'oil_change_when', 'fun_fact', 'yes', 'ingredient_substitution', 'redeem_rewards', 'gas_type', 'pay_bill', 'replacement_card_duration']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4174, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : order_status
pair class : ['order_status']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['next_holiday', 'cancel', 'distance', 'damaged_card', 'directions', 'mpg', 'who_do_you_work_for', 'how_old_are_you', 'smart_home', 'todo_list_update', 'recipe', 'order_status', 'yes', 'food_last', 'tell_joke', 'order_status']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5512, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : time
pair class : ['time']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['what_song', 'account_blocked', 'calendar', 'do_you_have_pets', 'reminder', 'transactions', 'schedule_meeting', 'taxes', 'credit_limit_change', 'credit_limit', 'carry_on', 'change_language', 'time', 'cancel', 'exchange_rate', 'time']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5028, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [4]
current labels : recipe
pair class : ['recipe']
mask: [False False False False  True False False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [15]
current labels : tire_change
pair class : ['tire_change']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [4, 15]
['repeat', 'recipe', 'calendar', 'maybe', 'recipe', 'restaurant_suggestion', 'timezone', 'what_is_your_name', 'pin_change', 'nutrition_info', 'tire_change', 'schedule_meeting', 'restaurant_reservation', 'ingredients_list', 'distance', 'tire_change']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0499, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [15]
current labels : order
pair class : ['order']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['play_music', 'bill_due', 'timezone', 'reset_settings', 'weather', 'tell_joke', 'nutrition_info', 'order', 'w2', 'where_are_you_from', 'insurance_change', 'traffic', 'change_user_name', 'payday', 'thank_you', 'order']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5153, grad_fn=<DivBackward0>)

[12,    40] loss_total: 41.517 loss_supervised_contrasive:  15.641 loss_intent :3007.169 
skip_time: 0
total : 557
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [15]
current labels : change_accent
pair class : ['change_accent']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [10]
current labels : credit_score
pair class : ['credit_score']
mask: [False False False False False False False False False False  True False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [14]
current labels : international_fees
pair class : ['international_fees']
mask: [False False False False False False False False False False False False
 False False  True False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [15, 10, 14]
['make_call', 'pto_used', 'oil_change_when', 'calories', 'todo_list', 'traffic', 'who_made_you', 'change_accent', 'report_lost_card', 'credit_score', 'credit_score', 'international_fees', 'spending_history', 'spelling', 'international_fees', 'change_accent']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.0924, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : text
pair class : ['text']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['travel_suggestion', 'time', 'text', 'plug_type', 'meeting_schedule', 'report_fraud', 'are_you_a_bot', 'timer', 'taxes', 'interest_rate', 'balance', 'alarm', 'confirm_reservation', 'cook_time', 'greeting', 'text']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4397, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [11]
current labels : uber
pair class : ['uber']
mask: [False False False False False False False False False False False  True
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [15]
current labels : exchange_rate
pair class : ['exchange_rate']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [10]
current labels : international_fees
pair class : ['international_fees']
mask: [False False False False False False False False False False  True False
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [11, 15, 10]
['jump_start', 'uber', 'change_language', 'meaning_of_life', 'play_music', 'who_do_you_work_for', 'exchange_rate', 'reminder_update', 'international_fees', 'date', 'international_fees', 'uber', 'repeat', 'replacement_card_duration', 'rewards_balance', 'exchange_rate']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.0904, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : goodbye
pair class : ['goodbye']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [13]
current labels : no
pair class : ['no']
mask: [False False False False False False False False False False False False
 False  True False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 13]
['user_name', 'goodbye', 'no', 'flip_coin', 'reminder', 'order_checks', 'food_last', 'apr', 'international_visa', 'change_speed', 'ingredient_substitution', 'min_payment', 'whisper_mode', 'no', 'timezone', 'goodbye']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0649, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [15]
current labels : transfer
pair class : ['transfer']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['pto_used', 'exchange_rate', 'what_are_your_hobbies', 'order_checks', 'are_you_a_bot', 'income', 'improve_credit_score', 'transfer', 'plug_type', 'food_last', 'nutrition_info', 'flip_coin', 'rewards_balance', 'routing', 'change_volume', 'transfer']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4770, grad_fn=<DivBackward0>)

----
masking label debug : ['insurance' 'insurance']
current labels  insurance
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 8
current skips : [10 15]
current labels : insurance
pair class : ['insurance' 'insurance']
mask: [False False False False False False False False False False  True False
 False False False  True]
count: 16
numbers of pairs one label : 2
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [10, 15]
['w2', 'restaurant_reservation', 'insurance_change', 'change_ai_name', 'book_hotel', 'report_fraud', 'thank_you', 'calendar_update', 'insurance', 'meal_suggestion', 'insurance', 'credit_limit_change', 'repeat', 'last_maintenance', 'report_lost_card', 'insurance']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0296, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [13]
current labels : plug_type
pair class : ['plug_type']
mask: [False False False False False False False False False False False False
 False  True]
count: 14
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [13]
['definition', 'direct_deposit', 'restaurant_suggestion', 'change_speed', 'meal_suggestion', 'improve_credit_score', 'new_card', 'book_flight', 'tell_joke', 'expiration_date', 'book_hotel', 'plug_type', 'cancel', 'plug_type']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([14, 14])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.2523, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [7]
current labels : flight_status
pair class : ['flight_status']
mask: [False False False False False False False  True False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [13]
current labels : thank_you
pair class : ['thank_you']
mask: [False False False False False False False False False False False False
 False  True False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [15]
current labels : exchange_rate
pair class : ['exchange_rate']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [7, 13, 15]
['flight_status', 'order', 'translate', 'expiration_date', 'change_accent', 'report_fraud', 'spending_history', 'flight_status', 'thank_you', 'reminder_update', 'exchange_rate', 'plug_type', 'do_you_have_pets', 'thank_you', 'min_payment', 'exchange_rate']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.0937, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : flight_status
pair class : ['flight_status']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['uber', 'smart_home', 'translate', 'weather', 'definition', 'sync_device', 'ingredients_list', 'report_fraud', 'time', 'flight_status', 'user_name', 'oil_change_when', 'calendar', 'share_location', 'replacement_card_duration', 'flight_status']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4709, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [15]
current labels : flip_coin
pair class : ['flip_coin']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['payday', 'routing', 'date', 'greeting', 'recipe', 'flip_coin', 'calculator', 'rollover_401k', 'maybe', 'ingredient_substitution', 'pto_request', 'pin_change', 'spending_history', 'freeze_account', 'oil_change_how', 'flip_coin']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4575, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [10]
current labels : update_playlist
pair class : ['update_playlist']
mask: [False False False False False False False False False False  True False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [13]
current labels : vaccines
pair class : ['vaccines']
mask: [False False False False False False False False False False False False
 False  True False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : change_ai_name
pair class : ['change_ai_name']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [10, 13, 15]
['routing', 'pto_used', 'account_blocked', 'update_playlist', 'todo_list', 'goodbye', 'confirm_reservation', 'direct_deposit', 'timezone', 'play_music', 'update_playlist', 'vaccines', 'change_ai_name', 'vaccines', 'schedule_maintenance', 'change_ai_name']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.0932, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [15]
current labels : transactions
pair class : ['transactions']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['international_fees', 'todo_list_update', 'restaurant_reservation', 'transactions', 'next_holiday', 'pto_balance', 'spelling', 'rewards_balance', 'no', 'change_language', 'gas_type', 'calories', 'pto_request_status', 'find_phone', 'who_made_you', 'transactions']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4890, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [13]
current labels : alarm
pair class : ['alarm']
mask: [False False False False False False False False False False False False
 False  True False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : card_declined
pair class : ['card_declined']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [13, 15]
['are_you_a_bot', 'credit_limit_change', 'international_visa', 'payday', 'alarm', 'flip_coin', 'redeem_rewards', 'change_language', 'greeting', 'rollover_401k', 'book_hotel', 'card_declined', 'weather', 'alarm', 'update_playlist', 'card_declined']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0412, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [15]
current labels : translate
pair class : ['translate']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['timezone', 'order_checks', 'next_song', 'share_location', 'exchange_rate', 'cook_time', 'translate', 'change_user_name', 'travel_alert', 'gas', 'date', 'do_you_have_pets', 'transfer', 'what_song', 'where_are_you_from', 'translate']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4426, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [12]
current labels : insurance_change
pair class : ['insurance_change']
mask: [False False False False False False False False False False False False
  True False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : mpg
pair class : ['mpg']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [12, 15]
['book_flight', 'what_can_i_ask_you', 'reset_settings', 'who_do_you_work_for', 'tire_pressure', 'meaning_of_life', 'what_are_your_hobbies', 'how_old_are_you', 'insurance_change', 'mpg', 'oil_change_how', 'calendar', 'insurance_change', 'cancel_reservation', 'change_speed', 'mpg']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0347, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [9]
current labels : min_payment
pair class : ['min_payment']
mask: [False False False False False False False False False  True False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [10]
current labels : calendar
pair class : ['calendar']
mask: [False False False False False False False False False False  True False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [15]
current labels : gas
pair class : ['gas']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [9, 10, 15]
['carry_on', 'min_payment', 'flip_coin', 'schedule_meeting', 'freeze_account', 'calendar', 'travel_alert', 'gas', 'what_can_i_ask_you', 'min_payment', 'calendar', 'change_speed', 'order_checks', 'uber', 'shopping_list', 'gas']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.1111, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [15]
current labels : schedule_meeting
pair class : ['schedule_meeting']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['improve_credit_score', 'application_status', 'change_speed', 'confirm_reservation', 'shopping_list', 'recipe', 'change_user_name', 'food_last', 'directions', 'flip_coin', 'schedule_meeting', 'play_music', 'pto_balance', 'lost_luggage', 'restaurant_suggestion', 'schedule_meeting']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5308, grad_fn=<DivBackward0>)

[13,    10] loss_total: 41.066 loss_supervised_contrasive:  3.476 loss_intent :751.783 
skip_time: 0
total : 574
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [10]
current labels : ingredients_list
pair class : ['ingredients_list']
mask: [False False False False False False False False False False  True False
 False False False False]
count: 16
numbers of pairs one label : 1
----
masking label debug : ['travel_notification' 'travel_notification']
current labels  travel_notification
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 6
current skips : [11 15]
current labels : travel_notification
pair class : ['travel_notification' 'travel_notification']
mask: [False False False False False False False False False False False  True
 False False False  True]
count: 16
numbers of pairs one label : 2
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [10, 11, 15]
['last_maintenance', 'calculator', 'ingredients_list', 'travel_alert', 'reset_settings', 'what_is_your_name', 'travel_notification', 'international_fees', 'change_ai_name', 'order_status', 'ingredients_list', 'travel_notification', 'who_do_you_work_for', 'pto_request', 'traffic', 'travel_notification']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.0968, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : routing
pair class : ['routing']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['pin_change', 'w2', 'carry_on', 'traffic', 'routing', 'nutrition_info', 'fun_fact', 'sync_device', 'reminder_update', 'make_call', 'cook_time', 'next_holiday', 'jump_start', 'confirm_reservation', 'credit_score', 'routing']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5131, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [11]
current labels : account_blocked
pair class : ['account_blocked']
mask: [False False False False False False False False False False False  True
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 13
current skips : [15]
current labels : shopping_list
pair class : ['shopping_list']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [11, 15]
['plug_type', 'next_song', 'pto_request_status', 'redeem_rewards', 'transfer', 'account_blocked', 'apr', 'change_accent', 'find_phone', 'date', 'vaccines', 'account_blocked', 'insurance', 'shopping_list', 'travel_notification', 'shopping_list']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0424, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [15]
current labels : do_you_have_pets
pair class : ['do_you_have_pets']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [11]
current labels : improve_credit_score
pair class : ['improve_credit_score']
mask: [False False False False False False False False False False False  True
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 11]
['do_you_have_pets', 'improve_credit_score', 'cancel', 'gas_type', 'book_hotel', 'balance', 'accept_reservations', 'report_fraud', 'tire_pressure', 'repeat', 'gas', 'improve_credit_score', 'restaurant_reviews', 'definition', 'next_song', 'do_you_have_pets']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0329, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : no
pair class : ['no']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['direct_deposit', 'pto_balance', 'time', 'tire_change', 'distance', 'oil_change_when', 'restaurant_suggestion', 'timezone', 'what_are_your_hobbies', 'roll_dice', 'mpg', 'repeat', 'no', 'play_music', 'oil_change_how', 'no']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4878, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [10]
current labels : how_busy
pair class : ['how_busy']
mask: [False False False False False False False False False False  True False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : text
pair class : ['text']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [10, 15]
['timer', 'how_busy', 'text', 'calories', 'apr', 'jump_start', 'meeting_schedule', 'reset_settings', 'book_flight', 'w2', 'how_busy', 'application_status', 'credit_limit_change', 'recipe', 'international_fees', 'text']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0466, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : thank_you
pair class : ['thank_you']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['jump_start', 'next_song', 'sync_device', 'cancel_reservation', 'pto_request', 'oil_change_when', 'mpg', 'food_last', 'order_checks', 'transactions', 'user_name', 'goodbye', 'thank_you', 'travel_alert', 'bill_balance', 'thank_you']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5455, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [15]
current labels : international_visa
pair class : ['international_visa']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['cancel', 'international_fees', 'insurance', 'book_flight', 'apr', 'international_visa', 'order', 'credit_limit', 'restaurant_reservation', 'sync_device', 'tell_joke', 'calendar_update', 'meaning_of_life', 'how_busy', 'replacement_card_duration', 'international_visa']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4643, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : spelling
pair class : ['spelling']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['change_ai_name', 'change_accent', 'how_old_are_you', 'change_volume', 'transfer', 'calculator', 'bill_due', 'how_busy', 'nutrition_info', 'greeting', 'fun_fact', 'pin_change', 'min_payment', 'bill_balance', 'spelling', 'spelling']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4559, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [11]
current labels : redeem_rewards
pair class : ['redeem_rewards']
mask: [False False False False False False False False False False False  True
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : report_lost_card
pair class : ['report_lost_card']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [11, 15]
['application_status', 'carry_on', 'plug_type', 'pto_balance', 'w2', 'schedule_maintenance', 'make_call', 'tire_change', 'redeem_rewards', 'directions', 'reset_settings', 'redeem_rewards', 'report_lost_card', 'direct_deposit', 'what_are_your_hobbies', 'report_lost_card']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0705, grad_fn=<DivBackward0>)

[13,    20] loss_total: 41.266 loss_supervised_contrasive:  7.152 loss_intent :1503.583 
skip_time: 0
total : 584
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : book_flight
pair class : ['book_flight']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['where_are_you_from', 'timer', 'accept_reservations', 'report_lost_card', 'freeze_account', 'order', 'todo_list_update', 'order_status', 'ingredient_substitution', 'pay_bill', 'report_fraud', 'book_flight', 'exchange_rate', 'bill_balance', 'improve_credit_score', 'book_flight']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4645, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [15]
current labels : sync_device
pair class : ['sync_device']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [9]
current labels : weather
pair class : ['weather']
mask: [False False False False False False False False False  True False False
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 9]
['sync_device', 'what_are_your_hobbies', 'reset_settings', 'weather', 'current_location', 'tire_pressure', 'food_last', 'repeat', 'what_is_your_name', 'weather', 'pay_bill', 'cancel', 'calories', 'tire_change', 'user_name', 'sync_device']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0533, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : shopping_list
pair class : ['shopping_list']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['timer', 'restaurant_reviews', 'cook_time', 'distance', 'reminder_update', 'bill_due', 'alarm', 'change_speed', 'shopping_list', 'make_call', 'expiration_date', 'how_old_are_you', 'book_hotel', 'insurance', 'current_location', 'shopping_list']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5263, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [15]
current labels : maybe
pair class : ['maybe']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [12]
current labels : shopping_list_update
pair class : ['shopping_list_update']
mask: [False False False False False False False False False False False False
  True False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 12]
['maybe', 'income', 'where_are_you_from', 'current_location', 'mpg', 'expiration_date', 'shopping_list', 'greeting', 'shopping_list_update', 'measurement_conversion', 'definition', 'traffic', 'shopping_list_update', 'tell_joke', 'user_name', 'maybe']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0296, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [15]
current labels : whisper_mode
pair class : ['whisper_mode']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [13]
current labels : gas_type
pair class : ['gas_type']
mask: [False False False False False False False False False False False False
 False  True False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [8]
current labels : card_declined
pair class : ['card_declined']
mask: [False False False False False False False False  True False False False
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [15, 13, 8]
['whisper_mode', 'rewards_balance', 'do_you_have_pets', 'gas_type', 'card_declined', 'direct_deposit', 'translate', 'make_call', 'card_declined', 'reminder', 'flip_coin', 'who_made_you', 'schedule_meeting', 'gas_type', 'calendar_update', 'whisper_mode']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.0895, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [7]
current labels : meeting_schedule
pair class : ['meeting_schedule']
mask: [False False False False False False False  True False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 13
current skips : [15]
current labels : insurance_change
pair class : ['insurance_change']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [7, 15]
['what_song', 'new_card', 'replacement_card_duration', 'greeting', 'book_hotel', 'meeting_schedule', 'timezone', 'meeting_schedule', 'international_visa', 'income', 'cancel', 'recipe', 'text', 'insurance_change', 'meal_suggestion', 'insurance_change']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0419, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [10]
current labels : lost_luggage
pair class : ['lost_luggage']
mask: [False False False False False False False False False False  True False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : restaurant_reservation
pair class : ['restaurant_reservation']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [10, 15]
['no', 'yes', 'pto_request_status', 'spelling', 'jump_start', 'change_language', 'what_are_your_hobbies', 'thank_you', 'lost_luggage', 'pto_used', 'lost_luggage', 'restaurant_reservation', 'flight_status', 'directions', 'redeem_rewards', 'restaurant_reservation']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0600, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [9]
current labels : new_card
pair class : ['new_card']
mask: [False False False False False False False False False  True False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : whisper_mode
pair class : ['whisper_mode']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [9, 15]
['income', 'recipe', 'text', 'change_language', 'meal_suggestion', 'gas_type', 'next_song', 'new_card', 'payday', 'new_card', 'insurance_change', 'food_last', 'goodbye', 'rewards_balance', 'whisper_mode', 'whisper_mode']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0458, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [6]
current labels : taxes
pair class : ['taxes']
mask: [False False False False False False  True False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : payday
pair class : ['payday']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [6, 15]
['taxes', 'pto_request', 'pto_request_status', 'change_volume', 'where_are_you_from', 'spending_history', 'taxes', 'maybe', 'balance', 'payday', 'international_fees', 'order_status', 'schedule_maintenance', 'report_lost_card', 'timer', 'payday']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0611, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [14]
current labels : balance
pair class : ['balance']
mask: [False False False False False False False False False False False False
 False False  True False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [15]
current labels : book_hotel
pair class : ['book_hotel']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [14, 15]
['rewards_balance', 'how_old_are_you', 'distance', 'tire_change', 'bill_balance', 'credit_limit', 'damaged_card', 'yes', 'balance', 'pto_used', 'book_hotel', 'accept_reservations', 'income', 'no', 'balance', 'book_hotel']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0765, grad_fn=<DivBackward0>)

[13,    30] loss_total: 40.234 loss_supervised_contrasive:  9.797 loss_intent :2255.373 
skip_time: 0
total : 594
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : calories
pair class : ['calories']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [14]
current labels : todo_list
pair class : ['todo_list']
mask: [False False False False False False False False False False False False
 False False  True False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [11]
current labels : measurement_conversion
pair class : ['measurement_conversion']
mask: [False False False False False False False False False False False  True
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [15, 14, 11]
['share_location', 'calories', 'translate', 'todo_list', 'measurement_conversion', 'meaning_of_life', 'credit_limit', 'pin_change', 'roll_dice', 'calculator', 'min_payment', 'measurement_conversion', 'taxes', 'current_location', 'todo_list', 'calories']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.0793, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [9]
current labels : damaged_card
pair class : ['damaged_card']
mask: [False False False False False False False False False  True False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : whisper_mode
pair class : ['whisper_mode']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [9, 15]
['last_maintenance', 'definition', 'application_status', 'damaged_card', 'meal_suggestion', 'travel_notification', 'distance', 'change_user_name', 'tell_joke', 'damaged_card', 'routing', 'date', 'cook_time', 'car_rental', 'whisper_mode', 'whisper_mode']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0484, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : reminder_update
pair class : ['reminder_update']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['directions', 'oil_change_how', 'application_status', 'order', 'smart_home', 'meal_suggestion', 'nutrition_info', 'w2', 'reminder', 'pay_bill', 'car_rental', 'spelling', 'timezone', 'are_you_a_bot', 'reminder_update', 'reminder_update']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4892, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [13]
current labels : traffic
pair class : ['traffic']
mask: [False False False False False False False False False False False False
 False  True False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : todo_list_update
pair class : ['todo_list_update']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [13, 15]
['traffic', 'credit_score', 'weather', 'flight_status', 'transfer', 'plug_type', 'bill_balance', 'what_is_your_name', 'pto_used', 'calendar_update', 'restaurant_reservation', 'travel_suggestion', 'time', 'traffic', 'todo_list_update', 'todo_list_update']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0479, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [13]
current labels : interest_rate
pair class : ['interest_rate']
mask: [False False False False False False False False False False False False
 False  True False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [15]
current labels : credit_score
pair class : ['credit_score']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [13, 15]
['damaged_card', 'are_you_a_bot', 'interest_rate', 'credit_score', 'restaurant_reviews', 'uber', 'whisper_mode', 'measurement_conversion', 'restaurant_suggestion', 'tire_pressure', 'improve_credit_score', 'rollover_401k', 'change_ai_name', 'interest_rate', 'cancel_reservation', 'credit_score']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0407, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [15]
current labels : time
pair class : ['time']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['rewards_balance', 'pto_request', 'change_accent', 'oil_change_when', 'bill_due', 'credit_score', 'car_rental', 'time', 'accept_reservations', 'pto_request_status', 'who_do_you_work_for', 'maybe', 'roll_dice', 'travel_notification', 'last_maintenance', 'time']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5107, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [12]
current labels : fun_fact
pair class : ['fun_fact']
mask: [False False False False False False False False False False False False
  True False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 13
current skips : [15]
current labels : transactions
pair class : ['transactions']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [12, 15]
['vaccines', 'alarm', 'fun_fact', 'next_holiday', 'international_visa', 'report_fraud', 'last_maintenance', 'change_language', 'roll_dice', 'reminder_update', 'plug_type', 'where_are_you_from', 'fun_fact', 'transactions', 'order_status', 'transactions']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0501, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : distance
pair class : ['distance']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['pto_used', 'reminder', 'insurance_change', 'what_can_i_ask_you', 'change_volume', 'roll_dice', 'what_song', 'expiration_date', 'timer', 'bill_due', 'find_phone', 'distance', 'next_holiday', 'yes', 'oil_change_when', 'distance']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5369, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [9]
current labels : freeze_account
pair class : ['freeze_account']
mask: [False False False False False False False False False  True False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : repeat
pair class : ['repeat']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [9, 15]
['book_flight', 'ingredient_substitution', 'new_card', 'shopping_list_update', 'international_visa', 'what_can_i_ask_you', 'no', 'insurance', 'freeze_account', 'freeze_account', 'lost_luggage', 'repeat', 'smart_home', 'ingredients_list', 'schedule_meeting', 'repeat']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0378, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : taxes
pair class : ['taxes']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['find_phone', 'confirm_reservation', 'whisper_mode', 'definition', 'calendar_update', 'play_music', 'carry_on', 'todo_list_update', 'spelling', 'restaurant_suggestion', 'transactions', 'shopping_list_update', 'schedule_maintenance', 'insurance', 'taxes', 'taxes']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4677, grad_fn=<DivBackward0>)

[13,    40] loss_total: 40.920 loss_supervised_contrasive:  13.128 loss_intent :3007.161 
skip_time: 0
total : 604
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [15]
current labels : tell_joke
pair class : ['tell_joke']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['tire_change', 'pto_balance', 'pay_bill', 'tell_joke', 'meaning_of_life', 'gas', 'restaurant_reservation', 'account_blocked', 'make_call', 'travel_alert', 'travel_suggestion', 'rollover_401k', 'goodbye', 'update_playlist', 'time', 'tell_joke']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4683, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [15]
current labels : todo_list_update
pair class : ['todo_list_update']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [10]
current labels : who_made_you
pair class : ['who_made_you']
mask: [False False False False False False False False False False  True False
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 10]
['balance', 'account_blocked', 'measurement_conversion', 'todo_list_update', 'who_made_you', 'uber', 'text', 'pay_bill', 'jump_start', 'schedule_maintenance', 'who_made_you', 'meaning_of_life', 'directions', 'food_last', 'damaged_card', 'todo_list_update']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0373, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [15]
current labels : what_is_your_name
pair class : ['what_is_your_name']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['change_user_name', 'what_song', 'cancel_reservation', 'card_declined', 'lost_luggage', 'what_is_your_name', 'maybe', 'schedule_meeting', 'new_card', 'restaurant_suggestion', 'report_lost_card', 'tell_joke', 'cook_time', 'change_ai_name', 'exchange_rate', 'what_is_your_name']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4755, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [15]
current labels : last_maintenance
pair class : ['last_maintenance']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['reminder', 'order_status', 'how_busy', 'order', 'order_checks', 'date', 'update_playlist', 'transactions', 'yes', 'who_do_you_work_for', 'last_maintenance', 'exchange_rate', 'credit_limit', 'todo_list', 'confirm_reservation', 'last_maintenance']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4830, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [15]
current labels : user_name
pair class : ['user_name']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['user_name', 'w2', 'routing', 'pin_change', 'calendar', 'direct_deposit', 'what_can_i_ask_you', 'change_speed', 'do_you_have_pets', 'next_holiday', 'accept_reservations', 'oil_change_how', 'nutrition_info', 'travel_suggestion', 'credit_score', 'user_name']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5045, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : repeat
pair class : ['repeat']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['change_volume', 'taxes', 'goodbye', 'bill_due', 'play_music', 'order_checks', 'meeting_schedule', 'rollover_401k', 'income', 'travel_suggestion', 'mpg', 'what_song', 'alarm', 'interest_rate', 'repeat', 'repeat']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5557, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [13]
current labels : find_phone
pair class : ['find_phone']
mask: [False False False False False False False False False False False False
 False  True]
count: 14
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [13]
['change_volume', 'nutrition_info', 'meeting_schedule', 'who_do_you_work_for', 'how_old_are_you', 'todo_list', 'restaurant_reviews', 'cancel_reservation', 'find_phone', 'shopping_list_update', 'spending_history', 'smart_home', 'calendar_update', 'find_phone']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([14, 14])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.2259, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [15]
current labels : food_last
pair class : ['food_last']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['direct_deposit', 'lost_luggage', 'redeem_rewards', 'restaurant_reservation', 'gas_type', 'spelling', 'food_last', 'do_you_have_pets', 'pto_request', 'restaurant_reviews', 'bill_balance', 'pay_bill', 'traffic', 'schedule_meeting', 'pto_balance', 'food_last']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5172, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [13]
current labels : pto_request
pair class : ['pto_request']
mask: [False False False False False False False False False False False False
 False  True False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [15]
current labels : make_call
pair class : ['make_call']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [13, 15]
['pto_request', 'application_status', 'improve_credit_score', 'todo_list_update', 'payday', 'oil_change_how', 'whisper_mode', 'make_call', 'change_language', 'tire_change', 'flight_status', 'international_visa', 'transactions', 'pto_request', 'restaurant_reservation', 'make_call']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0732, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : change_user_name
pair class : ['change_user_name']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['recipe', 'timezone', 'balance', 'maybe', 'change_user_name', 'credit_limit_change', 'exchange_rate', 'whisper_mode', 'pto_balance', 'schedule_maintenance', 'thank_you', 'spending_history', 'play_music', 'oil_change_how', 'transfer', 'change_user_name']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5045, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [1]
current labels : distance
pair class : ['distance']
mask: [False  True False False False False False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
----
masking label debug : ['order_checks' 'order_checks']
current labels  order_checks
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 6
current skips : [ 9 15]
current labels : order_checks
pair class : ['order_checks' 'order_checks']
mask: [False False False False False False False False False  True False False
 False False False  True]
count: 16
numbers of pairs one label : 2
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [1, 9, 15]
['distance', 'distance', 'international_fees', 'next_song', 'travel_alert', 'tire_pressure', 'order_checks', 'fun_fact', 'account_blocked', 'order_checks', 'vaccines', 'rewards_balance', 'gas_type', 'user_name', 'who_do_you_work_for', 'order_checks']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.0917, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : distance
pair class : ['distance']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['last_maintenance', 'text', 'thank_you', 'pin_change', 'redeem_rewards', 'ingredient_substitution', 'next_holiday', 'who_do_you_work_for', 'shopping_list', 'roll_dice', 'pto_used', 'calendar', 'fun_fact', 'order', 'distance', 'distance']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4586, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [11]
current labels : change_language
pair class : ['change_language']
mask: [False False False False False False False False False False False  True
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [8]
current labels : tire_pressure
pair class : ['tire_pressure']
mask: [False False False False False False False False  True False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [15]
current labels : reminder_update
pair class : ['reminder_update']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [11, 8, 15]
['share_location', 'accept_reservations', 'insurance_change', 'calendar_update', 'change_language', 'tire_pressure', 'bill_balance', 'goodbye', 'tire_pressure', 'calories', 'reminder_update', 'change_language', 'confirm_reservation', 'translate', 'travel_suggestion', 'reminder_update']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.1178, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : todo_list_update
pair class : ['todo_list_update']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['nutrition_info', 'freeze_account', 'time', 'expiration_date', 'change_user_name', 'change_volume', 'change_speed', 'rewards_balance', 'alarm', 'do_you_have_pets', 'how_busy', 'todo_list_update', 'traffic', 'mpg', 'insurance', 'todo_list_update']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4549, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : w2
pair class : ['w2']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['tire_change', 'direct_deposit', 'interest_rate', 'travel_suggestion', 'change_volume', 'carry_on', 'reminder_update', 'book_hotel', 'insurance_change', 'calendar_update', 'who_do_you_work_for', 'cook_time', 'what_song', 'credit_limit', 'w2', 'w2']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4364, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : transactions
pair class : ['transactions']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['goodbye', 'change_accent', 'book_hotel', 'who_made_you', 'travel_alert', 'time', 'schedule_maintenance', 'insurance', 'bill_balance', 'insurance_change', 'credit_limit', 'where_are_you_from', 'transactions', 'damaged_card', 'alarm', 'transactions']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5157, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [14]
current labels : smart_home
pair class : ['smart_home']
mask: [False False False False False False False False False False False False
 False False  True False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 13
current skips : [15]
current labels : meeting_schedule
pair class : ['meeting_schedule']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [14, 15]
['pay_bill', 'do_you_have_pets', 'international_visa', 'distance', 'change_accent', 'smart_home', 'pto_request_status', 'measurement_conversion', 'cancel_reservation', 'weather', 'exchange_rate', 'oil_change_when', 'lost_luggage', 'meeting_schedule', 'smart_home', 'meeting_schedule']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0968, grad_fn=<DivBackward0>)

[14,    10] loss_total: 41.515 loss_supervised_contrasive:  3.927 loss_intent :751.775 
skip_time: 0
total : 621
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [15]
current labels : shopping_list
pair class : ['shopping_list']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['fun_fact', 'uber', 'meeting_schedule', 'reset_settings', 'lost_luggage', 'report_lost_card', 'shopping_list', 'cancel', 'what_is_your_name', 'directions', 'order_status', 'change_ai_name', 'meaning_of_life', 'balance', 'change_user_name', 'shopping_list']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5130, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [15]
current labels : meal_suggestion
pair class : ['meal_suggestion']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['meal_suggestion', 'what_are_your_hobbies', 'spelling', 'food_last', 'date', 'bill_due', 'application_status', 'income', 'insurance_change', 'travel_notification', 'meaning_of_life', 'rollover_401k', 'card_declined', 'payday', 'schedule_meeting', 'meal_suggestion']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5182, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : roll_dice
pair class : ['roll_dice']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['restaurant_reviews', 'credit_score', 'roll_dice', 'nutrition_info', 'plug_type', 'cancel', 'next_holiday', 'w2', 'cancel_reservation', 'bill_balance', 'report_fraud', 'last_maintenance', 'pay_bill', 'measurement_conversion', 'timer', 'roll_dice']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4783, grad_fn=<DivBackward0>)

----
masking label debug : ['current_location' 'current_location']
current labels  current_location
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 6
current skips : [ 8 12]
current labels : current_location
pair class : ['current_location' 'current_location']
mask: [False False False False False False False False  True False False False
  True False False False]
count: 16
numbers of pairs one label : 2
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : accept_reservations
pair class : ['accept_reservations']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [8, 12, 15]
['repeat', 'income', 'text', 'last_maintenance', 'calendar_update', 'smart_home', 'current_location', 'rollover_401k', 'current_location', 'rewards_balance', 'ingredient_substitution', 'pto_request_status', 'current_location', 'what_can_i_ask_you', 'accept_reservations', 'accept_reservations']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.1074, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : spending_history
pair class : ['spending_history']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [14]
current labels : interest_rate
pair class : ['interest_rate']
mask: [False False False False False False False False False False False False
 False False  True False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 14]
['oil_change_how', 'shopping_list_update', 'what_is_your_name', 'are_you_a_bot', 'spending_history', 'application_status', 'calories', 'change_speed', 'report_lost_card', 'income', 'interest_rate', 'change_ai_name', 'repeat', 'freeze_account', 'interest_rate', 'spending_history']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0698, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : confirm_reservation
pair class : ['confirm_reservation']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['meaning_of_life', 'yes', 'card_declined', 'goodbye', 'transactions', 'min_payment', 'user_name', 'insurance', 'ingredient_substitution', 'calendar', 'travel_suggestion', 'confirm_reservation', 'recipe', 'greeting', 'account_blocked', 'confirm_reservation']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5153, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : car_rental
pair class : ['car_rental']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['min_payment', 'international_fees', 'spending_history', 'credit_score', 'flip_coin', 'tell_joke', 'what_can_i_ask_you', 'exchange_rate', 'jump_start', 'how_busy', 'calculator', 'w2', 'car_rental', 'redeem_rewards', 'routing', 'car_rental']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4868, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [15]
current labels : schedule_meeting
pair class : ['schedule_meeting']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['reset_settings', 'whisper_mode', 'order', 'schedule_meeting', 'ingredients_list', 'distance', 'order_status', 'confirm_reservation', 'tire_pressure', 'pin_change', 'balance', 'shopping_list', 'calculator', 'shopping_list_update', 'meeting_schedule', 'schedule_meeting']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5505, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [15]
current labels : how_old_are_you
pair class : ['how_old_are_you']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['how_old_are_you', 'user_name', 'date', 'travel_suggestion', 'rewards_balance', 'accept_reservations', 'who_made_you', 'carry_on', 'greeting', 'nutrition_info', 'pto_request', 'traffic', 'no', 'cancel', 'share_location', 'how_old_are_you']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4938, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [3]
current labels : next_song
pair class : ['next_song']
mask: [False False False  True False False False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : play_music
pair class : ['play_music']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [3, 15]
['next_song', 'weather', 'share_location', 'next_song', 'new_card', 'gas', 'timezone', 'find_phone', 'definition', 'uber', 'spelling', 'todo_list_update', 'play_music', 'plug_type', 'next_holiday', 'play_music']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0562, grad_fn=<DivBackward0>)

[14,    20] loss_total: 41.969 loss_supervised_contrasive:  8.306 loss_intent :1503.567 
skip_time: 0
total : 631
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : report_fraud
pair class : ['report_fraud']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['travel_alert', 'report_fraud', 'rollover_401k', 'restaurant_suggestion', 'schedule_maintenance', 'tire_change', 'maybe', 'pto_used', 'who_made_you', 'carry_on', 'credit_limit_change', 'meal_suggestion', 'order', 'pto_balance', 'thank_you', 'report_fraud']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5134, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [13]
current labels : routing
pair class : ['routing']
mask: [False False False False False False False False False False False False
 False  True False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [15]
current labels : transfer
pair class : ['transfer']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [13, 15]
['recipe', 'share_location', 'ingredients_list', 'change_user_name', 'routing', 'order_status', 'report_fraud', 'improve_credit_score', 'jump_start', 'make_call', 'transfer', 'change_volume', 'bill_due', 'routing', 'directions', 'transfer']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0586, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : taxes
pair class : ['taxes']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['fun_fact', 'uber', 'taxes', 'shopping_list', 'international_fees', 'book_hotel', 'confirm_reservation', 'restaurant_reviews', 'repeat', 'freeze_account', 'time', 'definition', 'pto_balance', 'jump_start', 'routing', 'taxes']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5225, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [15]
current labels : mpg
pair class : ['mpg']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['mpg', 'yes', 'are_you_a_bot', 'spelling', 'restaurant_suggestion', 'time', 'maybe', 'min_payment', 'direct_deposit', 'pto_used', 'what_are_your_hobbies', 'last_maintenance', 'transactions', 'repeat', 'update_playlist', 'mpg']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5130, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : accept_reservations
pair class : ['accept_reservations']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['date', 'international_fees', 'book_flight', 'calories', 'food_last', 'application_status', 'meal_suggestion', 'measurement_conversion', 'accept_reservations', 'replacement_card_duration', 'change_ai_name', 'restaurant_reviews', 'taxes', 'mpg', 'what_are_your_hobbies', 'accept_reservations']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4916, grad_fn=<DivBackward0>)

----
masking label debug : ['make_call' 'make_call']
current labels  make_call
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 7
current skips : [13 15]
current labels : make_call
pair class : ['make_call' 'make_call']
mask: [False False False False False False False False False False False False
 False  True False  True]
count: 16
numbers of pairs one label : 2
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [13, 15]
['oil_change_when', 'replacement_card_duration', 'what_song', 'improve_credit_score', 'direct_deposit', 'text', 'fun_fact', 'make_call', 'redeem_rewards', 'routing', 'transactions', 'credit_limit', 'traffic', 'make_call', 'account_blocked', 'make_call']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0716, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [14]
current labels : roll_dice
pair class : ['roll_dice']
mask: [False False False False False False False False False False False False
 False False  True False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [15]
current labels : update_playlist
pair class : ['update_playlist']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [14, 15]
['roll_dice', 'order', 'current_location', 'how_old_are_you', 'calendar', 'food_last', 'update_playlist', 'cancel_reservation', 'confirm_reservation', 'what_is_your_name', 'oil_change_how', 'travel_notification', 'flight_status', 'report_fraud', 'roll_dice', 'update_playlist']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0435, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [10]
current labels : change_volume
pair class : ['change_volume']
mask: [False False False False False False False False False False  True False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : oil_change_when
pair class : ['oil_change_when']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [10, 15]
['how_old_are_you', 'transfer', 'income', 'credit_limit_change', 'jump_start', 'international_visa', 'insurance', 'nutrition_info', 'vaccines', 'change_volume', 'change_volume', 'oil_change_when', 'update_playlist', 'reset_settings', 'apr', 'oil_change_when']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0393, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [6]
current labels : cancel_reservation
pair class : ['cancel_reservation']
mask: [False False False False False False  True False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [15]
current labels : travel_notification
pair class : ['travel_notification']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [6, 15]
['goodbye', 'vaccines', 'cancel_reservation', 'sync_device', 'change_language', 'calendar', 'cancel_reservation', 'application_status', 'maybe', 'change_accent', 'travel_notification', 'spending_history', 'uber', 'income', 'do_you_have_pets', 'travel_notification']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0623, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [5]
current labels : expiration_date
pair class : ['expiration_date']
mask: [False False False False False  True False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [15]
current labels : replacement_card_duration
pair class : ['replacement_card_duration']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [5, 15]
['maybe', 'expiration_date', 'timezone', 'uber', 'timer', 'expiration_date', 'flip_coin', 'replacement_card_duration', 'schedule_maintenance', 'pin_change', 'pay_bill', 'cancel', 'direct_deposit', 'definition', 'tell_joke', 'replacement_card_duration']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0545, grad_fn=<DivBackward0>)

[14,    30] loss_total: 41.026 loss_supervised_contrasive:  11.743 loss_intent :2255.355 
skip_time: 0
total : 641
----
masking label debug : ['new_card' 'new_card']
current labels  new_card
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 2
current skips : [12 15]
current labels : new_card
pair class : ['new_card' 'new_card']
mask: [False False False False False False False False False False False False
  True False False  True]
count: 16
numbers of pairs one label : 2
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [12, 15]
['translate', 'food_last', 'new_card', 'freeze_account', 'damaged_card', 'balance', 'no', 'card_declined', 'yes', 'weather', 'credit_score', 'gas', 'new_card', 'user_name', 'apr', 'new_card']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0628, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : plug_type
pair class : ['plug_type']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['who_made_you', 'travel_alert', 'tire_pressure', 'credit_limit_change', 'flight_status', 'insurance', 'ingredients_list', 'calories', 'improve_credit_score', 'plug_type', 'expiration_date', 'shopping_list', 'transfer', 'play_music', 'pay_bill', 'plug_type']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4958, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : plug_type
pair class : ['plug_type']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['thank_you', 'shopping_list_update', 'pto_request', 'min_payment', 'report_lost_card', 'directions', 'freeze_account', 'account_blocked', 'international_visa', 'sync_device', 'are_you_a_bot', 'plug_type', 'cook_time', 'bill_due', 'how_old_are_you', 'plug_type']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4780, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [15]
current labels : redeem_rewards
pair class : ['redeem_rewards']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['calculator', 'insurance_change', 'translate', 'find_phone', 'cook_time', 'redeem_rewards', 'date', 'order', 'recipe', 'timer', 'payday', 'alarm', 'change_accent', 'sync_device', 'smart_home', 'redeem_rewards']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5165, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [3]
current labels : todo_list
pair class : ['todo_list']
mask: [False False False  True False False False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : alarm
pair class : ['alarm']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [3, 15]
['todo_list', 'card_declined', 'alarm', 'todo_list', 'schedule_maintenance', 'translate', 'taxes', 'gas', 'oil_change_when', 'restaurant_reservation', 'replacement_card_duration', 'credit_score', 'whisper_mode', 'yes', 'user_name', 'alarm']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0482, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : order_checks
pair class : ['order_checks']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['current_location', 'ingredients_list', 'order_checks', 'share_location', 'directions', 'carry_on', 'credit_limit', 'car_rental', 'book_hotel', 'bill_due', 'reminder', 'smart_home', 'reminder_update', 'todo_list', 'rewards_balance', 'order_checks']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5249, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : roll_dice
pair class : ['roll_dice']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['taxes', 'time', 'reminder', 'who_do_you_work_for', 'interest_rate', 'book_flight', 'account_blocked', 'schedule_meeting', 'expiration_date', 'change_language', 'how_busy', 'find_phone', 'recipe', 'new_card', 'roll_dice', 'roll_dice']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5014, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : greeting
pair class : ['greeting']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['meeting_schedule', 'travel_notification', 'how_busy', 'text', 'weather', 'how_old_are_you', 'what_can_i_ask_you', 'definition', 'tell_joke', 'greeting', 'reminder', 'payday', 'what_are_your_hobbies', 'damaged_card', 'flip_coin', 'greeting']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5279, grad_fn=<DivBackward0>)

----
masking label debug : ['tell_joke' 'tell_joke']
current labels  tell_joke
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 1
current skips : [ 9 15]
current labels : tell_joke
pair class : ['tell_joke' 'tell_joke']
mask: [False False False False False False False False False  True False False
 False False False  True]
count: 16
numbers of pairs one label : 2
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [14]
current labels : gas_type
pair class : ['gas_type']
mask: [False False False False False False False False False False False False
 False False  True False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [9, 15, 14]
['cook_time', 'tell_joke', 'what_song', 'calendar', 'oil_change_when', 'replacement_card_duration', 'apr', 'book_hotel', 'plug_type', 'tell_joke', 'gas_type', 'interest_rate', 'timezone', 'order_checks', 'gas_type', 'tell_joke']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.1030, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : greeting
pair class : ['greeting']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['lost_luggage', 'pin_change', 'greeting', 'alarm', 'balance', 'calculator', 'change_user_name', 'ingredients_list', 'definition', 'measurement_conversion', 'sync_device', 'restaurant_suggestion', 'bill_balance', 'flight_status', 'directions', 'greeting']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5033, grad_fn=<DivBackward0>)

[14,    40] loss_total: 41.964 loss_supervised_contrasive:  16.119 loss_intent :3007.110 
skip_time: 0
total : 651
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [15]
current labels : how_busy
pair class : ['how_busy']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [5]
current labels : update_playlist
pair class : ['update_playlist']
mask: [False False False False False  True False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 5]
['how_busy', 'are_you_a_bot', 'mpg', 'jump_start', 'update_playlist', 'update_playlist', 'car_rental', 'w2', 'repeat', 'calories', 'where_are_you_from', 'next_song', 'timer', 'cancel', 'order_checks', 'how_busy']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0552, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : report_lost_card
pair class : ['report_lost_card']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['reminder', 'report_lost_card', 'credit_score', 'restaurant_reviews', 'pto_balance', 'rollover_401k', 'are_you_a_bot', 'pto_used', 'book_flight', 'international_visa', 'no', 'flip_coin', 'reset_settings', 'reminder_update', 'todo_list_update', 'report_lost_card']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4868, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : exchange_rate
pair class : ['exchange_rate']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['goodbye', 'yes', 'oil_change_how', 'change_speed', 'credit_limit', 'card_declined', 'find_phone', 'where_are_you_from', 'cook_time', 'change_ai_name', 'rollover_401k', 'exchange_rate', 'travel_notification', 'travel_alert', 'what_is_your_name', 'exchange_rate']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4311, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [10]
current labels : meaning_of_life
pair class : ['meaning_of_life']
mask: [False False False False False False False False False False  True False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [15]
current labels : todo_list
pair class : ['todo_list']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [13]
current labels : order_status
pair class : ['order_status']
mask: [False False False False False False False False False False False False
 False  True False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [10, 15, 13]
['change_ai_name', 'restaurant_reservation', 'meaning_of_life', 'vaccines', 'where_are_you_from', 'todo_list', 'gas_type', 'play_music', 'order_status', 'last_maintenance', 'meaning_of_life', 'meeting_schedule', 'reminder', 'order_status', 'shopping_list_update', 'todo_list']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.0943, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : whisper_mode
pair class : ['whisper_mode']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['schedule_meeting', 'next_song', 'what_song', 'reminder_update', 'pto_request_status', 'pto_used', 'timezone', 'damaged_card', 'calculator', 'next_holiday', 'calendar_update', 'no', 'whisper_mode', 'timer', 'thank_you', 'whisper_mode']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5336, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : greeting
pair class : ['greeting']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['accept_reservations', 'book_flight', 'tire_change', 'todo_list_update', 'restaurant_suggestion', 'carry_on', 'reset_settings', 'change_speed', 'apr', 'calendar_update', 'mpg', 'greeting', 'lost_luggage', 'traffic', 'car_rental', 'greeting']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5084, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [13]
current labels : translate
pair class : ['translate']
mask: [False False False False False False False False False False False False
 False  True]
count: 14
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [13]
['do_you_have_pets', 'bill_due', 'report_fraud', 'vaccines', 'text', 'restaurant_suggestion', 'report_lost_card', 'improve_credit_score', 'todo_list', 'what_song', 'translate', 'transfer', 'meal_suggestion', 'translate']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([14, 14])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.2460, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [15]
current labels : transactions
pair class : ['transactions']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [8]
current labels : roll_dice
pair class : ['roll_dice']
mask: [False False False False False False False False  True False False False
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 8]
['report_fraud', 'book_flight', 'cancel_reservation', 'transactions', 'roll_dice', 'recipe', 'tire_pressure', 'translate', 'roll_dice', 'text', 'insurance_change', 'reminder_update', 'w2', 'next_song', 'what_are_your_hobbies', 'transactions']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0571, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : ingredients_list
pair class : ['ingredients_list']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['sync_device', 'improve_credit_score', 'travel_suggestion', 'gas', 'flip_coin', 'pay_bill', 'make_call', 'tell_joke', 'change_speed', 'bill_due', 'whisper_mode', 'restaurant_suggestion', 'ingredients_list', 'next_holiday', 'new_card', 'ingredients_list']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5113, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 13
current skips : [15]
current labels : improve_credit_score
pair class : ['improve_credit_score']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['exchange_rate', 'who_do_you_work_for', 'pto_request_status', 'uber', 'pto_used', 'change_ai_name', 'what_is_your_name', 'pin_change', 'report_fraud', 'todo_list_update', 'meal_suggestion', 'travel_suggestion', 'whisper_mode', 'improve_credit_score', 'transfer', 'improve_credit_score']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5101, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [15]
current labels : jump_start
pair class : ['jump_start']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['change_language', 'bill_due', 'next_song', 'payday', 'freeze_account', 'jump_start', 'pto_request', 'credit_score', 'play_music', 'date', 'travel_notification', 'tire_pressure', 'transfer', 'change_user_name', 'calculator', 'jump_start']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5462, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : greeting
pair class : ['greeting']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['book_flight', 'balance', 'bill_balance', 'plug_type', 'calendar', 'order_status', 'todo_list', 'interest_rate', 'international_fees', 'application_status', 'time', 'greeting', 'report_lost_card', 'credit_limit', 'insurance', 'greeting']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5359, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [13]
current labels : last_maintenance
pair class : ['last_maintenance']
mask: [False False False False False False False False False False False False
 False  True False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [15]
current labels : cancel_reservation
pair class : ['cancel_reservation']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [13, 15]
['travel_alert', 'direct_deposit', 'yes', 'meeting_schedule', 'last_maintenance', 'ingredients_list', 'cancel_reservation', 'fun_fact', 'insurance_change', 'reminder_update', 'current_location', 'what_are_your_hobbies', 'travel_notification', 'last_maintenance', 'calories', 'cancel_reservation']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0554, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [15]
current labels : routing
pair class : ['routing']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['make_call', 'alarm', 'rollover_401k', 'routing', 'reminder_update', 'travel_notification', 'w2', 'bill_due', 'travel_alert', 'order_checks', 'ingredient_substitution', 'next_holiday', 'car_rental', 'tire_pressure', 'timezone', 'routing']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4721, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [15]
current labels : shopping_list_update
pair class : ['shopping_list_update']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['are_you_a_bot', 'new_card', 'play_music', 'next_song', 'how_old_are_you', 'credit_limit_change', 'shopping_list_update', 'distance', 'car_rental', 'transactions', 'date', 'oil_change_how', 'shopping_list', 'directions', 'tire_change', 'shopping_list_update']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4631, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [13]
current labels : schedule_meeting
pair class : ['schedule_meeting']
mask: [False False False False False False False False False False False False
 False  True False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [15]
current labels : cancel
pair class : ['cancel']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [13, 15]
['tire_pressure', 'schedule_meeting', 'how_old_are_you', 'next_holiday', 'accept_reservations', 'how_busy', 'weather', 'cancel', 'directions', 'no', 'spelling', 'shopping_list_update', 'repeat', 'schedule_meeting', 'account_blocked', 'cancel']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0719, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : bill_balance
pair class : ['bill_balance']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['direct_deposit', 'bill_balance', 'spelling', 'ingredient_substitution', 'sync_device', 'next_song', 'application_status', 'share_location', 'accept_reservations', 'routing', 'damaged_card', 'rewards_balance', 'carry_on', 'card_declined', 'account_blocked', 'bill_balance']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5259, grad_fn=<DivBackward0>)

[15,    10] loss_total: 42.063 loss_supervised_contrasive:  4.475 loss_intent :751.759 
skip_time: 0
total : 668
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [15]
current labels : oil_change_when
pair class : ['oil_change_when']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['expiration_date', 'report_lost_card', 'maybe', 'vaccines', 'directions', 'smart_home', 'cancel_reservation', 'oil_change_when', 'flip_coin', 'thank_you', 'uber', 'transactions', 'account_blocked', 'meeting_schedule', 'timezone', 'oil_change_when']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5408, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : change_user_name
pair class : ['change_user_name']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['todo_list', 'what_are_your_hobbies', 'international_fees', 'yes', 'change_user_name', 'routing', 'plug_type', 'meal_suggestion', 'translate', 'spending_history', 'make_call', 'insurance_change', 'order_status', 'traffic', 'weather', 'change_user_name']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4904, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [4]
current labels : timezone
pair class : ['timezone']
mask: [False False False False  True False False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [8]
current labels : oil_change_when
pair class : ['oil_change_when']
mask: [False False False False False False False False  True False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [15]
current labels : credit_score
pair class : ['credit_score']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [4, 8, 15]
['change_accent', 'timezone', 'schedule_maintenance', 'bill_due', 'timezone', 'oil_change_when', 'credit_score', 'book_flight', 'oil_change_when', 'gas_type', 'ingredients_list', 'expiration_date', 'nutrition_info', 'apr', 'yes', 'credit_score']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.0970, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : traffic
pair class : ['traffic']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['confirm_reservation', 'book_flight', 'reset_settings', 'min_payment', 'change_volume', 'travel_notification', 'timezone', 'measurement_conversion', 'cook_time', 'fun_fact', 'insurance', 'traffic', 'gas', 'spelling', 'translate', 'traffic']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4640, grad_fn=<DivBackward0>)

----
masking label debug : ['exchange_rate' 'exchange_rate']
current labels  exchange_rate
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 0
current skips : [6 7]
current labels : exchange_rate
pair class : ['exchange_rate' 'exchange_rate']
mask: [False False False False False False  True  True False False False False
 False False False False]
count: 16
numbers of pairs one label : 2
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [15]
current labels : cancel
pair class : ['cancel']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [6, 7, 15]
['exchange_rate', 'travel_suggestion', 'thank_you', 'calendar_update', 'no', 'reminder', 'exchange_rate', 'exchange_rate', 'jump_start', 'transfer', 'cancel', 'tell_joke', 'measurement_conversion', 'maybe', 'what_are_your_hobbies', 'cancel']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.1004, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 13
current skips : [15]
current labels : update_playlist
pair class : ['update_playlist']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['gas', 'jump_start', 'goodbye', 'current_location', 'next_song', 'pin_change', 'plug_type', 'report_lost_card', 'share_location', 'whisper_mode', 'meal_suggestion', 'date', 'restaurant_suggestion', 'update_playlist', 'pto_used', 'update_playlist']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4892, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [10]
current labels : calendar_update
pair class : ['calendar_update']
mask: [False False False False False False False False False False  True False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : taxes
pair class : ['taxes']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [10, 15]
['date', 'traffic', 'calendar_update', 'interest_rate', 'spending_history', 'repeat', 'change_accent', 'new_card', 'car_rental', 'travel_alert', 'calendar_update', 'taxes', 'tell_joke', 'are_you_a_bot', 'tire_change', 'taxes']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0582, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : recipe
pair class : ['recipe']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['pto_request', 'application_status', 'improve_credit_score', 'card_declined', 'damaged_card', 'timer', 'flight_status', 'accept_reservations', 'recipe', 'order_checks', 'reset_settings', 'roll_dice', 'pay_bill', 'alarm', 'flip_coin', 'recipe']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5063, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : smart_home
pair class : ['smart_home']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['ingredient_substitution', 'calculator', 'greeting', 'schedule_maintenance', 'expiration_date', 'book_hotel', 'sync_device', 'restaurant_reservation', 'routing', 'smart_home', 'credit_limit_change', 'share_location', 'time', 'goodbye', 'car_rental', 'smart_home']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5425, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : confirm_reservation
pair class : ['confirm_reservation']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['pto_used', 'flight_status', 'food_last', 'oil_change_when', 'gas', 'ingredients_list', 'lost_luggage', 'play_music', 'uber', 'international_fees', 'roll_dice', 'sync_device', 'confirm_reservation', 'change_language', 'alarm', 'confirm_reservation']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4730, grad_fn=<DivBackward0>)

[15,    20] loss_total: 41.866 loss_supervised_contrasive:  8.751 loss_intent :1503.551 
skip_time: 0
total : 678
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [15]
current labels : timer
pair class : ['timer']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['timer', 'directions', 'flight_status', 'no', 'how_old_are_you', 'mpg', 'replacement_card_duration', 'who_do_you_work_for', 'plug_type', 'change_speed', 'credit_limit_change', 'measurement_conversion', 'cancel', 'translate', 'meaning_of_life', 'timer']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5101, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : food_last
pair class : ['food_last']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['next_holiday', 'who_do_you_work_for', 'what_can_i_ask_you', 'fun_fact', 'who_made_you', 'distance', 'vaccines', 'cancel_reservation', 'where_are_you_from', 'application_status', 'greeting', 'confirm_reservation', 'book_hotel', 'freeze_account', 'food_last', 'food_last']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4859, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [10]
current labels : maybe
pair class : ['maybe']
mask: [False False False False False False False False False False  True False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : change_accent
pair class : ['change_accent']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [10, 15]
['fun_fact', 'order_status', 'change_language', 'yes', 'maybe', 'find_phone', 'timer', 'repeat', 'restaurant_reservation', 'translate', 'maybe', 'change_accent', 'calendar', 'min_payment', 'where_are_you_from', 'change_accent']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0568, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [15]
current labels : goodbye
pair class : ['goodbye']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['order_status', 'definition', 'what_song', 'transfer', 'shopping_list_update', 'calories', 'change_ai_name', 'goodbye', 'meal_suggestion', 'pto_request', 'min_payment', 'direct_deposit', 'book_hotel', 'change_accent', 'order', 'goodbye']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5185, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : alarm
pair class : ['alarm']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['order_checks', 'rewards_balance', 'last_maintenance', 'timer', 'user_name', 'replacement_card_duration', 'mpg', 'meeting_schedule', 'alarm', 'taxes', 'who_made_you', 'restaurant_suggestion', 'report_fraud', 'how_busy', 'shopping_list', 'alarm']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5012, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : international_visa
pair class : ['international_visa']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['do_you_have_pets', 'distance', 'income', 'flip_coin', 'report_fraud', 'schedule_maintenance', 'change_language', 'restaurant_reviews', 'redeem_rewards', 'taxes', 'change_user_name', 'change_ai_name', 'apr', 'next_holiday', 'international_visa', 'international_visa']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5061, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [15]
current labels : alarm
pair class : ['alarm']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['greeting', 'uber', 'oil_change_how', 'ingredient_substitution', 'how_busy', 'damaged_card', 'find_phone', 'smart_home', 'oil_change_when', 'improve_credit_score', 'alarm', 'meeting_schedule', 'apr', 'taxes', 'income', 'alarm']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4673, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [15]
current labels : card_declined
pair class : ['card_declined']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['order_status', 'carry_on', 'jump_start', 'what_can_i_ask_you', 'oil_change_how', 'date', 'user_name', 'travel_notification', 'apr', 'weather', 'card_declined', 'change_language', 'insurance', 'do_you_have_pets', 'damaged_card', 'card_declined']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4649, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [12]
current labels : restaurant_reviews
pair class : ['restaurant_reviews']
mask: [False False False False False False False False False False False False
  True False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : who_made_you
pair class : ['who_made_you']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [12, 15]
['balance', 'calories', 'schedule_meeting', 'make_call', 'user_name', 'meaning_of_life', 'restaurant_reviews', 'find_phone', 'todo_list', 'current_location', 'rollover_401k', 'who_made_you', 'restaurant_reviews', 'tire_change', 'nutrition_info', 'who_made_you']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0612, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : no
pair class : ['no']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['change_volume', 'repeat', 'traffic', 'payday', 'change_accent', 'credit_limit_change', 'gas', 'restaurant_reservation', 'no', 'carry_on', 'what_song', 'card_declined', 'bill_due', 'transactions', 'food_last', 'no']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5150, grad_fn=<DivBackward0>)

[15,    30] loss_total: 42.395 loss_supervised_contrasive:  13.560 loss_intent :2255.276 
skip_time: 0
total : 688
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : schedule_maintenance
pair class : ['schedule_maintenance']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['replacement_card_duration', 'reminder', 'order_checks', 'travel_alert', 'pto_request_status', 'time', 'insurance', 'sync_device', 'restaurant_reviews', 'balance', 'calculator', 'schedule_maintenance', 'current_location', 'transfer', 'where_are_you_from', 'schedule_maintenance']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5158, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : account_blocked
pair class : ['account_blocked']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['pto_balance', 'greeting', 'restaurant_reservation', 'are_you_a_bot', 'cancel', 'who_do_you_work_for', 'rollover_401k', 'confirm_reservation', 'card_declined', 'meal_suggestion', 'gas_type', 'account_blocked', 'calendar_update', 'play_music', 'update_playlist', 'account_blocked']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5083, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [10]
current labels : lost_luggage
pair class : ['lost_luggage']
mask: [False False False False False False False False False False  True False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : cook_time
pair class : ['cook_time']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [10, 15]
['directions', 'credit_limit', 'lost_luggage', 'where_are_you_from', 'improve_credit_score', 'user_name', 'update_playlist', 'reset_settings', 'cook_time', 'weather', 'lost_luggage', 'do_you_have_pets', 'interest_rate', 'share_location', 'vaccines', 'cook_time']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0586, grad_fn=<DivBackward0>)

----
masking label debug : ['change_ai_name' 'change_ai_name']
current labels  change_ai_name
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 0
current skips : [11 15]
current labels : change_ai_name
pair class : ['change_ai_name' 'change_ai_name']
mask: [False False False False False False False False False False False  True
 False False False  True]
count: 16
numbers of pairs one label : 2
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [11, 15]
['change_ai_name', 'definition', 'international_visa', 'nutrition_info', 'credit_limit_change', 'ingredient_substitution', 'food_last', 'credit_limit', 'find_phone', 'change_speed', 'pin_change', 'change_ai_name', 'accept_reservations', 'restaurant_suggestion', 'ingredients_list', 'change_ai_name']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0604, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [15]
current labels : new_card
pair class : ['new_card']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['recipe', 'rewards_balance', 'income', 'flight_status', 'change_volume', 'balance', 'damaged_card', 'what_can_i_ask_you', 'find_phone', 'whisper_mode', 'new_card', 'routing', 'flip_coin', 'travel_alert', 'text', 'new_card']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4700, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : what_are_your_hobbies
pair class : ['what_are_your_hobbies']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['do_you_have_pets', 'todo_list_update', 'calories', 'calendar', 'what_are_your_hobbies', 'uber', 'restaurant_suggestion', 'pin_change', 'new_card', 'roll_dice', 'report_lost_card', 'spending_history', 'definition', 'replacement_card_duration', 'mpg', 'what_are_your_hobbies']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5488, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [15]
current labels : meaning_of_life
pair class : ['meaning_of_life']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['what_is_your_name', 'report_lost_card', 'payday', 'car_rental', 'interest_rate', 'user_name', 'meaning_of_life', 'pto_request_status', 'oil_change_how', 'reminder_update', 'time', 'spending_history', 'apr', 'schedule_meeting', 'last_maintenance', 'meaning_of_life']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4887, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : calculator
pair class : ['calculator']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['change_user_name', 'freeze_account', 'international_visa', 'pto_used', 'reminder_update', 'change_volume', 'interest_rate', 'what_can_i_ask_you', 'calculator', 'yes', 'credit_score', 'calendar_update', 'distance', 'play_music', 'expiration_date', 'calculator']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4586, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : rollover_401k
pair class : ['rollover_401k']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['book_flight', 'goodbye', 'rollover_401k', 'pto_used', 'meeting_schedule', 'redeem_rewards', 'meaning_of_life', 'update_playlist', 'change_user_name', 'flight_status', 'w2', 'report_fraud', 'pto_request_status', 'calculator', 'do_you_have_pets', 'rollover_401k']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4649, grad_fn=<DivBackward0>)

----
masking label debug : ['tire_change' 'tire_change']
current labels  tire_change
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 0
current skips : [ 1 15]
current labels : tire_change
pair class : ['tire_change' 'tire_change']
mask: [False  True False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 2
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [1, 15]
['tire_change', 'tire_change', 'smart_home', 'confirm_reservation', 'last_maintenance', 'calories', 'tell_joke', 'todo_list', 'reset_settings', 'schedule_meeting', 'replacement_card_duration', 'reminder', 'definition', 'weather', 'repeat', 'tire_change']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0753, grad_fn=<DivBackward0>)

[15,    40] loss_total: 42.054 loss_supervised_contrasive:  18.025 loss_intent :3007.049 
skip_time: 0
total : 698
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : cancel_reservation
pair class : ['cancel_reservation']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [11]
current labels : thank_you
pair class : ['thank_you']
mask: [False False False False False False False False False False False  True
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [12]
current labels : bill_balance
pair class : ['bill_balance']
mask: [False False False False False False False False False False False False
  True False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [15, 11, 12]
['mpg', 'reminder', 'cancel_reservation', 'thank_you', 'min_payment', 'bill_balance', 'change_volume', 'spelling', 'credit_limit', 'shopping_list', 'insurance', 'thank_you', 'bill_balance', 'travel_suggestion', 'update_playlist', 'cancel_reservation']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.1097, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [9]
current labels : gas_type
pair class : ['gas_type']
mask: [False False False False False False False False False  True False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : who_do_you_work_for
pair class : ['who_do_you_work_for']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [9, 15]
['vaccines', 'gas_type', 'todo_list_update', 'meaning_of_life', 'pto_balance', 'international_fees', 'credit_score', 'travel_suggestion', 'order_checks', 'gas_type', 'schedule_maintenance', 'maybe', 'income', 'tire_pressure', 'who_do_you_work_for', 'who_do_you_work_for']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0361, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [15]
current labels : shopping_list
pair class : ['shopping_list']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['spending_history', 'traffic', 'plug_type', 'distance', 'pto_request_status', 'shopping_list', 'are_you_a_bot', 'transactions', 'freeze_account', 'expiration_date', 'calendar', 'direct_deposit', 'who_made_you', 'order', 'lost_luggage', 'shopping_list']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5016, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [2]
current labels : rewards_balance
pair class : ['rewards_balance']
mask: [False False  True False False False False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [15]
current labels : measurement_conversion
pair class : ['measurement_conversion']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [2, 15]
['rewards_balance', 'where_are_you_from', 'rewards_balance', 'food_last', 'pto_request', 'carry_on', 'measurement_conversion', 'shopping_list', 'application_status', 'international_fees', 'change_speed', 'vaccines', 'restaurant_reservation', 'what_song', 'definition', 'measurement_conversion']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0736, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [12]
current labels : insurance_change
pair class : ['insurance_change']
mask: [False False False False False False False False False False False False
  True False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : todo_list_update
pair class : ['todo_list_update']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [12, 15]
['recipe', 'nutrition_info', 'text', 'book_hotel', 'jump_start', 'change_speed', 'insurance_change', 'what_can_i_ask_you', 'direct_deposit', 'todo_list_update', 'redeem_rewards', 'how_busy', 'insurance_change', 'smart_home', 'mpg', 'todo_list_update']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0428, grad_fn=<DivBackward0>)

----
masking label debug : ['cook_time' 'cook_time']
current labels  cook_time
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 0
current skips : [ 4 11]
current labels : cook_time
pair class : ['cook_time' 'cook_time']
mask: [False False False False  True False False False False False False  True
 False False False False]
count: 16
numbers of pairs one label : 2
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : reset_settings
pair class : ['reset_settings']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [7]
current labels : pay_bill
pair class : ['pay_bill']
mask: [False False False False False False False  True False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 4
pairs see from labels :  4
All skippings : [4, 11, 15, 7]
['cook_time', 'reset_settings', 'redeem_rewards', 'pay_bill', 'cook_time', 'what_is_your_name', 'what_song', 'pay_bill', 'taxes', 'tell_joke', 'rollover_401k', 'cook_time', 'whisper_mode', 'international_visa', 'nutrition_info', 'reset_settings']
---------------------------------------------
concatenate got h_i : torch.Size([4, 768])
concatenate got h_j :  torch.Size([4, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([4, 1])
pos_sim.shape : torch.Size([4])
len(neg) : 4
loss_s_cl: tensor(0.6842, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [13]
current labels : no
pair class : ['no']
mask: [False False False False False False False False False False False False
 False  True]
count: 14
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [13]
['pin_change', 'who_made_you', 'how_old_are_you', 'carry_on', 'no', 'spelling', 'text', 'timer', 'fun_fact', 'income', 'what_is_your_name', 'how_busy', 'calendar', 'no']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([14, 14])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.2626, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [15]
current labels : rollover_401k
pair class : ['rollover_401k']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [13]
current labels : report_lost_card
pair class : ['report_lost_card']
mask: [False False False False False False False False False False False False
 False  True False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 13]
['weather', 'whisper_mode', 'ingredient_substitution', 'rollover_401k', 'how_busy', 'routing', 'credit_score', 'thank_you', 'jump_start', 'change_volume', 'account_blocked', 'report_lost_card', 'fun_fact', 'report_lost_card', 'meaning_of_life', 'rollover_401k']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0493, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : gas_type
pair class : ['gas_type']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['ingredients_list', 'change_speed', 'credit_limit_change', 'expiration_date', 'cancel', 'credit_limit', 'what_are_your_hobbies', 'schedule_meeting', 'travel_notification', 'gas_type', 'change_user_name', 'calories', 'freeze_account', 'what_is_your_name', 'reminder', 'gas_type']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4244, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : next_song
pair class : ['next_song']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [11]
current labels : time
pair class : ['time']
mask: [False False False False False False False False False False False  True
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 11]
['ingredient_substitution', 'exchange_rate', 'pin_change', 'taxes', 'next_song', 'schedule_meeting', 'credit_limit', 'apr', 'flip_coin', 'time', 'spelling', 'time', 'book_flight', 'expiration_date', 'tell_joke', 'next_song']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0586, grad_fn=<DivBackward0>)

----
masking label debug : ['food_last' 'food_last']
current labels  food_last
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 1
current skips : [ 6 15]
current labels : food_last
pair class : ['food_last' 'food_last']
mask: [False False False False False False  True False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 2
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [11]
current labels : balance
pair class : ['balance']
mask: [False False False False False False False False False False False  True
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [6, 15, 11]
['bill_due', 'food_last', 'tell_joke', 'update_playlist', 'pay_bill', 'balance', 'food_last', 'smart_home', 'confirm_reservation', 'rollover_401k', 'calories', 'balance', 'credit_limit', 'whisper_mode', 'tire_change', 'food_last']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.1136, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [15]
current labels : exchange_rate
pair class : ['exchange_rate']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['who_made_you', 'timezone', 'credit_score', 'jump_start', 'transfer', 'nutrition_info', 'exchange_rate', 'spending_history', 'what_song', 'change_speed', 'greeting', 'calendar', 'schedule_maintenance', 'routing', 'todo_list_update', 'exchange_rate']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4940, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : pto_request
pair class : ['pto_request']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['new_card', 'order', 'change_volume', 'car_rental', 'todo_list', 'restaurant_reservation', 'alarm', 'what_can_i_ask_you', 'travel_suggestion', 'pto_request', 'do_you_have_pets', 'mpg', 'income', 'next_holiday', 'application_status', 'pto_request']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4851, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : todo_list_update
pair class : ['todo_list_update']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['calendar', 'who_do_you_work_for', 'todo_list_update', 'goodbye', 'w2', 'todo_list', 'shopping_list_update', 'change_language', 'tell_joke', 'measurement_conversion', 'gas_type', 'update_playlist', 'flip_coin', 'change_ai_name', 'insurance', 'todo_list_update']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4516, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [15]
current labels : international_fees
pair class : ['international_fees']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['interest_rate', 'flip_coin', 'sync_device', 'shopping_list', 'international_visa', 'user_name', 'weather', 'w2', 'confirm_reservation', 'restaurant_reviews', 'international_fees', 'tire_pressure', 'pto_used', 'new_card', 'pto_request', 'international_fees']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4798, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [7]
current labels : international_fees
pair class : ['international_fees']
mask: [False False False False False False False  True False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : change_ai_name
pair class : ['change_ai_name']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [7, 15]
['roll_dice', 'international_fees', 'shopping_list_update', 'pto_request', 'maybe', 'lost_luggage', 'user_name', 'international_fees', 'taxes', 'are_you_a_bot', 'what_are_your_hobbies', 'share_location', 'alarm', 'plug_type', 'change_ai_name', 'change_ai_name']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0798, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [15]
current labels : flight_status
pair class : ['flight_status']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [12]
current labels : todo_list_update
pair class : ['todo_list_update']
mask: [False False False False False False False False False False False False
  True False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 12]
['flight_status', 'change_accent', 'calendar', 'todo_list_update', 'make_call', 'who_do_you_work_for', 'insurance_change', 'are_you_a_bot', 'exchange_rate', 'find_phone', 'ingredient_substitution', 'balance', 'todo_list_update', 'translate', 'change_speed', 'flight_status']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0692, grad_fn=<DivBackward0>)

[16,    10] loss_total: 41.260 loss_supervised_contrasive:  3.671 loss_intent :751.790 
skip_time: 0
total : 715
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [15]
current labels : maybe
pair class : ['maybe']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['min_payment', 'shopping_list', 'apr', 'find_phone', 'insurance', 'maybe', 'yes', 'report_fraud', 'restaurant_suggestion', 'bill_due', 'thank_you', 'w2', 'cook_time', 'definition', 'application_status', 'maybe']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5534, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : exchange_rate
pair class : ['exchange_rate']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['bill_balance', 'shopping_list', 'order_status', 'time', 'travel_notification', 'directions', 'weather', 'rewards_balance', 'exchange_rate', 'what_can_i_ask_you', 'translate', 'how_old_are_you', 'spelling', 'cancel', 'what_song', 'exchange_rate']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4761, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [6]
current labels : interest_rate
pair class : ['interest_rate']
mask: [False False False False False False  True False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [15]
current labels : timer
pair class : ['timer']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [6, 15]
['meal_suggestion', 'interest_rate', 'replacement_card_duration', 'do_you_have_pets', 'who_made_you', 'maybe', 'interest_rate', 'timer', 'accept_reservations', 'improve_credit_score', 'exchange_rate', 'current_location', 'meaning_of_life', 'pto_request', 'calendar_update', 'timer']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0623, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : where_are_you_from
pair class : ['where_are_you_from']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['what_can_i_ask_you', 'redeem_rewards', 'reset_settings', 'bill_balance', 'fun_fact', 'lost_luggage', 'apr', 'meal_suggestion', 'oil_change_when', 'where_are_you_from', 'update_playlist', 'vaccines', 'cook_time', 'thank_you', 'share_location', 'where_are_you_from']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4683, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [11]
current labels : order_checks
pair class : ['order_checks']
mask: [False False False False False False False False False False False  True
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [14]
current labels : replacement_card_duration
pair class : ['replacement_card_duration']
mask: [False False False False False False False False False False False False
 False False  True False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : cancel_reservation
pair class : ['cancel_reservation']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [11, 14, 15]
['ingredient_substitution', 'mpg', 'ingredients_list', 'goodbye', 'book_flight', 'order', 'order_checks', 'international_visa', 'change_user_name', 'replacement_card_duration', 'traffic', 'order_checks', 'cancel_reservation', 'carry_on', 'replacement_card_duration', 'cancel_reservation']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.1163, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [15]
current labels : last_maintenance
pair class : ['last_maintenance']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['cancel_reservation', 'cook_time', 'pay_bill', 'update_playlist', 'greeting', 'shopping_list_update', 'pto_balance', 'last_maintenance', 'replacement_card_duration', 'tire_pressure', 'restaurant_reviews', 'change_speed', 'who_made_you', 'account_blocked', 'income', 'last_maintenance']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5371, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : mpg
pair class : ['mpg']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['cancel_reservation', 'share_location', 'reset_settings', 'measurement_conversion', 'calories', 'damaged_card', 'car_rental', 'alarm', 'new_card', 'next_holiday', 'insurance', 'mpg', 'what_song', 'greeting', 'last_maintenance', 'mpg']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4299, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : taxes
pair class : ['taxes']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['credit_limit_change', 'taxes', 'change_language', 'yes', 'directions', 'ingredient_substitution', 'insurance_change', 'calendar', 'pto_used', 'change_accent', 'greeting', 'reminder_update', 'travel_alert', 'flight_status', 'what_song', 'taxes']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5123, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 13
current skips : [15]
current labels : change_volume
pair class : ['change_volume']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['repeat', 'rollover_401k', 'whisper_mode', 'time', 'gas_type', 'sync_device', 'how_old_are_you', 'text', 'income', 'uber', 'traffic', 'report_lost_card', 'greeting', 'change_volume', 'improve_credit_score', 'change_volume']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5432, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [15]
current labels : restaurant_reservation
pair class : ['restaurant_reservation']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [13]
current labels : uber
pair class : ['uber']
mask: [False False False False False False False False False False False False
 False  True False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 13]
['what_is_your_name', 'damaged_card', 'carry_on', 'food_last', 'payday', 'do_you_have_pets', 'tire_change', 'restaurant_reservation', 'cancel', 'play_music', 'uber', 'credit_limit', 'tell_joke', 'uber', 'oil_change_how', 'restaurant_reservation']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0537, grad_fn=<DivBackward0>)

[16,    20] loss_total: 41.963 loss_supervised_contrasive:  8.046 loss_intent :1503.554 
skip_time: 0
total : 725
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [15]
current labels : meal_suggestion
pair class : ['meal_suggestion']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['make_call', 'where_are_you_from', 'no', 'time', 'oil_change_how', 'oil_change_when', 'meal_suggestion', 'change_ai_name', 'min_payment', 'pin_change', 'routing', 'food_last', 'redeem_rewards', 'pay_bill', 'traffic', 'meal_suggestion']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5008, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [5]
current labels : todo_list
pair class : ['todo_list']
mask: [False False False False False  True False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [15]
current labels : direct_deposit
pair class : ['direct_deposit']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [5, 15]
['todo_list', 'flight_status', 'report_fraud', 'calendar', 'order_status', 'todo_list', 'pto_balance', 'direct_deposit', 'reset_settings', 'min_payment', 'todo_list_update', 'directions', 'food_last', 'travel_suggestion', 'account_blocked', 'direct_deposit']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0575, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [6]
current labels : no
pair class : ['no']
mask: [False False False False False False  True False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [7]
current labels : card_declined
pair class : ['card_declined']
mask: [False False False False False False False  True False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : redeem_rewards
pair class : ['redeem_rewards']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [6, 7, 15]
['yes', 'calendar_update', 'transactions', 'no', 'book_hotel', 'card_declined', 'no', 'card_declined', 'repeat', 'pto_balance', 'confirm_reservation', 'redeem_rewards', 'international_fees', 'where_are_you_from', 'travel_suggestion', 'redeem_rewards']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.1058, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [15]
current labels : freeze_account
pair class : ['freeze_account']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['distance', 'international_visa', 'balance', 'routing', 'tire_change', 'pto_used', 'change_accent', 'freeze_account', 'date', 'bill_balance', 'thank_you', 'improve_credit_score', 'uber', 'rollover_401k', 'payday', 'freeze_account']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4788, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 13
current skips : [15]
current labels : tire_change
pair class : ['tire_change']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['change_volume', 'flip_coin', 'nutrition_info', 'goodbye', 'pin_change', 'ingredients_list', 'timer', 'restaurant_suggestion', 'pto_balance', 'measurement_conversion', 'next_holiday', 'meaning_of_life', 'reminder_update', 'tire_change', 'transfer', 'tire_change']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4972, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [12]
current labels : roll_dice
pair class : ['roll_dice']
mask: [False False False False False False False False False False False False
  True False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : are_you_a_bot
pair class : ['are_you_a_bot']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [12, 15]
['tire_pressure', 'oil_change_when', 'flip_coin', 'roll_dice', 'are_you_a_bot', 'shopping_list_update', 'timezone', 'how_busy', 'maybe', 'calculator', 'user_name', 'vaccines', 'roll_dice', 'carry_on', 'restaurant_reviews', 'are_you_a_bot']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0393, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [14]
current labels : schedule_maintenance
pair class : ['schedule_maintenance']
mask: [False False False False False False False False False False False False
 False False  True False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [9]
current labels : translate
pair class : ['translate']
mask: [False False False False False False False False False  True False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : jump_start
pair class : ['jump_start']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [14, 9, 15]
['repeat', 'interest_rate', 'min_payment', 'schedule_maintenance', 'meaning_of_life', 'current_location', 'definition', 'translate', 'what_are_your_hobbies', 'translate', 'redeem_rewards', 'change_user_name', 'jump_start', 'find_phone', 'schedule_maintenance', 'jump_start']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.0990, grad_fn=<DivBackward0>)

----
masking label debug : ['meal_suggestion' 'meal_suggestion']
current labels  meal_suggestion
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 5
current skips : [ 7 15]
current labels : meal_suggestion
pair class : ['meal_suggestion' 'meal_suggestion']
mask: [False False False False False False False  True False False False False
 False False False  True]
count: 16
numbers of pairs one label : 2
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [12]
current labels : play_music
pair class : ['play_music']
mask: [False False False False False False False False False False False False
  True False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [13]
current labels : restaurant_suggestion
pair class : ['restaurant_suggestion']
mask: [False False False False False False False False False False False False
 False  True False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 4
pairs see from labels :  4
All skippings : [7, 15, 12, 13]
['order_checks', 'order_status', 'apr', 'rewards_balance', 'repeat', 'meal_suggestion', 'play_music', 'meal_suggestion', 'change_volume', 'yes', 'application_status', 'restaurant_suggestion', 'play_music', 'restaurant_suggestion', 'timezone', 'meal_suggestion']
---------------------------------------------
concatenate got h_i : torch.Size([4, 768])
concatenate got h_j :  torch.Size([4, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([4, 1])
pos_sim.shape : torch.Size([4])
len(neg) : 4
loss_s_cl: tensor(0.6889, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : credit_score
pair class : ['credit_score']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['recipe', 'no', 'min_payment', 'tire_pressure', 'fun_fact', 'alarm', 'sync_device', 'next_holiday', 'last_maintenance', 'change_user_name', 'how_busy', 'credit_score', 'interest_rate', 'smart_home', 'book_hotel', 'credit_score']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4999, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [12]
current labels : credit_limit_change
pair class : ['credit_limit_change']
mask: [False False False False False False False False False False False False
  True False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [15]
current labels : current_location
pair class : ['current_location']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [12, 15]
['redeem_rewards', 'text', 'credit_limit_change', 'current_location', 'change_ai_name', 'spending_history', 'cancel_reservation', 'what_are_your_hobbies', 'weather', 'meeting_schedule', 'timezone', 'plug_type', 'credit_limit_change', 'whisper_mode', 'next_song', 'current_location']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0724, grad_fn=<DivBackward0>)

[16,    30] loss_total: 40.691 loss_supervised_contrasive:  11.150 loss_intent :2255.300 
skip_time: 0
total : 735
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : gas
pair class : ['gas']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['pto_request_status', 'next_song', 'balance', 'make_call', 'where_are_you_from', 'w2', 'insurance_change', 'thank_you', 'repeat', 'mpg', 'maybe', 'spending_history', 'what_is_your_name', 'reminder', 'gas', 'gas']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5320, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [15]
current labels : bill_due
pair class : ['bill_due']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['damaged_card', 'pin_change', 'oil_change_how', 'ingredients_list', 'play_music', 'flight_status', 'payday', 'bill_due', 'pay_bill', 'book_flight', 'smart_home', 'reminder_update', 'taxes', 'pto_request_status', 'direct_deposit', 'bill_due']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5339, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [12]
current labels : do_you_have_pets
pair class : ['do_you_have_pets']
mask: [False False False False False False False False False False False False
  True False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [13]
current labels : freeze_account
pair class : ['freeze_account']
mask: [False False False False False False False False False False False False
 False  True False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [15]
current labels : plug_type
pair class : ['plug_type']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [12, 13, 15]
['sync_device', 'gas', 'gas_type', 'order', 'account_blocked', 'make_call', 'transactions', 'do_you_have_pets', 'date', 'freeze_account', 'plug_type', 'meeting_schedule', 'do_you_have_pets', 'freeze_account', 'current_location', 'plug_type']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.0993, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [15]
current labels : insurance_change
pair class : ['insurance_change']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [8]
current labels : gas
pair class : ['gas']
mask: [False False False False False False False False  True False False False
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 8]
['insurance_change', 'restaurant_reservation', 'measurement_conversion', 'distance', 'gas', 'tire_pressure', 'apr', 'find_phone', 'gas', 'pto_balance', 'date', 'jump_start', 'are_you_a_bot', 'damaged_card', 'pto_request_status', 'insurance_change']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0741, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : nutrition_info
pair class : ['nutrition_info']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['report_fraud', 'nutrition_info', 'credit_score', 'change_ai_name', 'report_lost_card', 'bill_due', 'income', 'order_checks', 'recipe', 'last_maintenance', 'translate', 'travel_notification', 'smart_home', 'reminder_update', 'distance', 'nutrition_info']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5250, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : what_is_your_name
pair class : ['what_is_your_name']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['restaurant_reservation', 'next_holiday', 'what_is_your_name', 'vaccines', 'what_song', 'pto_used', 'current_location', 'insurance', 'expiration_date', 'play_music', 'rewards_balance', 'restaurant_reviews', 'update_playlist', 'cancel_reservation', 'timer', 'what_is_your_name']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5232, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [3]
current labels : accept_reservations
pair class : ['accept_reservations']
mask: [False False False  True False False False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : bill_balance
pair class : ['bill_balance']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [3, 15]
['accept_reservations', 'bill_balance', 'travel_alert', 'accept_reservations', 'credit_limit_change', 'confirm_reservation', 'book_hotel', 'oil_change_when', 'travel_notification', 'distance', 'shopping_list_update', 'application_status', 'where_are_you_from', 'carry_on', 'international_visa', 'bill_balance']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0650, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [3]
current labels : calculator
pair class : ['calculator']
mask: [False False False  True False False False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : change_language
pair class : ['change_language']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [3, 15]
['calculator', 'who_do_you_work_for', 'vaccines', 'calculator', 'change_language', 'replacement_card_duration', 'bill_balance', 'date', 'order_checks', 'travel_alert', 'calories', 'travel_notification', 'insurance', 'what_can_i_ask_you', 'nutrition_info', 'change_language']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0473, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : gas_type
pair class : ['gas_type']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['calendar_update', 'definition', 'rewards_balance', 'mpg', 'cancel', 'reminder', 'accept_reservations', 'how_old_are_you', 'reminder_update', 'routing', 'direct_deposit', 'gas_type', 'plug_type', 'text', 'goodbye', 'gas_type']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4887, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : carry_on
pair class : ['carry_on']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [11]
current labels : change_language
pair class : ['change_language']
mask: [False False False False False False False False False False False  True
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 11]
['meaning_of_life', 'fun_fact', 'recipe', 'travel_alert', 'carry_on', 'schedule_meeting', 'expiration_date', 'change_language', 'order_status', 'freeze_account', 'sync_device', 'change_language', 'spelling', 'what_can_i_ask_you', 'calendar_update', 'carry_on']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0625, grad_fn=<DivBackward0>)

[16,    40] loss_total: 41.282 loss_supervised_contrasive:  14.845 loss_intent :3007.047 
skip_time: 0
total : 745
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [15]
current labels : what_is_your_name
pair class : ['what_is_your_name']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['what_is_your_name', 'taxes', 'calories', 'account_blocked', 'make_call', 'roll_dice', 'recipe', 'restaurant_reviews', 'damaged_card', 'application_status', 'calculator', 'cook_time', 'how_busy', 'insurance_change', 'last_maintenance', 'what_is_your_name']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5375, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [15]
current labels : order
pair class : ['order']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [12]
current labels : card_declined
pair class : ['card_declined']
mask: [False False False False False False False False False False False False
  True False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 12]
['order', 'travel_suggestion', 'reset_settings', 'change_speed', 'restaurant_suggestion', 'smart_home', 'car_rental', 'oil_change_how', 'rewards_balance', 'share_location', 'meeting_schedule', 'card_declined', 'card_declined', 'spending_history', 'traffic', 'order']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0738, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [5]
current labels : reminder
pair class : ['reminder']
mask: [False False False False False  True False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : how_old_are_you
pair class : ['how_old_are_you']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [5, 15]
['measurement_conversion', 'order', 'vaccines', 'find_phone', 'reminder', 'reminder', 'schedule_meeting', 'lost_luggage', 'accept_reservations', 'who_do_you_work_for', 'what_are_your_hobbies', 'who_made_you', 'spelling', 'improve_credit_score', 'how_old_are_you', 'how_old_are_you']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0759, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : definition
pair class : ['definition']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['distance', 'international_visa', 'definition', 'nutrition_info', 'flight_status', 'text', 'user_name', 'tire_change', 'fun_fact', 'next_song', 'yes', 'cook_time', 'traffic', 'calculator', 'who_do_you_work_for', 'definition']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5061, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : are_you_a_bot
pair class : ['are_you_a_bot']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['how_busy', 'timer', 'book_hotel', 'credit_limit', 'pto_request_status', 'report_fraud', 'transfer', 'new_card', 'jump_start', 'international_fees', 'change_accent', 'are_you_a_bot', 'payday', 'direct_deposit', 'bill_due', 'are_you_a_bot']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5087, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [15]
current labels : credit_score
pair class : ['credit_score']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['restaurant_reservation', 'improve_credit_score', 'rollover_401k', 'new_card', 'book_hotel', 'schedule_meeting', 'pto_used', 'credit_score', 'travel_alert', 'confirm_reservation', 'directions', 'calendar_update', 'income', 'report_lost_card', 'timer', 'credit_score']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5394, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [13]
current labels : report_fraud
pair class : ['report_fraud']
mask: [False False False False False False False False False False False False
 False  True]
count: 14
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [13]
['definition', 'payday', 'alarm', 'gas', 'todo_list', 'roll_dice', 'report_fraud', 'w2', 'pto_request_status', 'meeting_schedule', 'date', 'text', 'whisper_mode', 'report_fraud']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([14, 14])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.2414, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : change_user_name
pair class : ['change_user_name']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['change_volume', 'report_lost_card', 'shopping_list', 'international_fees', 'what_are_your_hobbies', 'replacement_card_duration', 'who_made_you', 'cook_time', 'pto_request', 'roll_dice', 'transfer', 'are_you_a_bot', 'last_maintenance', 'oil_change_when', 'change_user_name', 'change_user_name']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4871, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [14]
current labels : transfer
pair class : ['transfer']
mask: [False False False False False False False False False False False False
 False False  True False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : date
pair class : ['date']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [14, 15]
['calories', 'order', 'what_are_your_hobbies', 'spelling', 'transfer', 'flight_status', 'rewards_balance', 'book_flight', 'pay_bill', 'date', 'tire_pressure', 'measurement_conversion', 'next_song', 'food_last', 'transfer', 'date']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0546, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [15]
current labels : cancel_reservation
pair class : ['cancel_reservation']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [9]
current labels : routing
pair class : ['routing']
mask: [False False False False False False False False False  True False False
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 9]
['car_rental', 'calendar', 'travel_notification', 'cancel_reservation', 'routing', 'bill_due', 'todo_list', 'book_hotel', 'distance', 'routing', 'flip_coin', 'application_status', 'carry_on', 'credit_limit', 'report_fraud', 'cancel_reservation']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0584, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [15]
current labels : update_playlist
pair class : ['update_playlist']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['repeat', 'accept_reservations', 'sync_device', 'who_do_you_work_for', 'calendar_update', 'yes', 'do_you_have_pets', 'update_playlist', 'uber', 'bill_balance', 'change_volume', 'time', 'mpg', 'definition', 'restaurant_reviews', 'update_playlist']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5012, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : restaurant_reviews
pair class : ['restaurant_reviews']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [7]
current labels : account_blocked
pair class : ['account_blocked']
mask: [False False False False False False False  True False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 7]
['pin_change', 'restaurant_reviews', 'confirm_reservation', 'restaurant_reservation', 'account_blocked', 'gas', 'greeting', 'account_blocked', 'text', 'calculator', 'vaccines', 'freeze_account', 'thank_you', 'timer', 'cook_time', 'restaurant_reviews']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0701, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [8]
current labels : rewards_balance
pair class : ['rewards_balance']
mask: [False False False False False False False False  True False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 13
current skips : [15]
current labels : cancel_reservation
pair class : ['cancel_reservation']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [8, 15]
['restaurant_reviews', 'where_are_you_from', 'do_you_have_pets', 'todo_list', 'order', 'recipe', 'taxes', 'rewards_balance', 'rewards_balance', 'distance', 'tell_joke', 'directions', 'who_do_you_work_for', 'cancel_reservation', 'roll_dice', 'cancel_reservation']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0716, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [15]
current labels : no
pair class : ['no']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [13]
current labels : change_speed
pair class : ['change_speed']
mask: [False False False False False False False False False False False False
 False  True False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 13]
['uber', 'directions', 'freeze_account', 'w2', 'change_ai_name', 'no', 'application_status', 'calendar', 'international_visa', 'change_speed', 'gas', 'smart_home', 'do_you_have_pets', 'change_speed', 'traffic', 'no']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0820, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 13
current skips : [15]
current labels : pto_request
pair class : ['pto_request']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['no', 'insurance', 'freeze_account', 'measurement_conversion', 'gas_type', 'make_call', 'lost_luggage', 'tire_change', 'insurance_change', 'pto_request_status', 'jump_start', 'travel_notification', 'where_are_you_from', 'pto_request', 'gas', 'pto_request']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4868, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [15]
current labels : what_song
pair class : ['what_song']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [14]
current labels : jump_start
pair class : ['jump_start']
mask: [False False False False False False False False False False False False
 False False  True False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 14]
['international_fees', 'schedule_meeting', 'book_flight', 'reset_settings', 'gas_type', 'roll_dice', 'new_card', 'min_payment', 'change_ai_name', 'time', 'what_song', 'jump_start', 'card_declined', 'translate', 'jump_start', 'what_song']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0683, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [15]
current labels : food_last
pair class : ['food_last']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [12]
current labels : apr
pair class : ['apr']
mask: [False False False False False False False False False False False False
  True False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 12]
['food_last', 'credit_limit_change', 'change_accent', 'text', 'oil_change_how', 'apr', 'improve_credit_score', 'travel_alert', 'current_location', 'travel_notification', 'meal_suggestion', 'damaged_card', 'apr', 'oil_change_when', 'insurance_change', 'food_last']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0619, grad_fn=<DivBackward0>)

[17,    10] loss_total: 40.680 loss_supervised_contrasive:  3.094 loss_intent :751.721 
skip_time: 0
total : 762
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : cancel_reservation
pair class : ['cancel_reservation']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['plug_type', 'roll_dice', 'schedule_maintenance', 'balance', 'share_location', 'oil_change_when', 'schedule_meeting', 'credit_score', 'meaning_of_life', 'w2', 'mpg', 'lost_luggage', 'reset_settings', 'nutrition_info', 'cancel_reservation', 'cancel_reservation']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4876, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [14]
current labels : update_playlist
pair class : ['update_playlist']
mask: [False False False False False False False False False False False False
 False False  True False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [15]
current labels : payday
pair class : ['payday']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [14, 15]
['weather', 'user_name', 'tell_joke', 'meal_suggestion', 'update_playlist', 'find_phone', 'payday', 'rewards_balance', 'direct_deposit', 'oil_change_when', 'order_checks', 'whisper_mode', 'change_user_name', 'apr', 'update_playlist', 'payday']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0809, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : alarm
pair class : ['alarm']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['reminder', 'freeze_account', 'alarm', 'rollover_401k', 'definition', 'routing', 'report_lost_card', 'flight_status', 'plug_type', 'find_phone', 'change_language', 'bill_due', 'insurance', 'directions', 'book_hotel', 'alarm']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5075, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [2]
current labels : meaning_of_life
pair class : ['meaning_of_life']
mask: [False False  True False False False False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [15]
current labels : replacement_card_duration
pair class : ['replacement_card_duration']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [2, 15]
['meaning_of_life', 'accept_reservations', 'meaning_of_life', 'cancel', 'make_call', 'interest_rate', 'bill_balance', 'replacement_card_duration', 'tire_pressure', 'jump_start', 'schedule_meeting', 'direct_deposit', 'international_fees', 'vaccines', 'payday', 'replacement_card_duration']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0612, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : change_language
pair class : ['change_language']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['calendar', 'pto_request', 'change_language', 'international_visa', 'report_lost_card', 'weather', 'whisper_mode', 'restaurant_suggestion', 'share_location', 'international_fees', 'plug_type', 'transactions', 'tire_pressure', 'car_rental', 'cook_time', 'change_language']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5213, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [4]
current labels : credit_limit_change
pair class : ['credit_limit_change']
mask: [False False False False  True False False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [15]
current labels : ingredient_substitution
pair class : ['ingredient_substitution']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [4, 15]
['pto_balance', 'credit_limit_change', 'meeting_schedule', 'where_are_you_from', 'credit_limit_change', 'report_fraud', 'oil_change_how', 'share_location', 'min_payment', 'recipe', 'ingredient_substitution', 'transactions', 'pto_request', 'taxes', 'weather', 'ingredient_substitution']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0612, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : book_flight
pair class : ['book_flight']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['order_status', 'report_fraud', 'income', 'shopping_list_update', 'alarm', 'improve_credit_score', 'application_status', 'timezone', 'vaccines', 'book_flight', 'fun_fact', 'update_playlist', 'shopping_list', 'sync_device', 'pto_request_status', 'book_flight']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5053, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : todo_list
pair class : ['todo_list']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['no', 'what_are_your_hobbies', 'todo_list', 'distance', 'bill_due', 'reset_settings', 'todo_list_update', 'interest_rate', 'cancel', 'w2', 'insurance_change', 'maybe', 'last_maintenance', 'book_hotel', 'pin_change', 'todo_list']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5121, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [9]
current labels : tire_change
pair class : ['tire_change']
mask: [False False False False False False False False False  True False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [11]
current labels : nutrition_info
pair class : ['nutrition_info']
mask: [False False False False False False False False False False False  True
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [15]
current labels : shopping_list_update
pair class : ['shopping_list_update']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [9, 11, 15]
['tire_change', 'weather', 'nutrition_info', 'shopping_list_update', 'balance', 'what_can_i_ask_you', 'calories', 'reminder', 'transactions', 'tire_change', 'timer', 'nutrition_info', 'find_phone', 'how_old_are_you', 'ingredients_list', 'shopping_list_update']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.1056, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [9]
current labels : change_language
pair class : ['change_language']
mask: [False False False False False False False False False  True False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : book_hotel
pair class : ['book_hotel']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [9, 15]
['yes', 'who_made_you', 'current_location', 'weather', 'last_maintenance', 'restaurant_suggestion', 'change_language', 'travel_suggestion', 'balance', 'change_language', 'time', 'book_hotel', 'do_you_have_pets', 'gas_type', 'change_user_name', 'book_hotel']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0445, grad_fn=<DivBackward0>)

[17,    20] loss_total: 41.276 loss_supervised_contrasive:  6.783 loss_intent :1503.459 
skip_time: 0
total : 772
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : order_status
pair class : ['order_status']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['transfer', 'vaccines', 'schedule_meeting', 'shopping_list_update', 'current_location', 'who_do_you_work_for', 'restaurant_reservation', 'exchange_rate', 'order_status', 'timer', 'lost_luggage', 'redeem_rewards', 'new_card', 'whisper_mode', 'cancel_reservation', 'order_status']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5279, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [15]
current labels : repeat
pair class : ['repeat']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [11]
current labels : play_music
pair class : ['play_music']
mask: [False False False False False False False False False False False  True
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 11]
['reset_settings', 'order_checks', 'how_busy', 'measurement_conversion', 'yes', 'repeat', 'income', 'play_music', 'apr', 'calories', 'what_is_your_name', 'play_music', 'application_status', 'flight_status', 'maybe', 'repeat']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0669, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [2]
current labels : thank_you
pair class : ['thank_you']
mask: [False False  True False False False False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [15]
current labels : gas
pair class : ['gas']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [2, 15]
['meal_suggestion', 'thank_you', 'thank_you', 'min_payment', 'calendar', 'change_user_name', 'flip_coin', 'credit_limit', 'tell_joke', 'gas_type', 'gas', 'traffic', 'directions', 'accept_reservations', 'damaged_card', 'gas']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0600, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [15]
current labels : make_call
pair class : ['make_call']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['calendar_update', 'min_payment', 'flip_coin', 'order_status', 'restaurant_reviews', 'what_song', 'make_call', 'expiration_date', 'change_accent', 'date', 'bill_balance', 'smart_home', 'text', 'jump_start', 'fun_fact', 'make_call']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5533, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [15]
current labels : what_is_your_name
pair class : ['what_is_your_name']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [9]
current labels : confirm_reservation
pair class : ['confirm_reservation']
mask: [False False False False False False False False False  True False False
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 9]
['what_is_your_name', 'are_you_a_bot', 'fun_fact', 'nutrition_info', 'order', 'confirm_reservation', 'change_speed', 'transfer', 'reset_settings', 'confirm_reservation', 'cook_time', 'distance', 'how_old_are_you', 'pay_bill', 'improve_credit_score', 'what_is_your_name']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0734, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : sync_device
pair class : ['sync_device']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['redeem_rewards', 'shopping_list_update', 'todo_list', 'interest_rate', 'whisper_mode', 'calories', 'next_holiday', 'who_do_you_work_for', 'sync_device', 'lost_luggage', 'order_status', 'food_last', 'last_maintenance', 'credit_limit', 'card_declined', 'sync_device']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5107, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : pto_request_status
pair class : ['pto_request_status']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['restaurant_suggestion', 'pto_request_status', 'flight_status', 'yes', 'share_location', 'travel_alert', 'next_song', 'travel_suggestion', 'car_rental', 'reminder_update', 'current_location', 'who_made_you', 'change_accent', 'distance', 'schedule_maintenance', 'pto_request_status']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5271, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : what_are_your_hobbies
pair class : ['what_are_your_hobbies']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['tire_change', 'gas', 'what_are_your_hobbies', 'ingredient_substitution', 'where_are_you_from', 'bill_due', 'pto_used', 'uber', 'recipe', 'carry_on', 'meeting_schedule', 'todo_list_update', 'shopping_list', 'current_location', 'taxes', 'what_are_your_hobbies']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5105, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [15]
current labels : pay_bill
pair class : ['pay_bill']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [13]
current labels : greeting
pair class : ['greeting']
mask: [False False False False False False False False False False False False
 False  True False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 13]
['pay_bill', 'credit_limit_change', 'calories', 'account_blocked', 'change_volume', 'greeting', 'shopping_list', 'income', 'damaged_card', 'taxes', 'report_fraud', 'report_lost_card', 'exchange_rate', 'greeting', 'make_call', 'pay_bill']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0686, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [15]
current labels : pay_bill
pair class : ['pay_bill']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['credit_limit_change', 'travel_suggestion', 'play_music', 'transactions', 'spelling', 'smart_home', 'pay_bill', 'insurance', 'exchange_rate', 'who_made_you', 'new_card', 'what_is_your_name', 'routing', 'pto_request', 'card_declined', 'pay_bill']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5290, grad_fn=<DivBackward0>)

[17,    30] loss_total: 41.729 loss_supervised_contrasive:  10.926 loss_intent :2255.192 
skip_time: 0
total : 782
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [14]
current labels : definition
pair class : ['definition']
mask: [False False False False False False False False False False False False
 False False  True False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : share_location
pair class : ['share_location']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [12]
current labels : spelling
pair class : ['spelling']
mask: [False False False False False False False False False False False False
  True False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [14, 15, 12]
['definition', 'schedule_meeting', 'mpg', 'todo_list_update', 'share_location', 'what_is_your_name', 'alarm', 'calendar_update', 'oil_change_when', 'spelling', 'restaurant_suggestion', 'change_accent', 'spelling', 'order', 'definition', 'share_location']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.1064, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [11]
current labels : oil_change_how
pair class : ['oil_change_how']
mask: [False False False False False False False False False False False  True
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : application_status
pair class : ['application_status']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [11, 15]
['oil_change_how', 'how_old_are_you', 'payday', 'find_phone', 'reminder_update', 'interest_rate', 'order', 'calculator', 'freeze_account', 'traffic', 'flip_coin', 'oil_change_how', 'application_status', 'insurance_change', 'bill_due', 'application_status']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0624, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : redeem_rewards
pair class : ['redeem_rewards']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['tire_pressure', 'change_ai_name', 'replacement_card_duration', 'maybe', 'pto_request_status', 'what_can_i_ask_you', 'find_phone', 'timezone', 'payday', 'redeem_rewards', 'calendar_update', 'tell_joke', 'restaurant_reservation', 'travel_suggestion', 'order_checks', 'redeem_rewards']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5203, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [15]
current labels : todo_list_update
pair class : ['todo_list_update']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [8]
current labels : goodbye
pair class : ['goodbye']
mask: [False False False False False False False False  True False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [10]
current labels : rollover_401k
pair class : ['rollover_401k']
mask: [False False False False False False False False False False  True False
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [15, 8, 10]
['pto_used', 'exchange_rate', 'date', 'todo_list_update', 'user_name', 'goodbye', 'account_blocked', 'rollover_401k', 'goodbye', 'travel_notification', 'rollover_401k', 'oil_change_how', 'flight_status', 'reminder', 'replacement_card_duration', 'todo_list_update']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.1045, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [6]
current labels : how_busy
pair class : ['how_busy']
mask: [False False False False False False  True False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : international_visa
pair class : ['international_visa']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [6, 15]
['are_you_a_bot', 'confirm_reservation', 'how_busy', 'sync_device', 'spending_history', 'smart_home', 'how_busy', 'calendar', 'recipe', 'change_ai_name', 'meal_suggestion', 'calculator', 'order_status', 'user_name', 'international_visa', 'international_visa']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0710, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [6]
current labels : next_song
pair class : ['next_song']
mask: [False False False False False False  True False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : income
pair class : ['income']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [6, 15]
['how_busy', 'pto_used', 'next_song', 'todo_list', 'recipe', 'spending_history', 'next_song', 'balance', 'meaning_of_life', 'change_speed', 'do_you_have_pets', 'income', 'schedule_maintenance', 'thank_you', 'no', 'income']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0656, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [15]
current labels : plug_type
pair class : ['plug_type']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['user_name', 'restaurant_reviews', 'no', 'maybe', 'what_song', 'plug_type', 'vaccines', 'exchange_rate', 'interest_rate', 'mpg', 'cancel', 'tell_joke', 'next_song', 'rollover_401k', 'text', 'plug_type']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5081, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [5]
current labels : change_volume
pair class : ['change_volume']
mask: [False False False False False  True False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [14]
current labels : direct_deposit
pair class : ['direct_deposit']
mask: [False False False False False False False False False False False False
 False False  True False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : pay_bill
pair class : ['pay_bill']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [5, 14, 15]
['change_volume', 'pto_balance', 'next_holiday', 'reminder_update', 'direct_deposit', 'change_volume', 'calculator', 'time', 'pay_bill', 'what_can_i_ask_you', 'travel_suggestion', 'gas_type', 'international_visa', 'yes', 'direct_deposit', 'pay_bill']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.1110, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : roll_dice
pair class : ['roll_dice']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['text', 'redeem_rewards', 'todo_list_update', 'timezone', 'damaged_card', 'next_holiday', 'credit_score', 'cancel', 'goodbye', 'traffic', 'travel_alert', 'roll_dice', 'repeat', 'shopping_list_update', 'definition', 'roll_dice']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4963, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [10]
current labels : expiration_date
pair class : ['expiration_date']
mask: [False False False False False False False False False False  True False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [14]
current labels : w2
pair class : ['w2']
mask: [False False False False False False False False False False False False
 False False  True False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [15]
current labels : travel_alert
pair class : ['travel_alert']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [10, 14, 15]
['expiration_date', 'restaurant_reservation', 'rewards_balance', 'w2', 'card_declined', 'travel_notification', 'cancel', 'travel_alert', 'order_checks', 'report_lost_card', 'expiration_date', 'traffic', 'improve_credit_score', 'insurance_change', 'w2', 'travel_alert']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.1082, grad_fn=<DivBackward0>)

[17,    40] loss_total: 40.302 loss_supervised_contrasive:  13.641 loss_intent :3006.914 
skip_time: 0
total : 792
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : book_flight
pair class : ['book_flight']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['what_is_your_name', 'meaning_of_life', 'sync_device', 'how_old_are_you', 'ingredients_list', 'what_can_i_ask_you', 'make_call', 'transactions', 'credit_limit', 'repeat', 'translate', 'book_flight', 'rollover_401k', 'damaged_card', 'what_song', 'book_flight']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5172, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [15]
current labels : insurance
pair class : ['insurance']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['what_can_i_ask_you', 'how_old_are_you', 'report_fraud', 'improve_credit_score', 'car_rental', 'calendar_update', 'insurance', 'reminder_update', 'ingredient_substitution', 'maybe', 'fun_fact', 'cook_time', 'user_name', 'timezone', 'are_you_a_bot', 'insurance']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4768, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : change_accent
pair class : ['change_accent']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['goodbye', 'what_song', 'credit_limit', 'translate', 'change_accent', 'order_checks', 'accept_reservations', 'redeem_rewards', 'greeting', 'are_you_a_bot', 'ingredients_list', 'tire_pressure', 'spelling', 'repeat', 'schedule_maintenance', 'change_accent']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5712, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [14]
current labels : translate
pair class : ['translate']
mask: [False False False False False False False False False False False False
 False False  True False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : book_hotel
pair class : ['book_hotel']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [14, 15]
['pin_change', 'translate', 'international_visa', 'car_rental', 'cancel_reservation', 'ingredients_list', 'taxes', 'routing', 'book_hotel', 'lost_luggage', 'meeting_schedule', 'meal_suggestion', 'pto_request_status', 'tire_change', 'translate', 'book_hotel']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0629, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : meeting_schedule
pair class : ['meeting_schedule']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [12]
current labels : carry_on
pair class : ['carry_on']
mask: [False False False False False False False False False False False False
  True False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 12]
['min_payment', 'book_flight', 'restaurant_suggestion', 'insurance', 'who_do_you_work_for', 'pto_balance', 'confirm_reservation', 'ingredient_substitution', 'meeting_schedule', 'carry_on', 'timer', 'who_made_you', 'carry_on', 'pto_used', 'balance', 'meeting_schedule']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0719, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [12]
current labels : next_holiday
pair class : ['next_holiday']
mask: [False False False False False False False False False False False False
  True False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [4]
current labels : date
pair class : ['date']
mask: [False False False False  True False False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [11]
current labels : credit_score
pair class : ['credit_score']
mask: [False False False False False False False False False False False  True
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 13
current skips : [15]
current labels : expiration_date
pair class : ['expiration_date']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 4
pairs see from labels :  4
All skippings : [12, 4, 11, 15]
['next_holiday', 'date', 'spending_history', 'credit_score', 'date', 'where_are_you_from', 'pin_change', 'ingredient_substitution', 'reminder', 'meeting_schedule', 'travel_alert', 'credit_score', 'next_holiday', 'expiration_date', 'accept_reservations', 'expiration_date']
---------------------------------------------
concatenate got h_i : torch.Size([4, 768])
concatenate got h_j :  torch.Size([4, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([4, 1])
pos_sim.shape : torch.Size([4])
len(neg) : 4
loss_s_cl: tensor(0.6840, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [13]
current labels : income
pair class : ['income']
mask: [False False False False False False False False False False False False
 False  True]
count: 14
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [9]
current labels : spending_history
pair class : ['spending_history']
mask: [False False False False False False False False False  True False False
 False False]
count: 14
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [13, 9]
['payday', 'income', 'plug_type', 'time', 'spending_history', 'pto_balance', 'alarm', 'last_maintenance', 'mpg', 'spending_history', 'food_last', 'timezone', 'smart_home', 'income']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([14, 14])
after summing res.shape : torch.Size([])
res.shape : torch.Size([14, 14])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.9282, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [15]
current labels : travel_suggestion
pair class : ['travel_suggestion']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['make_call', 'alarm', 'who_do_you_work_for', 'shopping_list', 'find_phone', 'tire_change', 'redeem_rewards', 'next_song', 'credit_limit_change', 'measurement_conversion', 'travel_suggestion', 'meal_suggestion', 'card_declined', 'what_is_your_name', 'no', 'travel_suggestion']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5351, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [10]
current labels : what_can_i_ask_you
pair class : ['what_can_i_ask_you']
mask: [False False False False False False False False False False  True False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [15]
current labels : change_ai_name
pair class : ['change_ai_name']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [10, 15]
['exchange_rate', 'calories', 'uber', 'distance', 'what_can_i_ask_you', 'meaning_of_life', 'change_ai_name', 'roll_dice', 'travel_suggestion', 'next_holiday', 'what_can_i_ask_you', 'restaurant_reservation', 'vaccines', 'credit_score', 'bill_due', 'change_ai_name']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0583, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : where_are_you_from
pair class : ['where_are_you_from']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['pay_bill', 'where_are_you_from', 'pto_request_status', 'next_holiday', 'yes', 'roll_dice', 'book_flight', 'user_name', 'thank_you', 'pto_balance', 'weather', 'timezone', 'w2', 'what_song', 'balance', 'where_are_you_from']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5281, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [15]
current labels : reminder_update
pair class : ['reminder_update']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['last_maintenance', 'min_payment', 'pto_used', 'next_song', 'how_busy', 'reminder_update', 'tell_joke', 'credit_limit', 'traffic', 'current_location', 'greeting', 'flight_status', 'alarm', 'replacement_card_duration', 'international_visa', 'reminder_update']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5276, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [9]
current labels : gas
pair class : ['gas']
mask: [False False False False False False False False False  True False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [15]
current labels : book_flight
pair class : ['book_flight']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [9, 15]
['gas', 'ingredient_substitution', 'recipe', 'cook_time', 'report_fraud', 'where_are_you_from', 'book_flight', 'how_busy', 'balance', 'gas', 'spelling', 'vaccines', 'bill_due', 'rollover_401k', 'credit_score', 'book_flight']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0562, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : credit_limit_change
pair class : ['credit_limit_change']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['schedule_maintenance', 'flight_status', 'confirm_reservation', 'pto_request', 'credit_limit_change', 'text', 'income', 'pto_used', 'smart_home', 'calories', 'tire_change', 'tire_pressure', 'todo_list', 'restaurant_reservation', 'damaged_card', 'credit_limit_change']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5052, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [15]
current labels : improve_credit_score
pair class : ['improve_credit_score']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['tell_joke', 'international_fees', 'play_music', 'confirm_reservation', 'recipe', 'improve_credit_score', 'meaning_of_life', 'direct_deposit', 'order_status', 'car_rental', 'where_are_you_from', 'restaurant_suggestion', 'last_maintenance', 'taxes', 'balance', 'improve_credit_score']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5184, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [7]
current labels : next_holiday
pair class : ['next_holiday']
mask: [False False False False False False False  True False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : carry_on
pair class : ['carry_on']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [7, 15]
['cancel_reservation', 'accept_reservations', 'directions', 'fun_fact', 'gas', 'min_payment', 'next_holiday', 'next_holiday', 'timezone', 'carry_on', 'change_language', 'sync_device', 'transactions', 'whisper_mode', 'who_made_you', 'carry_on']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0648, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : mpg
pair class : ['mpg']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['new_card', 'maybe', 'repeat', 'how_old_are_you', 'ingredient_substitution', 'transfer', 'bill_balance', 'order_status', 'todo_list_update', 'mpg', 'rewards_balance', 'expiration_date', 'measurement_conversion', 'calendar_update', 'routing', 'mpg']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5269, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : pto_balance
pair class : ['pto_balance']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['tell_joke', 'change_volume', 'new_card', 'meeting_schedule', 'pto_balance', 'order', 'report_lost_card', 'transfer', 'rewards_balance', 'replacement_card_duration', 'transactions', 'travel_suggestion', 'text', 'no', 'repeat', 'pto_balance']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5652, grad_fn=<DivBackward0>)

[18,    10] loss_total: 42.076 loss_supervised_contrasive:  4.489 loss_intent :751.745 
skip_time: 0
total : 809
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [15]
current labels : reminder
pair class : ['reminder']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
----
masking label debug : ['international_visa' 'international_visa']
current labels  international_visa
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 2
current skips : [6 7]
current labels : international_visa
pair class : ['international_visa' 'international_visa']
mask: [False False False False False False  True  True False False False False
 False False False False]
count: 16
numbers of pairs one label : 2
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [15, 6, 7]
['reminder', 'flip_coin', 'international_visa', 'pay_bill', 'credit_limit', 'what_song', 'international_visa', 'international_visa', 'share_location', 'todo_list', 'user_name', 'what_is_your_name', 'pto_balance', 'food_last', 'meeting_schedule', 'reminder']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.1068, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [15]
current labels : roll_dice
pair class : ['roll_dice']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['schedule_meeting', 'calories', 'translate', 'roll_dice', 'spending_history', 'timezone', 'goodbye', 'definition', 'min_payment', 'date', 'reset_settings', 'replacement_card_duration', 'make_call', 'timer', 'application_status', 'roll_dice']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5062, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : credit_limit
pair class : ['credit_limit']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['plug_type', 'credit_limit', 'restaurant_reservation', 'food_last', 'find_phone', 'how_old_are_you', 'order_checks', 'vaccines', 'calendar', 'payday', 'play_music', 'greeting', 'reminder_update', 'confirm_reservation', 'flip_coin', 'credit_limit']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5250, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : change_speed
pair class : ['change_speed']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['schedule_meeting', 'spelling', 'translate', 'definition', 'ingredients_list', 'pin_change', 'reminder_update', 'distance', 'pto_request_status', 'change_volume', 'do_you_have_pets', 'apr', 'change_speed', 'change_user_name', 'whisper_mode', 'change_speed']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5287, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [15]
current labels : report_fraud
pair class : ['report_fraud']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [11]
current labels : nutrition_info
pair class : ['nutrition_info']
mask: [False False False False False False False False False False False  True
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 11]
['timer', 'travel_alert', 'sync_device', 'definition', 'change_accent', 'report_lost_card', 'change_ai_name', 'report_fraud', 'damaged_card', 'nutrition_info', 'recipe', 'nutrition_info', 'pto_used', 'transfer', 'income', 'report_fraud']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0660, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : report_fraud
pair class : ['report_fraud']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['shopping_list_update', 'meeting_schedule', 'application_status', 'spending_history', 'cancel', 'make_call', 'goodbye', 'pto_used', 'change_speed', 'reminder', 'recipe', 'report_fraud', 'find_phone', 'alarm', 'change_accent', 'report_fraud']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5179, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : smart_home
pair class : ['smart_home']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['insurance_change', 'w2', 'do_you_have_pets', 'carry_on', 'are_you_a_bot', 'travel_alert', 'credit_limit', 'report_fraud', 'improve_credit_score', 'account_blocked', 'calculator', 'greeting', 'smart_home', 'accept_reservations', 'calendar_update', 'smart_home']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5181, grad_fn=<DivBackward0>)

----
masking label debug : ['what_are_your_hobbies' 'what_are_your_hobbies']
current labels  what_are_your_hobbies
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 9
current skips : [11 15]
current labels : what_are_your_hobbies
pair class : ['what_are_your_hobbies' 'what_are_your_hobbies']
mask: [False False False False False False False False False False False  True
 False False False  True]
count: 16
numbers of pairs one label : 2
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [11, 15]
['w2', 'do_you_have_pets', 'income', 'credit_score', 'calendar_update', 'gas_type', 'time', 'apr', 'ingredient_substitution', 'what_are_your_hobbies', 'bill_due', 'what_are_your_hobbies', 'travel_alert', 'calories', 'pin_change', 'what_are_your_hobbies']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0580, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [15]
current labels : application_status
pair class : ['application_status']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['change_speed', 'definition', 'jump_start', 'oil_change_how', 'reset_settings', 'lost_luggage', 'application_status', 'interest_rate', 'car_rental', 'restaurant_reservation', 'are_you_a_bot', 'exchange_rate', 'book_hotel', 'todo_list', 'meaning_of_life', 'application_status']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5340, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : cancel_reservation
pair class : ['cancel_reservation']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['sync_device', 'timer', 'cancel_reservation', 'min_payment', 'international_fees', 'what_are_your_hobbies', 'distance', 'routing', 'pin_change', 'jump_start', 'payday', 'shopping_list_update', 'calendar_update', 'next_song', 'book_flight', 'cancel_reservation']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5395, grad_fn=<DivBackward0>)

[18,    20] loss_total: 41.976 loss_supervised_contrasive:  8.879 loss_intent :1503.471 
skip_time: 0
total : 819
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [15]
current labels : maybe
pair class : ['maybe']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['maybe', 'time', 'how_busy', 'restaurant_reservation', 'bill_balance', 'current_location', 'distance', 'expiration_date', 'exchange_rate', 'book_hotel', 'what_is_your_name', 'change_accent', 'ingredients_list', 'calculator', 'restaurant_suggestion', 'maybe']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5032, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [12]
current labels : restaurant_reviews
pair class : ['restaurant_reviews']
mask: [False False False False False False False False False False False False
  True False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [15]
current labels : goodbye
pair class : ['goodbye']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [12, 15]
['restaurant_reviews', 'interest_rate', 'cook_time', 'meeting_schedule', 'travel_notification', 'goodbye', 'cancel_reservation', 'lost_luggage', 'whisper_mode', 'smart_home', 'distance', 'application_status', 'restaurant_reviews', 'last_maintenance', 'schedule_maintenance', 'goodbye']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0635, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [15]
current labels : travel_alert
pair class : ['travel_alert']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['rollover_401k', 'change_ai_name', 'improve_credit_score', 'report_lost_card', 'no', 'oil_change_how', 'change_user_name', 'travel_alert', 'direct_deposit', 'cancel_reservation', 'reminder', 'greeting', 'directions', 'who_made_you', 'lost_luggage', 'travel_alert']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5121, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [15]
current labels : how_busy
pair class : ['how_busy']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['how_busy', 'redeem_rewards', 'ingredient_substitution', 'timer', 'yes', 'car_rental', 'flip_coin', 'confirm_reservation', 'expiration_date', 'shopping_list', 'food_last', 'calendar', 'meaning_of_life', 'insurance', 'thank_you', 'how_busy']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5208, grad_fn=<DivBackward0>)

----
masking label debug : ['share_location' 'share_location']
current labels  share_location
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 0
current skips : [ 5 15]
current labels : share_location
pair class : ['share_location' 'share_location']
mask: [False False False False False  True False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 2
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [5, 15]
['share_location', 'order_status', 'gas_type', 'date', 'cook_time', 'share_location', 'calculator', 'spelling', 'find_phone', 'food_last', 'repeat', 'calendar_update', 'pto_balance', 'redeem_rewards', 'measurement_conversion', 'share_location']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0735, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [14]
current labels : what_can_i_ask_you
pair class : ['what_can_i_ask_you']
mask: [False False False False False False False False False False False False
 False False  True False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 13
current skips : [15]
current labels : expiration_date
pair class : ['expiration_date']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [14, 15]
['tell_joke', 'taxes', 'share_location', 'tire_pressure', 'min_payment', 'rollover_401k', 'pto_request', 'recipe', 'flip_coin', 'nutrition_info', 'transactions', 'travel_notification', 'what_can_i_ask_you', 'expiration_date', 'what_can_i_ask_you', 'expiration_date']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0687, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [12]
current labels : gas_type
pair class : ['gas_type']
mask: [False False False False False False False False False False False False
  True False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 13
current skips : [15]
current labels : who_made_you
pair class : ['who_made_you']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [12, 15]
['gas_type', 'play_music', 'goodbye', 'international_fees', 'schedule_meeting', 'food_last', 'lost_luggage', 'book_hotel', 'change_speed', 'make_call', 'time', 'pto_request', 'gas_type', 'who_made_you', 'no', 'who_made_you']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0626, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [4]
current labels : taxes
pair class : ['taxes']
mask: [False False False False  True False False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
----
masking label debug : ['order_checks' 'order_checks']
current labels  order_checks
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 5
current skips : [11 15]
current labels : order_checks
pair class : ['order_checks' 'order_checks']
mask: [False False False False False False False False False False False  True
 False False False  True]
count: 16
numbers of pairs one label : 2
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [4, 11, 15]
['taxes', 'calculator', 'maybe', 'gas_type', 'taxes', 'order_checks', 'direct_deposit', 'change_language', 'mpg', 'transfer', 'improve_credit_score', 'order_checks', 'gas', 'accept_reservations', 'transactions', 'order_checks']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.1045, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [15]
current labels : bill_balance
pair class : ['bill_balance']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['are_you_a_bot', 'share_location', 'how_old_are_you', 'application_status', 'jump_start', 'replacement_card_duration', 'bill_balance', 'international_fees', 'plug_type', 'rewards_balance', 'date', 'routing', 'car_rental', 'card_declined', 'change_ai_name', 'bill_balance']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5293, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : book_hotel
pair class : ['book_hotel']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [12]
current labels : update_playlist
pair class : ['update_playlist']
mask: [False False False False False False False False False False False False
  True False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 12]
['shopping_list_update', 'yes', 'pin_change', 'change_language', 'book_hotel', 'what_are_your_hobbies', 'schedule_maintenance', 'do_you_have_pets', 'sync_device', 'credit_limit', 'next_holiday', 'update_playlist', 'update_playlist', 'travel_suggestion', 'what_can_i_ask_you', 'book_hotel']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0579, grad_fn=<DivBackward0>)

[18,    30] loss_total: 40.936 loss_supervised_contrasive:  12.228 loss_intent :2255.195 
skip_time: 0
total : 829
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [5]
current labels : insurance_change
pair class : ['insurance_change']
mask: [False False False False False  True False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [9]
current labels : todo_list_update
pair class : ['todo_list_update']
mask: [False False False False False False False False False  True False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 13
current skips : [15]
current labels : traffic
pair class : ['traffic']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [5, 9, 15]
['how_old_are_you', 'insurance_change', 'alarm', 'change_accent', 'todo_list_update', 'insurance_change', 'thank_you', 'tire_change', 'current_location', 'todo_list_update', 'reset_settings', 'oil_change_when', 'credit_score', 'traffic', 'play_music', 'traffic']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.0974, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 13
current skips : [15]
current labels : nutrition_info
pair class : ['nutrition_info']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['order', 'travel_notification', 'travel_suggestion', 'restaurant_reviews', 'do_you_have_pets', 'schedule_meeting', 'insurance', 'cancel', 'change_volume', 'spelling', 'card_declined', 'cook_time', 'damaged_card', 'nutrition_info', 'accept_reservations', 'nutrition_info']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5337, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : bill_due
pair class : ['bill_due']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['fun_fact', 'w2', 'update_playlist', 'jump_start', 'measurement_conversion', 'expiration_date', 'credit_score', 'calendar', 'bill_due', 'tell_joke', 'car_rental', 'payday', 'spending_history', 'repeat', 'account_blocked', 'bill_due']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5316, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 13
current skips : [15]
current labels : schedule_meeting
pair class : ['schedule_meeting']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['uber', 'calories', 'exchange_rate', 'weather', 'cook_time', 'change_volume', 'what_are_your_hobbies', 'change_user_name', 'insurance', 'calculator', 'travel_alert', 'shopping_list', 'book_flight', 'schedule_meeting', 'todo_list_update', 'schedule_meeting']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5017, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [15]
current labels : insurance_change
pair class : ['insurance_change']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['jump_start', 'plug_type', 'meal_suggestion', 'interest_rate', 'update_playlist', 'restaurant_suggestion', 'maybe', 'insurance_change', 'make_call', 'reset_settings', 'direct_deposit', 'translate', 'redeem_rewards', 'todo_list', 'international_visa', 'insurance_change']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5142, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : freeze_account
pair class : ['freeze_account']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [9]
current labels : mpg
pair class : ['mpg']
mask: [False False False False False False False False False  True False False
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 9]
['where_are_you_from', 'freeze_account', 'flight_status', 'plug_type', 'credit_limit_change', 'mpg', 'interest_rate', 'last_maintenance', 'pay_bill', 'mpg', 'redeem_rewards', 'reminder', 'user_name', 'cancel_reservation', 'reminder_update', 'freeze_account']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0711, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [12]
current labels : traffic
pair class : ['traffic']
mask: [False False False False False False False False False False False False
  True False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [15]
current labels : what_is_your_name
pair class : ['what_is_your_name']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [12, 15]
['time', 'traffic', 'vaccines', 'travel_notification', 'what_song', 'fun_fact', 'plug_type', 'what_is_your_name', 'play_music', 'oil_change_when', 'direct_deposit', 'change_accent', 'traffic', 'payday', 'date', 'what_is_your_name']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0632, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [4]
current labels : oil_change_how
pair class : ['oil_change_how']
mask: [False False False False  True False False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : text
pair class : ['text']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [14]
current labels : who_made_you
pair class : ['who_made_you']
mask: [False False False False False False False False False False False False
 False False  True False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [4, 15, 14]
['oil_change_how', 'text', 'bill_balance', 'accept_reservations', 'oil_change_how', 'calendar', 'order_status', 'book_flight', 'apr', 'pin_change', 'shopping_list_update', 'weather', 'who_made_you', 'rollover_401k', 'who_made_you', 'text']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.1074, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : are_you_a_bot
pair class : ['are_you_a_bot']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['routing', 'meal_suggestion', 'order', 'mpg', 'uber', 'income', 'next_song', 'current_location', 'w2', 'todo_list_update', 'flight_status', 'are_you_a_bot', 'reminder', 'whisper_mode', 'calendar', 'are_you_a_bot']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5248, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [3]
current labels : account_blocked
pair class : ['account_blocked']
mask: [False False False  True False False False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : balance
pair class : ['balance']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [3, 15]
['account_blocked', 'shopping_list', 'shopping_list_update', 'account_blocked', 'balance', 'gas', 'transactions', 'traffic', 'who_do_you_work_for', 'timer', 'carry_on', 'ingredients_list', 'where_are_you_from', 'no', 'oil_change_how', 'balance']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0786, grad_fn=<DivBackward0>)

[18,    40] loss_total: 41.189 loss_supervised_contrasive:  15.831 loss_intent :3006.925 
skip_time: 0
total : 839
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : change_language
pair class : ['change_language']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['who_do_you_work_for', 'routing', 'meaning_of_life', 'how_old_are_you', 'change_language', 'interest_rate', 'schedule_maintenance', 'order', 'card_declined', 'directions', 'roll_dice', 'todo_list', 'change_ai_name', 'translate', 'new_card', 'change_language']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4976, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [12]
current labels : cancel
pair class : ['cancel']
mask: [False False False False False False False False False False False False
  True False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : shopping_list
pair class : ['shopping_list']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [12, 15]
['weather', 'change_user_name', 'find_phone', 'cancel', 'improve_credit_score', 'card_declined', 'directions', 'freeze_account', 'order', 'shopping_list', 'flip_coin', 'new_card', 'cancel', 'uber', 'change_speed', 'shopping_list']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0766, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [15]
current labels : pto_used
pair class : ['pto_used']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['pto_used', 'smart_home', 'rewards_balance', 'directions', 'translate', 'current_location', 'report_lost_card', 'last_maintenance', 'carry_on', 'spending_history', 'international_fees', 'reset_settings', 'oil_change_when', 'sync_device', 'cancel', 'pto_used']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5170, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : account_blocked
pair class : ['account_blocked']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [14]
current labels : pay_bill
pair class : ['pay_bill']
mask: [False False False False False False False False False False False False
 False False  True False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 14]
['whisper_mode', 'account_blocked', 'are_you_a_bot', 'pay_bill', 'rewards_balance', 'report_lost_card', 'fun_fact', 'restaurant_suggestion', 'text', 'change_volume', 'pto_balance', 'user_name', 'definition', 'meal_suggestion', 'pay_bill', 'account_blocked']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0707, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [15]
current labels : tire_pressure
pair class : ['tire_pressure']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['spelling', 'uber', 'timezone', 'spending_history', 'order_status', 'report_fraud', 'user_name', 'tire_pressure', 'apr', 'replacement_card_duration', 'damaged_card', 'how_busy', 'who_do_you_work_for', 'bill_balance', 'restaurant_suggestion', 'tire_pressure']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4961, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : transfer
pair class : ['transfer']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['goodbye', 'lost_luggage', 'oil_change_when', 'greeting', 'pto_request_status', 'insurance_change', 'damaged_card', 'timezone', 'transfer', 'ingredients_list', 'pto_request', 'tire_pressure', 'apr', 'yes', 'date', 'transfer']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5253, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [13]
current labels : tire_change
pair class : ['tire_change']
mask: [False False False False False False False False False False False False
 False  True]
count: 14
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [13]
['thank_you', 'smart_home', 'flight_status', 'vaccines', 'what_song', 'rollover_401k', 'travel_notification', 'freeze_account', 'confirm_reservation', 'tire_change', 'alarm', 'taxes', 'text', 'tire_change']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([14, 14])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.2660, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : pay_bill
pair class : ['pay_bill']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['smart_home', 'date', 'weather', 'credit_score', 'gas_type', 'meal_suggestion', 'international_visa', 'insurance', 'restaurant_reservation', 'text', 'calendar', 'account_blocked', 'make_call', 'payday', 'pay_bill', 'pay_bill']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5133, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [3]
current labels : gas
pair class : ['gas']
mask: [False False False  True False False False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [15]
current labels : fun_fact
pair class : ['fun_fact']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [10]
current labels : transactions
pair class : ['transactions']
mask: [False False False False False False False False False False  True False
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [3, 15, 10]
['car_rental', 'gas', 'pin_change', 'gas', 'text', 'rollover_401k', 'fun_fact', 'how_old_are_you', 'transactions', 'change_user_name', 'transactions', 'meeting_schedule', 'find_phone', 'user_name', 'change_accent', 'fun_fact']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.1051, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [12]
current labels : flight_status
pair class : ['flight_status']
mask: [False False False False False False False False False False False False
  True False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [6]
current labels : sync_device
pair class : ['sync_device']
mask: [False False False False False False  True False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [15]
current labels : next_holiday
pair class : ['next_holiday']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [12, 6, 15]
['order_checks', 'what_song', 'flight_status', 'sync_device', 'cook_time', 'restaurant_reviews', 'sync_device', 'next_holiday', 'calories', 'report_fraud', 'apr', 'plug_type', 'flight_status', 'mpg', 'do_you_have_pets', 'next_holiday']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.1113, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [11]
current labels : lost_luggage
pair class : ['lost_luggage']
mask: [False False False False False False False False False False False  True
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : todo_list_update
pair class : ['todo_list_update']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [11, 15]
['lost_luggage', 'who_made_you', 'no', 'order', 'replacement_card_duration', 'account_blocked', 'balance', 'calendar_update', 'cancel_reservation', 'todo_list_update', 'time', 'lost_luggage', 'measurement_conversion', 'confirm_reservation', 'fun_fact', 'todo_list_update']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0745, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [15]
current labels : cancel
pair class : ['cancel']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['who_do_you_work_for', 'traffic', 'directions', 'cancel', 'card_declined', 'accept_reservations', 'todo_list_update', 'how_busy', 'bill_due', 'payday', 'pto_used', 'alarm', 'apr', 'tire_change', 'exchange_rate', 'cancel']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5387, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : book_hotel
pair class : ['book_hotel']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['who_made_you', 'text', 'thank_you', 'taxes', 'spelling', 'car_rental', 'redeem_rewards', 'uber', 'lost_luggage', 'oil_change_how', 'income', 'shopping_list_update', 'translate', 'greeting', 'book_hotel', 'book_hotel']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4848, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [4]
current labels : travel_suggestion
pair class : ['travel_suggestion']
mask: [False False False False  True False False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [15]
current labels : order_checks
pair class : ['order_checks']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [4, 15]
['repeat', 'change_ai_name', 'travel_suggestion', 'transfer', 'travel_suggestion', 'are_you_a_bot', 'order_checks', 'balance', 'calories', 'whisper_mode', 'sync_device', 'food_last', 'bill_due', 'definition', 'user_name', 'order_checks']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0742, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : bill_balance
pair class : ['bill_balance']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [8]
current labels : insurance
pair class : ['insurance']
mask: [False False False False False False False False  True False False False
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 8]
['direct_deposit', 'how_busy', 'bill_balance', 'ingredient_substitution', 'tire_change', 'report_lost_card', 'freeze_account', 'insurance', 'insurance', 'alarm', 'restaurant_reviews', 'change_speed', 'shopping_list', 'travel_alert', 'change_volume', 'bill_balance']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0674, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [15]
current labels : smart_home
pair class : ['smart_home']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['smart_home', 'damaged_card', 'change_language', 'confirm_reservation', 'user_name', 'shopping_list_update', 'w2', 'pto_balance', 'shopping_list', 'who_made_you', 'greeting', 'are_you_a_bot', 'pto_request', 'recipe', 'tire_change', 'smart_home']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5232, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : distance
pair class : ['distance']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['routing', 'cook_time', 'distance', 'min_payment', 'schedule_maintenance', 'insurance_change', 'reminder_update', 'time', 'rewards_balance', 'book_hotel', 'timezone', 'goodbye', 'recipe', 'balance', 'pto_request_status', 'distance']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4921, grad_fn=<DivBackward0>)

[19,    10] loss_total: 41.184 loss_supervised_contrasive:  3.598 loss_intent :751.713 
skip_time: 0
total : 856
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [15]
current labels : alarm
pair class : ['alarm']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['insurance_change', 'flip_coin', 'yes', 'make_call', 'restaurant_suggestion', 'alarm', 'pto_request_status', 'change_user_name', 'cancel', 'order_status', 'cook_time', 'bill_due', 'tell_joke', 'confirm_reservation', 'direct_deposit', 'alarm']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5747, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [15]
current labels : improve_credit_score
pair class : ['improve_credit_score']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['translate', 'change_language', 'report_fraud', 'tire_pressure', 'next_holiday', 'no', 'report_lost_card', 'improve_credit_score', 'timer', 'thank_you', 'schedule_meeting', 'insurance_change', 'shopping_list', 'insurance', 'definition', 'improve_credit_score']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4987, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [10]
current labels : ingredients_list
pair class : ['ingredients_list']
mask: [False False False False False False False False False False  True False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 13
current skips : [15]
current labels : what_is_your_name
pair class : ['what_is_your_name']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [10, 15]
['ingredients_list', 'directions', 'yes', 'spending_history', 'oil_change_when', 'pin_change', 'cancel', 'new_card', 'measurement_conversion', 'text', 'ingredients_list', 'make_call', 'meal_suggestion', 'what_is_your_name', 'plug_type', 'what_is_your_name']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0663, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : text
pair class : ['text']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['share_location', 'how_old_are_you', 'food_last', 'todo_list', 'credit_limit_change', 'new_card', 'thank_you', 'mpg', 'travel_notification', 'taxes', 'interest_rate', 'cancel_reservation', 'reset_settings', 'what_song', 'text', 'text']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5188, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [12]
current labels : travel_notification
pair class : ['travel_notification']
mask: [False False False False False False False False False False False False
  True False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [15]
current labels : todo_list
pair class : ['todo_list']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [12, 15]
['ingredient_substitution', 'travel_notification', 'restaurant_reviews', 'routing', 'next_holiday', 'meal_suggestion', 'taxes', 'todo_list', 'shopping_list_update', 'last_maintenance', 'tire_change', 'bill_balance', 'travel_notification', 'change_user_name', 'book_flight', 'todo_list']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0663, grad_fn=<DivBackward0>)

----
masking label debug : ['current_location' 'current_location']
current labels  current_location
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 0
current skips : [ 2 15]
current labels : current_location
pair class : ['current_location' 'current_location']
mask: [False False  True False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 2
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [2, 15]
['current_location', 'calories', 'current_location', 'credit_limit', 'distance', 'flip_coin', 'mpg', 'what_song', 'uber', 'account_blocked', 'roll_dice', 'lost_luggage', 'how_busy', 'rollover_401k', 'date', 'current_location']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0715, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [15]
current labels : bill_due
pair class : ['bill_due']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['calories', 'smart_home', 'credit_limit_change', 'nutrition_info', 'gas_type', 'meal_suggestion', 'pto_request', 'bill_due', 'account_blocked', 'meaning_of_life', 'jump_start', 'flight_status', 'are_you_a_bot', 'weather', 'book_flight', 'bill_due']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5219, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [2]
current labels : reset_settings
pair class : ['reset_settings']
mask: [False False  True False False False False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : improve_credit_score
pair class : ['improve_credit_score']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [2, 15]
['reset_settings', 'book_flight', 'reset_settings', 'bill_balance', 'order', 'timer', 'nutrition_info', 'what_can_i_ask_you', 'current_location', 'gas_type', 'change_ai_name', 'calendar_update', 'improve_credit_score', 'weather', 'jump_start', 'improve_credit_score']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0641, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [4]
current labels : carry_on
pair class : ['carry_on']
mask: [False False False False  True False False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [15]
current labels : cancel_reservation
pair class : ['cancel_reservation']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [4, 15]
['time', 'carry_on', 'tire_pressure', 'distance', 'carry_on', 'taxes', 'greeting', 'cancel_reservation', 'calendar', 'calendar_update', 'credit_limit', 'change_volume', 'vaccines', 'next_holiday', 'where_are_you_from', 'cancel_reservation']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0709, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : update_playlist
pair class : ['update_playlist']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['ingredients_list', 'order_checks', 'vaccines', 'reminder_update', 'timezone', 'credit_score', 'calculator', 'book_flight', 'update_playlist', 'freeze_account', 'damaged_card', 'who_made_you', 'order_status', 'change_accent', 'how_old_are_you', 'update_playlist']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5165, grad_fn=<DivBackward0>)

[19,    20] loss_total: 41.384 loss_supervised_contrasive:  7.395 loss_intent :1503.458 
skip_time: 0
total : 866
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [8]
current labels : spending_history
pair class : ['spending_history']
mask: [False False False False False False False False  True False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [15]
current labels : change_language
pair class : ['change_language']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [8, 15]
['spending_history', 'schedule_meeting', 'oil_change_how', 'pin_change', 'pto_balance', 'lost_luggage', 'spelling', 'definition', 'spending_history', 'plug_type', 'change_language', 'jump_start', 'where_are_you_from', 'update_playlist', 'w2', 'change_language']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0675, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : share_location
pair class : ['share_location']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['restaurant_reviews', 'rollover_401k', 'cancel', 'travel_alert', 'direct_deposit', 'directions', 'what_are_your_hobbies', 'change_language', 'share_location', 'smart_home', 'what_is_your_name', 'expiration_date', 'balance', 'distance', 'reminder_update', 'share_location']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5117, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [14]
current labels : time
pair class : ['time']
mask: [False False False False False False False False False False False False
 False False  True False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : change_accent
pair class : ['change_accent']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [14, 15]
['uber', 'last_maintenance', 'replacement_card_duration', 'time', 'tell_joke', 'ingredient_substitution', 'application_status', 'cancel_reservation', 'pto_request_status', 'restaurant_suggestion', 'todo_list_update', 'change_accent', 'change_volume', 'international_fees', 'time', 'change_accent']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0721, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : insurance_change
pair class : ['insurance_change']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['sync_device', 'insurance_change', 'next_song', 'find_phone', 'travel_notification', 'pto_balance', 'jump_start', 'restaurant_reservation', 'accept_reservations', 'transactions', 'last_maintenance', 'schedule_meeting', 'who_do_you_work_for', 'timer', 'freeze_account', 'insurance_change']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5194, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [15]
current labels : oil_change_how
pair class : ['oil_change_how']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [13]
current labels : international_fees
pair class : ['international_fees']
mask: [False False False False False False False False False False False False
 False  True False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 13]
['oil_change_how', 'what_are_your_hobbies', 'restaurant_reservation', 'tell_joke', 'translate', 'restaurant_suggestion', 'international_fees', 'shopping_list_update', 'maybe', 'food_last', 'meaning_of_life', 'car_rental', 'pto_request', 'international_fees', 'fun_fact', 'oil_change_how']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0663, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [15]
current labels : credit_limit_change
pair class : ['credit_limit_change']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['oil_change_when', 'spending_history', 'reminder', 'w2', 'pto_request_status', 'credit_limit_change', 'yes', 'roll_dice', 'damaged_card', 'rewards_balance', 'nutrition_info', 'oil_change_how', 'ingredient_substitution', 'routing', 'meeting_schedule', 'credit_limit_change']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5167, grad_fn=<DivBackward0>)

----
masking label debug : ['whisper_mode' 'whisper_mode']
current labels  whisper_mode
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 3
current skips : [10 15]
current labels : whisper_mode
pair class : ['whisper_mode' 'whisper_mode']
mask: [False False False False False False False False False False  True False
 False False False  True]
count: 16
numbers of pairs one label : 2
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [10, 15]
['damaged_card', 'definition', 'schedule_maintenance', 'whisper_mode', 'measurement_conversion', 'balance', 'find_phone', 'flight_status', 'expiration_date', 'no', 'whisper_mode', 'vaccines', 'pto_request', 'weather', 'update_playlist', 'whisper_mode']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.1004, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [10]
current labels : food_last
pair class : ['food_last']
mask: [False False False False False False False False False False  True False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 13
current skips : [15]
current labels : redeem_rewards
pair class : ['redeem_rewards']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [10, 15]
['oil_change_when', 'where_are_you_from', 'interest_rate', 'schedule_meeting', 'tire_pressure', 'recipe', 'restaurant_reviews', 'smart_home', 'credit_score', 'food_last', 'food_last', 'international_visa', 'report_fraud', 'redeem_rewards', 'what_can_i_ask_you', 'redeem_rewards']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0679, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : interest_rate
pair class : ['interest_rate']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['change_volume', 'current_location', 'pay_bill', 'min_payment', 'change_ai_name', 'find_phone', 'expiration_date', 'sync_device', 'interest_rate', 'pto_request', 'update_playlist', 'restaurant_suggestion', 'spelling', 'pto_used', 'calculator', 'interest_rate']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5448, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [8]
current labels : where_are_you_from
pair class : ['where_are_you_from']
mask: [False False False False False False False False  True False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : accept_reservations
pair class : ['accept_reservations']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [8, 15]
['timezone', 'nutrition_info', 'calendar', 'international_fees', 'book_flight', 'flip_coin', 'where_are_you_from', 'income', 'where_are_you_from', 'improve_credit_score', 'redeem_rewards', 'accept_reservations', 'restaurant_reservation', 'calendar_update', 'bill_balance', 'accept_reservations']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0690, grad_fn=<DivBackward0>)

[19,    30] loss_total: 41.040 loss_supervised_contrasive:  10.849 loss_intent :2255.186 
skip_time: 0
total : 876
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : no
pair class : ['no']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['pto_used', 'no', 'reminder_update', 'w2', 'how_busy', 'pay_bill', 'next_song', 'fun_fact', 'flight_status', 'taxes', 'uber', 'distance', 'credit_score', 'rewards_balance', 'card_declined', 'no']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5315, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : whisper_mode
pair class : ['whisper_mode']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['change_ai_name', 'traffic', 'whisper_mode', 'how_old_are_you', 'change_volume', 'weather', 'spending_history', 'shopping_list', 'reset_settings', 'timezone', 'play_music', 'pin_change', 'last_maintenance', 'what_is_your_name', 'change_accent', 'whisper_mode']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5271, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [15]
current labels : transfer
pair class : ['transfer']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['share_location', 'uber', 'car_rental', 'spelling', 'measurement_conversion', 'order_status', 'transfer', 'apr', 'freeze_account', 'meeting_schedule', 'interest_rate', 'make_call', 'reminder', 'repeat', 'income', 'transfer']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5071, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : roll_dice
pair class : ['roll_dice']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['cook_time', 'translate', 'international_visa', 'meeting_schedule', 'gas', 'what_song', 'are_you_a_bot', 'who_do_you_work_for', 'thank_you', 'roll_dice', 'transfer', 'change_speed', 'rewards_balance', 'change_accent', 'book_hotel', 'roll_dice']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5402, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [14]
current labels : meaning_of_life
pair class : ['meaning_of_life']
mask: [False False False False False False False False False False False False
 False False  True False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : thank_you
pair class : ['thank_you']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [14, 15]
['tire_pressure', 'date', 'change_speed', 'bill_balance', 'meaning_of_life', 'no', 'ingredient_substitution', 'restaurant_reservation', 'thank_you', 'redeem_rewards', 'timer', 'repeat', 'order_status', 'insurance_change', 'meaning_of_life', 'thank_you']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0710, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : confirm_reservation
pair class : ['confirm_reservation']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [12]
current labels : change_speed
pair class : ['change_speed']
mask: [False False False False False False False False False False False False
  True False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 12]
['international_visa', 'transactions', 'confirm_reservation', 'user_name', 'next_holiday', 'improve_credit_score', 'change_speed', 'next_song', 'mpg', 'ingredients_list', 'routing', 'calculator', 'change_speed', 'share_location', 'play_music', 'confirm_reservation']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0695, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : report_fraud
pair class : ['report_fraud']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['order_checks', 'flip_coin', 'new_card', 'schedule_meeting', 'credit_limit', 'insurance', 'replacement_card_duration', 'plug_type', 'who_do_you_work_for', 'share_location', 'rewards_balance', 'what_can_i_ask_you', 'report_fraud', 'timezone', 'calendar_update', 'report_fraud']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5158, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [6]
current labels : pto_used
pair class : ['pto_used']
mask: [False False False False False False  True False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : replacement_card_duration
pair class : ['replacement_card_duration']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [6, 15]
['pto_used', 'nutrition_info', 'what_is_your_name', 'payday', 'replacement_card_duration', 'min_payment', 'pto_used', 'roll_dice', 'calculator', 'carry_on', 'oil_change_how', 'meal_suggestion', 'meeting_schedule', 'new_card', 'card_declined', 'replacement_card_duration']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0738, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [4]
current labels : travel_suggestion
pair class : ['travel_suggestion']
mask: [False False False False  True False False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : transfer
pair class : ['transfer']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [4, 15]
['directions', 'travel_suggestion', 'cook_time', 'account_blocked', 'travel_suggestion', 'date', 'maybe', 'todo_list', 'gas', 'exchange_rate', 'expiration_date', 'report_fraud', 'international_fees', 'gas_type', 'transfer', 'transfer']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0694, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [15]
current labels : exchange_rate
pair class : ['exchange_rate']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [13]
current labels : goodbye
pair class : ['goodbye']
mask: [False False False False False False False False False False False False
 False  True False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 13]
['alarm', 'greeting', 'directions', 'make_call', 'book_hotel', 'translate', 'exchange_rate', 'accept_reservations', 'goodbye', 'tell_joke', 'routing', 'pay_bill', 'jump_start', 'goodbye', 'direct_deposit', 'exchange_rate']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0955, grad_fn=<DivBackward0>)

[19,    40] loss_total: 41.386 loss_supervised_contrasive:  14.649 loss_intent :3006.897 
skip_time: 0
total : 886
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [11]
current labels : what_can_i_ask_you
pair class : ['what_can_i_ask_you']
mask: [False False False False False False False False False False False  True
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : redeem_rewards
pair class : ['redeem_rewards']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [12]
current labels : maybe
pair class : ['maybe']
mask: [False False False False False False False False False False False False
  True False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [11, 15, 12]
['oil_change_when', 'current_location', 'schedule_maintenance', 'calendar', 'last_maintenance', 'what_can_i_ask_you', 'goodbye', 'shopping_list', 'redeem_rewards', 'pto_balance', 'maybe', 'what_can_i_ask_you', 'maybe', 'yes', 'pin_change', 'redeem_rewards']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.1141, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [10]
current labels : min_payment
pair class : ['min_payment']
mask: [False False False False False False False False False False  True False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : traffic
pair class : ['traffic']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [10, 15]
['new_card', 'cancel_reservation', 'measurement_conversion', 'tire_change', 'shopping_list_update', 'do_you_have_pets', 'min_payment', 'credit_limit', 'application_status', 'reminder_update', 'min_payment', 'calendar', 'traffic', 'todo_list_update', 'bill_due', 'traffic']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0692, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [15]
current labels : update_playlist
pair class : ['update_playlist']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [10]
current labels : what_are_your_hobbies
pair class : ['what_are_your_hobbies']
mask: [False False False False False False False False False False  True False
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 10]
['w2', 'cancel', 'play_music', 'update_playlist', 'calories', 'accept_reservations', 'next_song', 'credit_score', 'car_rental', 'what_are_your_hobbies', 'what_are_your_hobbies', 'expiration_date', 'card_declined', 'change_user_name', 'do_you_have_pets', 'update_playlist']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0660, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [15]
current labels : how_busy
pair class : ['how_busy']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['change_user_name', 'maybe', 'reminder', 'how_busy', 'todo_list', 'apr', 'order_checks', 'restaurant_suggestion', 'meaning_of_life', 'mpg', 'schedule_maintenance', 'payday', 'card_declined', 'application_status', 'yes', 'how_busy']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5183, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [15]
current labels : change_language
pair class : ['change_language']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['travel_alert', 'recipe', 'pto_request_status', 'do_you_have_pets', 'credit_limit', 'freeze_account', 'change_language', 'traffic', 'gas_type', 'pto_balance', 'how_old_are_you', 'what_is_your_name', 'report_lost_card', 'schedule_maintenance', 'reset_settings', 'change_language']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5208, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [5]
current labels : exchange_rate
pair class : ['exchange_rate']
mask: [False False False False False  True False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : transfer
pair class : ['transfer']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [5, 15]
['goodbye', 'exchange_rate', 'damaged_card', 'rollover_401k', 'oil_change_when', 'exchange_rate', 'are_you_a_bot', 'do_you_have_pets', 'travel_notification', 'international_visa', 'report_lost_card', 'ingredients_list', 'transfer', 'play_music', 'book_hotel', 'transfer']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0703, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [13]
current labels : interest_rate
pair class : ['interest_rate']
mask: [False False False False False False False False False False False False
 False  True]
count: 14
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [13]
['interest_rate', 'whisper_mode', 'transactions', 'traffic', 'order', 'replacement_card_duration', 'flip_coin', 'who_made_you', 'user_name', 'credit_limit_change', 'timer', 'travel_alert', 'todo_list', 'interest_rate']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([14, 14])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.2665, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [13]
current labels : transfer
pair class : ['transfer']
mask: [False False False False False False False False False False False False
 False  True False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [11]
current labels : oil_change_how
pair class : ['oil_change_how']
mask: [False False False False False False False False False False False  True
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : exchange_rate
pair class : ['exchange_rate']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [13, 11, 15]
['user_name', 'pin_change', 'what_are_your_hobbies', 'transfer', 'payday', 'nutrition_info', 'travel_suggestion', 'oil_change_how', 'mpg', 'exchange_rate', 'tire_pressure', 'oil_change_how', 'restaurant_reviews', 'transfer', 'time', 'exchange_rate']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.1033, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : play_music
pair class : ['play_music']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['shopping_list_update', 'change_speed', 'roll_dice', 'share_location', 'ingredients_list', 'smart_home', 'shopping_list', 'redeem_rewards', 'interest_rate', 'replacement_card_duration', 'balance', 'what_are_your_hobbies', 'play_music', 'plug_type', 'traffic', 'play_music']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5423, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : pto_used
pair class : ['pto_used']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [5]
current labels : pto_request
pair class : ['pto_request']
mask: [False False False False False  True False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 5]
['income', 'pto_used', 'travel_notification', 'timer', 'pto_request', 'pto_request', 'calories', 'restaurant_reservation', 'damaged_card', 'time', 'ingredient_substitution', 'make_call', 'update_playlist', 'pto_request_status', 'expiration_date', 'pto_used']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0743, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : carry_on
pair class : ['carry_on']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['roll_dice', 'greeting', 'carry_on', 'date', 'car_rental', 'oil_change_how', 'oil_change_when', 'spending_history', 'income', 'insurance_change', 'user_name', 'food_last', 'timezone', 'cook_time', 'change_ai_name', 'carry_on']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5104, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 13
current skips : [15]
current labels : meeting_schedule
pair class : ['meeting_schedule']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['next_song', 'change_speed', 'ingredient_substitution', 'yes', 'measurement_conversion', 'mpg', 'reset_settings', 'carry_on', 'book_hotel', 'jump_start', 'change_volume', 'payday', 'min_payment', 'meeting_schedule', 'order_status', 'meeting_schedule']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5091, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [4]
current labels : fun_fact
pair class : ['fun_fact']
mask: [False False False False  True False False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : calories
pair class : ['calories']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [4, 15]
['redeem_rewards', 'reset_settings', 'do_you_have_pets', 'fun_fact', 'fun_fact', 'mpg', 'shopping_list_update', 'roll_dice', 'gas', 'what_song', 'who_do_you_work_for', 'spelling', 'calories', 'pay_bill', 'schedule_meeting', 'calories']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0663, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [14]
current labels : confirm_reservation
pair class : ['confirm_reservation']
mask: [False False False False False False False False False False False False
 False False  True False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [15]
current labels : meal_suggestion
pair class : ['meal_suggestion']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [14, 15]
['confirm_reservation', 'make_call', 'pto_request', 'tire_pressure', 'goodbye', 'are_you_a_bot', 'travel_notification', 'meal_suggestion', 'play_music', 'order_status', 'international_fees', 'improve_credit_score', 'roll_dice', 'current_location', 'confirm_reservation', 'meal_suggestion']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0741, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [15]
current labels : reminder
pair class : ['reminder']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['taxes', 'accept_reservations', 'who_made_you', 'book_hotel', 'repeat', 'definition', 'reminder', 'last_maintenance', 'rollover_401k', 'restaurant_reviews', 'w2', 'tire_change', 'how_busy', 'translate', 'text', 'reminder']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5334, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 13
current skips : [15]
current labels : calories
pair class : ['calories']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['nutrition_info', 'international_fees', 'replacement_card_duration', 'cancel_reservation', 'who_do_you_work_for', 'timezone', 'card_declined', 'change_speed', 'rollover_401k', 'translate', 'change_language', 'accept_reservations', 'restaurant_suggestion', 'calories', 'cancel', 'calories']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5842, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [8]
current labels : new_card
pair class : ['new_card']
mask: [False False False False False False False False  True False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [9]
current labels : pto_request_status
pair class : ['pto_request_status']
mask: [False False False False False False False False False  True False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : traffic
pair class : ['traffic']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [8, 9, 15]
['new_card', 'reminder_update', 'application_status', 'income', 'travel_notification', 'oil_change_when', 'pto_request_status', 'time', 'new_card', 'pto_request_status', 'mpg', 'tire_change', 'account_blocked', 'gas_type', 'traffic', 'traffic']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.1169, grad_fn=<DivBackward0>)

[20,    10] loss_total: 41.198 loss_supervised_contrasive:  3.611 loss_intent :751.723 
skip_time: 0
total : 903
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [9]
current labels : bill_due
pair class : ['bill_due']
mask: [False False False False False False False False False  True False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : definition
pair class : ['definition']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [14]
current labels : share_location
pair class : ['share_location']
mask: [False False False False False False False False False False False False
 False False  True False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [9, 15, 14]
['account_blocked', 'are_you_a_bot', 'bill_due', 'insurance', 'definition', 'pto_used', 'share_location', 'text', 'routing', 'bill_due', 'cook_time', 'repeat', 'meeting_schedule', 'pay_bill', 'share_location', 'definition']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.1105, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [15]
current labels : apr
pair class : ['apr']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['plug_type', 'date', 'international_visa', 'smart_home', 'maybe', 'apr', 'car_rental', 'pto_used', 'find_phone', 'improve_credit_score', 'order', 'transactions', 'rewards_balance', 'lost_luggage', 'nutrition_info', 'apr']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5441, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [15]
current labels : make_call
pair class : ['make_call']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['calculator', 'plug_type', 'food_last', 'todo_list_update', 'credit_score', 'make_call', 'travel_alert', 'translate', 'calendar_update', 'thank_you', 'damaged_card', 'insurance_change', 'weather', 'do_you_have_pets', 'yes', 'make_call']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5164, grad_fn=<DivBackward0>)

----
masking label debug : ['maybe' 'maybe']
current labels  maybe
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 4
current skips : [ 5 15]
current labels : maybe
pair class : ['maybe' 'maybe']
mask: [False False False False False  True False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 2
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [5, 15]
['balance', 'whisper_mode', 'exchange_rate', 'timer', 'maybe', 'maybe', 'lost_luggage', 'travel_suggestion', 'flip_coin', 'travel_alert', 'order_checks', 'traffic', 'account_blocked', 'alarm', 'routing', 'maybe']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0781, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : rollover_401k
pair class : ['rollover_401k']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['jump_start', 'how_old_are_you', 'insurance', 'pto_request', 'ingredients_list', 'apr', 'todo_list', 'schedule_meeting', 'rollover_401k', 'travel_suggestion', 'reminder_update', 'credit_limit', 'vaccines', 'carry_on', 'report_lost_card', 'rollover_401k']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5209, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : cancel_reservation
pair class : ['cancel_reservation']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['who_made_you', 'cancel_reservation', 'what_is_your_name', 'are_you_a_bot', 'credit_score', 'insurance_change', 'what_can_i_ask_you', 'what_song', 'what_are_your_hobbies', 'how_old_are_you', 'change_language', 'definition', 'transactions', 'interest_rate', 'report_fraud', 'cancel_reservation']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5460, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : where_are_you_from
pair class : ['where_are_you_from']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['next_song', 'credit_limit_change', 'whisper_mode', 'update_playlist', 'current_location', 'change_volume', 'min_payment', 'recipe', 'insurance', 'balance', 'restaurant_reservation', 'directions', 'cancel', 'travel_alert', 'where_are_you_from', 'where_are_you_from']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5215, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : credit_limit
pair class : ['credit_limit']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['timer', 'travel_notification', 'book_hotel', 'bill_balance', 'credit_limit', 'international_visa', 'meaning_of_life', 'restaurant_suggestion', 'cook_time', 'reset_settings', 'tire_change', 'sync_device', 'distance', 'uber', 'find_phone', 'credit_limit']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5204, grad_fn=<DivBackward0>)

----
masking label debug : ['improve_credit_score' 'improve_credit_score']
current labels  improve_credit_score
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 4
current skips : [ 7 15]
current labels : improve_credit_score
pair class : ['improve_credit_score' 'improve_credit_score']
mask: [False False False False False False False  True False False False False
 False False False  True]
count: 16
numbers of pairs one label : 2
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [11]
current labels : gas
pair class : ['gas']
mask: [False False False False False False False False False False False  True
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [7, 15, 11]
['rewards_balance', 'find_phone', 'order_status', 'schedule_maintenance', 'improve_credit_score', 'schedule_meeting', 'pay_bill', 'improve_credit_score', 'pto_balance', 'gas', 'book_hotel', 'gas', 'new_card', 'tell_joke', 'balance', 'improve_credit_score']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.1113, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : credit_limit_change
pair class : ['credit_limit_change']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [10]
current labels : alarm
pair class : ['alarm']
mask: [False False False False False False False False False False  True False
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 10]
['book_flight', 'change_language', 'credit_limit_change', 'calculator', 'update_playlist', 'alarm', 'current_location', 'timezone', 'cancel', 'pto_request_status', 'alarm', 'change_ai_name', 'credit_score', 'order_status', 'last_maintenance', 'credit_limit_change']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0693, grad_fn=<DivBackward0>)

[20,    20] loss_total: 41.540 loss_supervised_contrasive:  7.565 loss_intent :1503.448 
skip_time: 0
total : 913
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [15]
current labels : change_accent
pair class : ['change_accent']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [11]
current labels : greeting
pair class : ['greeting']
mask: [False False False False False False False False False False False  True
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 11]
['change_accent', 'flight_status', 'greeting', 'change_ai_name', 'carry_on', 'gas', 'car_rental', 'meeting_schedule', 'w2', 'user_name', 'meal_suggestion', 'greeting', 'credit_limit_change', 'taxes', 'what_song', 'change_accent']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0730, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : no
pair class : ['no']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['next_holiday', 'make_call', 'measurement_conversion', 'international_fees', 'play_music', 'todo_list_update', 'how_busy', 'pto_used', 'no', 'card_declined', 'bill_balance', 'thank_you', 'income', 'ingredient_substitution', 'report_lost_card', 'no']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5145, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [12]
current labels : payday
pair class : ['payday']
mask: [False False False False False False False False False False False False
  True False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 13
current skips : [15]
current labels : travel_suggestion
pair class : ['travel_suggestion']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [12, 15]
['todo_list_update', 'text', 'improve_credit_score', 'payday', 'pto_balance', 'expiration_date', 'report_lost_card', 'min_payment', 'transfer', 'spending_history', 'what_is_your_name', 'pin_change', 'payday', 'travel_suggestion', 'repeat', 'travel_suggestion']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0724, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : reset_settings
pair class : ['reset_settings']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['freeze_account', 'reset_settings', 'redeem_rewards', 'uber', 'calendar', 'translate', 'spending_history', 'next_song', 'book_flight', 'measurement_conversion', 'report_fraud', 'direct_deposit', 'jump_start', 'food_last', 'calculator', 'reset_settings']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5130, grad_fn=<DivBackward0>)

----
masking label debug : ['where_are_you_from' 'where_are_you_from']
current labels  where_are_you_from
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 1
current skips : [ 3 15]
current labels : where_are_you_from
pair class : ['where_are_you_from' 'where_are_you_from']
mask: [False False False  True False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 2
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [3, 15]
['next_holiday', 'where_are_you_from', 'schedule_maintenance', 'where_are_you_from', 'translate', 'credit_limit', 'meaning_of_life', 'restaurant_reviews', 'change_accent', 'spelling', 'change_user_name', 'car_rental', 'lost_luggage', 'freeze_account', 'flight_status', 'where_are_you_from']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0854, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [15]
current labels : transfer
pair class : ['transfer']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['uber', 'confirm_reservation', 'play_music', 'sync_device', 'fun_fact', 'transfer', 'oil_change_when', 'shopping_list', 'international_visa', 'international_fees', 'book_flight', 'w2', 'routing', 'spending_history', 'yes', 'transfer']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5040, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [15]
current labels : flight_status
pair class : ['flight_status']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['meaning_of_life', 'vaccines', 'repeat', 'pto_balance', 'rewards_balance', 'new_card', 'gas_type', 'flight_status', 'traffic', 'uber', 'pin_change', 'tell_joke', 'reminder_update', 'what_can_i_ask_you', 'cancel_reservation', 'flight_status']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5379, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [15]
current labels : share_location
pair class : ['share_location']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [10]
current labels : whisper_mode
pair class : ['whisper_mode']
mask: [False False False False False False False False False False  True False
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 10]
['share_location', 'who_do_you_work_for', 'shopping_list', 'calendar', 'fun_fact', 'card_declined', 'mpg', 'credit_limit', 'travel_alert', 'whisper_mode', 'whisper_mode', 'oil_change_how', 'credit_score', 'direct_deposit', 'todo_list', 'share_location']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0712, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : next_holiday
pair class : ['next_holiday']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['change_accent', 'change_volume', 'plug_type', 'calculator', 'restaurant_suggestion', 'rollover_401k', 'confirm_reservation', 'sync_device', 'ingredients_list', 'shopping_list_update', 'ingredient_substitution', 'report_fraud', 'next_holiday', 'weather', 'todo_list', 'next_holiday']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5099, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : accept_reservations
pair class : ['accept_reservations']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['change_user_name', 'accept_reservations', 'last_maintenance', 'no', 'new_card', 'calendar_update', 'cancel_reservation', 'what_is_your_name', 'restaurant_reservation', 'meeting_schedule', 'pin_change', 'what_are_your_hobbies', 'shopping_list', 'what_song', 'cook_time', 'accept_reservations']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5145, grad_fn=<DivBackward0>)

[20,    30] loss_total: 41.725 loss_supervised_contrasive:  11.705 loss_intent :2255.160 
skip_time: 0
total : 923
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : replacement_card_duration
pair class : ['replacement_card_duration']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['order', 'calendar', 'transactions', 'directions', 'no', 'what_can_i_ask_you', 'do_you_have_pets', 'vaccines', 'replacement_card_duration', 'food_last', 'freeze_account', 'date', 'credit_score', 'w2', 'change_language', 'replacement_card_duration']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5005, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [15]
current labels : whisper_mode
pair class : ['whisper_mode']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [7]
current labels : damaged_card
pair class : ['damaged_card']
mask: [False False False False False False False  True False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 7]
['whisper_mode', 'distance', 'where_are_you_from', 'flip_coin', 'order', 'order_checks', 'damaged_card', 'damaged_card', 'last_maintenance', 'change_volume', 'reminder_update', 'who_made_you', 'insurance', 'meaning_of_life', 'balance', 'whisper_mode']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0643, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [15]
current labels : calendar_update
pair class : ['calendar_update']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['calendar_update', 'find_phone', 'shopping_list_update', 'credit_limit_change', 'change_speed', 'definition', 'gas_type', 'routing', 'change_language', 'roll_dice', 'reminder', 'greeting', 'goodbye', 'transactions', 'pto_balance', 'calendar_update']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5268, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : travel_suggestion
pair class : ['travel_suggestion']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['directions', 'credit_limit_change', 'card_declined', 'report_lost_card', 'change_speed', 'restaurant_suggestion', 'exchange_rate', 'rewards_balance', 'thank_you', 'travel_suggestion', 'order_checks', 'confirm_reservation', 'oil_change_when', 'change_accent', 'timezone', 'travel_suggestion']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5438, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : transfer
pair class : ['transfer']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [14]
current labels : sync_device
pair class : ['sync_device']
mask: [False False False False False False False False False False False False
 False False  True False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 14]
['insurance_change', 'current_location', 'text', 'goodbye', 'international_visa', 'directions', 'account_blocked', 'no', 'transfer', 'tell_joke', 'flip_coin', 'sync_device', 'replacement_card_duration', 'direct_deposit', 'sync_device', 'transfer']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0782, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : apr
pair class : ['apr']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['distance', 'expiration_date', 'gas_type', 'todo_list_update', 'apr', 'uber', 'direct_deposit', 'oil_change_how', 'plug_type', 'how_old_are_you', 'application_status', 'spelling', 'travel_alert', 'bill_due', 'taxes', 'apr']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5282, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [6]
current labels : yes
pair class : ['yes']
mask: [False False False False False False  True False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : reset_settings
pair class : ['reset_settings']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [6, 15]
['where_are_you_from', 'schedule_maintenance', 'calories', 'yes', 'what_is_your_name', 'interest_rate', 'yes', 'direct_deposit', 'reset_settings', 'thank_you', 'greeting', 'insurance', 'what_can_i_ask_you', 'spelling', 'reminder', 'reset_settings']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0709, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [15]
current labels : tell_joke
pair class : ['tell_joke']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['tell_joke', 'transactions', 'book_flight', 'calendar', 'schedule_maintenance', 'calendar_update', 'restaurant_reviews', 'flip_coin', 'card_declined', 'account_blocked', 'vaccines', 'rewards_balance', 'expiration_date', 'schedule_meeting', 'order_status', 'tell_joke']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5391, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [15]
current labels : change_user_name
pair class : ['change_user_name']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [12]
current labels : recipe
pair class : ['recipe']
mask: [False False False False False False False False False False False False
  True False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 12]
['change_user_name', 'recipe', 'bill_due', 'bill_balance', 'ingredients_list', 'pto_used', 'what_is_your_name', 'timer', 'restaurant_suggestion', 'reminder', 'exchange_rate', 'todo_list_update', 'recipe', 'routing', 'find_phone', 'change_user_name']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0662, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : cancel
pair class : ['cancel']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['calendar_update', 'cancel', 'alarm', 'weather', 'spending_history', 'what_can_i_ask_you', 'tire_change', 'do_you_have_pets', 'tell_joke', 'interest_rate', 'insurance_change', 'how_old_are_you', 'flip_coin', 'measurement_conversion', 'shopping_list', 'cancel']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5333, grad_fn=<DivBackward0>)

[20,    40] loss_total: 41.731 loss_supervised_contrasive:  15.850 loss_intent :3006.876 
skip_time: 0
total : 933
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [6]
current labels : todo_list
pair class : ['todo_list']
mask: [False False False False False False  True False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : bill_balance
pair class : ['bill_balance']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [6, 15]
['international_visa', 'how_busy', 'replacement_card_duration', 'order_checks', 'todo_list', 'change_accent', 'todo_list', 'shopping_list_update', 'apr', 'freeze_account', 'credit_limit', 'pto_request', 'rollover_401k', 'redeem_rewards', 'bill_balance', 'bill_balance']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0680, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 13
current skips : [15]
current labels : alarm
pair class : ['alarm']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['interest_rate', 'make_call', 'income', 'definition', 'cancel', 'order', 'calendar', 'w2', 'taxes', 'damaged_card', 'book_hotel', 'timer', 'gas_type', 'alarm', 'are_you_a_bot', 'alarm']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5316, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : distance
pair class : ['distance']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['bill_balance', 'ingredients_list', 'recipe', 'pin_change', 'report_fraud', 'book_flight', 'application_status', 'next_holiday', 'restaurant_reservation', 'restaurant_reviews', 'smart_home', 'maybe', 'distance', 'share_location', 'next_song', 'distance']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5128, grad_fn=<DivBackward0>)

----
masking label debug : ['tire_pressure' 'tire_pressure']
current labels  tire_pressure
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 5
current skips : [10 15]
current labels : tire_pressure
pair class : ['tire_pressure' 'tire_pressure']
mask: [False False False False False False False False False False  True False
 False False False  True]
count: 16
numbers of pairs one label : 2
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [10, 15]
['pay_bill', 'time', 'next_song', 'lost_luggage', 'min_payment', 'tire_pressure', 'change_user_name', 'how_old_are_you', 'application_status', 'flight_status', 'tire_pressure', 'user_name', 'text', 'meal_suggestion', 'last_maintenance', 'tire_pressure']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0717, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [15]
current labels : report_fraud
pair class : ['report_fraud']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['report_fraud', 'freeze_account', 'schedule_meeting', 'update_playlist', 'fun_fact', 'lost_luggage', 'order', 'distance', 'apr', 'who_made_you', 'meal_suggestion', 'exchange_rate', 'schedule_maintenance', 'change_user_name', 'meeting_schedule', 'report_fraud']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5313, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [15]
current labels : nutrition_info
pair class : ['nutrition_info']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['recipe', 'min_payment', 'calculator', 'update_playlist', 'time', 'measurement_conversion', 'accept_reservations', 'nutrition_info', 'change_ai_name', 'weather', 'ingredient_substitution', 'international_fees', 'date', 'reminder', 'are_you_a_bot', 'nutrition_info']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5341, grad_fn=<DivBackward0>)

----
masking label debug : ['how_busy' 'how_busy']
current labels  how_busy
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 0
current skips : [ 1 13]
current labels : how_busy
pair class : ['how_busy' 'how_busy']
mask: [False  True False False False False False False False False False False
 False  True]
count: 14
numbers of pairs one label : 2
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [1, 13]
['how_busy', 'how_busy', 'next_holiday', 'travel_notification', 'what_song', 'taxes', 'vaccines', 'no', 'expiration_date', 'thank_you', 'goodbye', 'jump_start', 'repeat', 'how_busy']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([14, 14])
after summing res.shape : torch.Size([])
res.shape : torch.Size([14, 14])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.9405, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [15]
current labels : book_flight
pair class : ['book_flight']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['play_music', 'freeze_account', 'expiration_date', 'food_last', 'gas_type', 'change_ai_name', 'book_flight', 'travel_alert', 'todo_list', 'measurement_conversion', 'user_name', 'travel_notification', 'restaurant_reservation', 'payday', 'credit_score', 'book_flight']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5150, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [15]
current labels : spending_history
pair class : ['spending_history']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['goodbye', 'how_old_are_you', 'what_is_your_name', 'account_blocked', 'no', 'reminder', 'measurement_conversion', 'last_maintenance', 'balance', 'current_location', 'spending_history', 'calculator', 'ingredient_substitution', 'greeting', 'time', 'spending_history']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5366, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [7]
current labels : directions
pair class : ['directions']
mask: [False False False False False False False  True False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : ingredients_list
pair class : ['ingredients_list']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [12]
current labels : change_accent
pair class : ['change_accent']
mask: [False False False False False False False False False False False False
  True False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [7, 15, 12]
['do_you_have_pets', 'directions', 'vaccines', 'reset_settings', 'ingredients_list', 'who_made_you', 'distance', 'directions', 'calories', 'change_accent', 'repeat', 'book_hotel', 'change_accent', 'restaurant_reviews', 'report_fraud', 'ingredients_list']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.1089, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : bill_due
pair class : ['bill_due']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['shopping_list_update', 'date', 'accept_reservations', 'travel_notification', 'bill_due', 'what_song', 'flight_status', 'find_phone', 'reminder', 'no', 'carry_on', 'distance', 'insurance_change', 'car_rental', 'bill_balance', 'bill_due']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5207, grad_fn=<DivBackward0>)

----
masking label debug : ['meeting_schedule' 'meeting_schedule']
current labels  meeting_schedule
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 1
current skips : [13 15]
current labels : meeting_schedule
pair class : ['meeting_schedule' 'meeting_schedule']
mask: [False False False False False False False False False False False False
 False  True False  True]
count: 16
numbers of pairs one label : 2
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [13, 15]
['pto_balance', 'meeting_schedule', 'exchange_rate', 'pto_used', 'travel_suggestion', 'user_name', 'damaged_card', 'next_song', 'vaccines', 'uber', 'how_busy', 'what_is_your_name', 'calculator', 'meeting_schedule', 'insurance', 'meeting_schedule']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0885, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [15]
current labels : distance
pair class : ['distance']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [11]
current labels : todo_list
pair class : ['todo_list']
mask: [False False False False False False False False False False False  True
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 11]
['taxes', 'spending_history', 'recipe', 'distance', 'confirm_reservation', 'smart_home', 'carry_on', 'gas_type', 'spelling', 'direct_deposit', 'todo_list', 'todo_list', 'report_lost_card', 'travel_suggestion', 'pin_change', 'distance']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0597, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : directions
pair class : ['directions']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [10]
current labels : ingredient_substitution
pair class : ['ingredient_substitution']
mask: [False False False False False False False False False False  True False
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 10]
['what_are_your_hobbies', 'directions', 'income', 'calendar_update', 'ingredient_substitution', 'lost_luggage', 'where_are_you_from', 'smart_home', 'payday', 'do_you_have_pets', 'ingredient_substitution', 'interest_rate', 'pin_change', 'spelling', 'reset_settings', 'directions']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0733, grad_fn=<DivBackward0>)

----
masking label debug : ['todo_list_update' 'todo_list_update']
current labels  todo_list_update
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 0
current skips : [3 7]
current labels : todo_list_update
pair class : ['todo_list_update' 'todo_list_update']
mask: [False False False  True False False False  True False False False False
 False False False False]
count: 16
numbers of pairs one label : 2
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [15]
current labels : next_song
pair class : ['next_song']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [13]
current labels : what_song
pair class : ['what_song']
mask: [False False False False False False False False False False False False
 False  True False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 4
pairs see from labels :  4
All skippings : [3, 7, 15, 13]
['todo_list_update', 'who_do_you_work_for', 'what_are_your_hobbies', 'todo_list_update', 'calendar', 'next_song', 'meal_suggestion', 'todo_list_update', 'what_song', 'pto_request', 'expiration_date', 'traffic', 'calories', 'what_song', 'tire_change', 'next_song']
---------------------------------------------
concatenate got h_i : torch.Size([4, 768])
concatenate got h_j :  torch.Size([4, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([4, 1])
pos_sim.shape : torch.Size([4])
len(neg) : 4
loss_s_cl: tensor(0.6918, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [13]
current labels : bill_due
pair class : ['bill_due']
mask: [False False False False False False False False False False False False
 False  True False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [15]
current labels : alarm
pair class : ['alarm']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [13, 15]
['flip_coin', 'credit_limit_change', 'find_phone', 'user_name', 'who_do_you_work_for', 'bill_due', 'alarm', 'nutrition_info', 'yes', 'new_card', 'rewards_balance', 'replacement_card_duration', 'w2', 'bill_due', 'bill_balance', 'alarm']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0673, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : no
pair class : ['no']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [14]
current labels : cancel
pair class : ['cancel']
mask: [False False False False False False False False False False False False
 False False  True False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 14]
['spelling', 'no', 'weather', 'new_card', 'timer', 'cancel', 'restaurant_reviews', 'balance', 'account_blocked', 'order', 'meal_suggestion', 'share_location', 'taxes', 'flight_status', 'cancel', 'no']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0748, grad_fn=<DivBackward0>)

[21,    10] loss_total: 40.459 loss_supervised_contrasive:  2.874 loss_intent :751.717 
skip_time: 0
total : 950
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : change_speed
pair class : ['change_speed']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['translate', 'change_user_name', 'interest_rate', 'ingredient_substitution', 'change_speed', 'cancel', 'yes', 'oil_change_when', 'damaged_card', 'plug_type', 'confirm_reservation', 'pto_request', 'travel_alert', 'tell_joke', 'report_fraud', 'change_speed']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5347, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [15]
current labels : restaurant_suggestion
pair class : ['restaurant_suggestion']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['apr', 'min_payment', 'roll_dice', 'shopping_list', 'thank_you', 'restaurant_suggestion', 'share_location', 'bill_balance', 'date', 'who_do_you_work_for', 'oil_change_how', 'redeem_rewards', 'order_status', 'change_accent', 'reset_settings', 'restaurant_suggestion']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5453, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : food_last
pair class : ['food_last']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['replacement_card_duration', 'food_last', 'date', 'expiration_date', 'w2', 'international_visa', 'mpg', 'vaccines', 'are_you_a_bot', 'weather', 'traffic', 'play_music', 'flip_coin', 'lost_luggage', 'jump_start', 'food_last']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5388, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [15]
current labels : where_are_you_from
pair class : ['where_are_you_from']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [14]
current labels : exchange_rate
pair class : ['exchange_rate']
mask: [False False False False False False False False False False False False
 False False  True False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 14]
['how_busy', 'cancel', 'spelling', 'book_flight', 'improve_credit_score', 'where_are_you_from', 'restaurant_reservation', 'exchange_rate', 'order_checks', 'find_phone', 'nutrition_info', 'reminder', 'damaged_card', 'how_old_are_you', 'exchange_rate', 'where_are_you_from']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0705, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : what_are_your_hobbies
pair class : ['what_are_your_hobbies']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['goodbye', 'change_ai_name', 'what_are_your_hobbies', 'change_volume', 'ingredients_list', 'definition', 'translate', 'reminder_update', 'insurance', 'change_user_name', 'pay_bill', 'share_location', 'yes', 'restaurant_reviews', 'book_hotel', 'what_are_your_hobbies']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5318, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : pto_request_status
pair class : ['pto_request_status']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['report_lost_card', 'nutrition_info', 'restaurant_suggestion', 'account_blocked', 'pto_request', 'routing', 'are_you_a_bot', 'car_rental', 'pto_request_status', 'no', 'share_location', 'international_visa', 'food_last', 'credit_limit_change', 'play_music', 'pto_request_status']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5347, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [15]
current labels : mpg
pair class : ['mpg']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['mpg', 'reminder', 'directions', 'report_fraud', 'update_playlist', 'shopping_list', 'change_language', 'improve_credit_score', 'shopping_list_update', 'tell_joke', 'repeat', 'gas', 'order_status', 'calendar', 'gas_type', 'mpg']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5228, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [15]
current labels : meaning_of_life
pair class : ['meaning_of_life']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [13]
current labels : schedule_maintenance
pair class : ['schedule_maintenance']
mask: [False False False False False False False False False False False False
 False  True False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 13]
['pto_used', 'change_ai_name', 'transactions', 'meaning_of_life', 'min_payment', 'taxes', 'thank_you', 'international_visa', 'transfer', 'recipe', 'routing', 'how_old_are_you', 'schedule_maintenance', 'schedule_maintenance', 'shopping_list_update', 'meaning_of_life']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0698, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [15]
current labels : pto_request_status
pair class : ['pto_request_status']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['order', 'oil_change_when', 'travel_notification', 'thank_you', 'update_playlist', 'fun_fact', 'pto_request_status', 'credit_limit_change', 'income', 'what_song', 'travel_alert', 'last_maintenance', 'are_you_a_bot', 'calendar_update', 'card_declined', 'pto_request_status']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5473, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [15]
current labels : pto_used
pair class : ['pto_used']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [8]
current labels : greeting
pair class : ['greeting']
mask: [False False False False False False False False  True False False False
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 8]
['pto_used', 'routing', 'improve_credit_score', 'whisper_mode', 'last_maintenance', 'greeting', 'definition', 'meeting_schedule', 'greeting', 'goodbye', 'thank_you', 'travel_suggestion', 'cancel_reservation', 'how_busy', 'no', 'pto_used']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0785, grad_fn=<DivBackward0>)

[21,    20] loss_total: 42.083 loss_supervised_contrasive:  7.371 loss_intent :1503.422 
skip_time: 0
total : 960
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : recipe
pair class : ['recipe']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['timezone', 'min_payment', 'change_volume', 'what_can_i_ask_you', 'find_phone', 'how_busy', 'whisper_mode', 'traffic', 'balance', 'restaurant_reviews', 'income', 'schedule_meeting', 'recipe', 'user_name', 'redeem_rewards', 'recipe']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5235, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [9]
current labels : todo_list_update
pair class : ['todo_list_update']
mask: [False False False False False False False False False  True False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : carry_on
pair class : ['carry_on']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [9, 15]
['what_can_i_ask_you', 'maybe', 'pay_bill', 'direct_deposit', 'application_status', 'text', 'todo_list_update', 'meeting_schedule', 'damaged_card', 'todo_list_update', 'book_flight', 'repeat', 'pto_request', 'tire_pressure', 'carry_on', 'carry_on']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0651, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [10]
current labels : insurance
pair class : ['insurance']
mask: [False False False False False False False False False False  True False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : sync_device
pair class : ['sync_device']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [10, 15]
['meaning_of_life', 'next_song', 'insurance', 'uber', 'change_ai_name', 'change_speed', 'recipe', 'apr', 'whisper_mode', 'what_is_your_name', 'insurance', 'change_volume', 'sync_device', 'smart_home', 'todo_list', 'sync_device']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0618, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [11]
current labels : oil_change_how
pair class : ['oil_change_how']
mask: [False False False False False False False False False False False  True
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : gas
pair class : ['gas']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [11, 15]
['book_hotel', 'update_playlist', 'insurance_change', 'credit_limit', 'expiration_date', 'cancel_reservation', 'oil_change_how', 'report_lost_card', 'gas', 'replacement_card_duration', 'user_name', 'oil_change_how', 'what_song', 'change_user_name', 'measurement_conversion', 'gas']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0748, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [12]
current labels : pto_request_status
pair class : ['pto_request_status']
mask: [False False False False False False False False False False False False
  True False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : meaning_of_life
pair class : ['meaning_of_life']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [13]
current labels : flight_status
pair class : ['flight_status']
mask: [False False False False False False False False False False False False
 False  True False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [12, 15, 13]
['cook_time', 'pto_request_status', 'meaning_of_life', 'do_you_have_pets', 'change_user_name', 'travel_suggestion', 'confirm_reservation', 'flight_status', 'restaurant_suggestion', 'weather', 'book_hotel', 'lost_luggage', 'pto_request_status', 'flight_status', 'credit_limit', 'meaning_of_life']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.1112, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [3]
current labels : who_made_you
pair class : ['who_made_you']
mask: [False False False  True False False False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : min_payment
pair class : ['min_payment']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [3, 15]
['directions', 'who_made_you', 'calories', 'who_made_you', 'schedule_meeting', 'calculator', 'whisper_mode', 'how_busy', 'pay_bill', 'gas_type', 'insurance_change', 'gas', 'reminder_update', 'yes', 'min_payment', 'min_payment']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0586, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : credit_score
pair class : ['credit_score']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 13
current skips : [14]
current labels : roll_dice
pair class : ['roll_dice']
mask: [False False False False False False False False False False False False
 False False  True False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 14]
['report_lost_card', 'where_are_you_from', 'car_rental', 'confirm_reservation', 'timer', 'todo_list', 'time', 'travel_notification', 'card_declined', 'bill_balance', 'flip_coin', 'calculator', 'credit_score', 'roll_dice', 'roll_dice', 'credit_score']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0695, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [15]
current labels : next_holiday
pair class : ['next_holiday']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['pay_bill', 'international_fees', 'travel_suggestion', 'next_holiday', 'schedule_meeting', 'make_call', 'jump_start', 'restaurant_suggestion', 'min_payment', 'ingredient_substitution', 'report_fraud', 'carry_on', 'order_status', 'expiration_date', 'gas', 'next_holiday']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5416, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [4]
current labels : weather
pair class : ['weather']
mask: [False False False False  True False False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [5]
current labels : uber
pair class : ['uber']
mask: [False False False False False  True False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : what_are_your_hobbies
pair class : ['what_are_your_hobbies']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [4, 5, 15]
['weather', 'flight_status', 'meaning_of_life', 'uber', 'weather', 'uber', 'payday', 'yes', 'rollover_401k', 'make_call', 'sync_device', 'cancel', 'calories', 'measurement_conversion', 'what_are_your_hobbies', 'what_are_your_hobbies']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.1132, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [15]
current labels : shopping_list_update
pair class : ['shopping_list_update']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['timezone', 'insurance_change', 'maybe', 'goodbye', 'shopping_list', 'shopping_list_update', 'date', 'what_are_your_hobbies', 'timer', 'rewards_balance', 'card_declined', 'pin_change', 'redeem_rewards', 'calendar', 'transfer', 'shopping_list_update']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5219, grad_fn=<DivBackward0>)

[21,    30] loss_total: 40.500 loss_supervised_contrasive:  10.285 loss_intent :2255.148 
skip_time: 0
total : 970
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : plug_type
pair class : ['plug_type']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['recipe', 'bill_due', 'balance', 'book_flight', 'order_status', 'replacement_card_duration', 'interest_rate', 'schedule_maintenance', 'tire_change', 'fun_fact', 'play_music', 'freeze_account', 'plug_type', 'schedule_meeting', 'who_do_you_work_for', 'plug_type']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5266, grad_fn=<DivBackward0>)

----
masking label debug : ['change_speed' 'change_speed']
current labels  change_speed
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 6
current skips : [11 15]
current labels : change_speed
pair class : ['change_speed' 'change_speed']
mask: [False False False False False False False False False False False  True
 False False False  True]
count: 16
numbers of pairs one label : 2
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [11, 15]
['balance', 'timer', 'gas', 'definition', 'reminder_update', 'ingredients_list', 'change_speed', 'change_language', 'repeat', 'transfer', 'exchange_rate', 'change_speed', 'reset_settings', 'w2', 'tire_pressure', 'change_speed']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0747, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : who_made_you
pair class : ['who_made_you']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['restaurant_reservation', 'find_phone', 'who_made_you', 'mpg', 'change_accent', 'direct_deposit', 'cancel_reservation', 'pto_request', 'account_blocked', 'oil_change_when', 'pto_balance', 'text', 'accept_reservations', 'traffic', 'food_last', 'who_made_you']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5226, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : order
pair class : ['order']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['order_checks', 'order', 'cancel_reservation', 'sync_device', 'credit_limit_change', 'credit_limit', 'international_visa', 'reset_settings', 'redeem_rewards', 'alarm', 'payday', 'pto_balance', 'repeat', 'timezone', 'calendar_update', 'order']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5524, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [11]
current labels : what_can_i_ask_you
pair class : ['what_can_i_ask_you']
mask: [False False False False False False False False False False False  True
 False False False False]
count: 16
numbers of pairs one label : 1
----
masking label debug : ['rewards_balance' 'rewards_balance']
current labels  rewards_balance
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 3
current skips : [13 15]
current labels : rewards_balance
pair class : ['rewards_balance' 'rewards_balance']
mask: [False False False False False False False False False False False False
 False  True False  True]
count: 16
numbers of pairs one label : 2
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [11, 13, 15]
['exchange_rate', 'what_can_i_ask_you', 'bill_balance', 'rewards_balance', 'accept_reservations', 'ingredients_list', 'vaccines', 'confirm_reservation', 'alarm', 'oil_change_when', 'routing', 'what_can_i_ask_you', 'thank_you', 'rewards_balance', 'insurance_change', 'rewards_balance']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.1102, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : freeze_account
pair class : ['freeze_account']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['change_accent', 'travel_notification', 'reminder', 'jump_start', 'date', 'tire_pressure', 'traffic', 'apr', 'change_language', 'meaning_of_life', 'smart_home', 'change_volume', 'credit_limit', 'direct_deposit', 'freeze_account', 'freeze_account']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5152, grad_fn=<DivBackward0>)

----
masking label debug : ['application_status' 'application_status']
current labels  application_status
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 1
current skips : [13 15]
current labels : application_status
pair class : ['application_status' 'application_status']
mask: [False False False False False False False False False False False False
 False  True False  True]
count: 16
numbers of pairs one label : 2
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [6]
current labels : current_location
pair class : ['current_location']
mask: [False False False False False False  True False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [13, 15, 6]
['definition', 'application_status', 'make_call', 'interest_rate', 'tire_change', 'current_location', 'current_location', 'last_maintenance', 'carry_on', 'mpg', 'report_fraud', 'shopping_list', 'transactions', 'application_status', 'routing', 'application_status']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.1116, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [15]
current labels : flip_coin
pair class : ['flip_coin']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['book_flight', 'text', 'roll_dice', 'car_rental', 'rollover_401k', 'alarm', 'flip_coin', 'rewards_balance', 'calendar', 'credit_limit_change', 'make_call', 'apr', 'current_location', 'next_song', 'gas_type', 'flip_coin']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5201, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [15]
current labels : definition
pair class : ['definition']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['spending_history', 'transactions', 'who_do_you_work_for', 'definition', 'plug_type', 'vaccines', 'taxes', 'food_last', 'improve_credit_score', 'international_visa', 'cook_time', 'make_call', 'maybe', 'new_card', 'calories', 'definition']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.6367, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [15]
current labels : translate
pair class : ['translate']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['tell_joke', 'application_status', 'shopping_list', 'translate', 'schedule_maintenance', 'next_holiday', 'update_playlist', 'cancel_reservation', 'credit_limit', 'car_rental', 'time', 'schedule_meeting', 'tire_change', 'calendar', 'rollover_401k', 'translate']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5190, grad_fn=<DivBackward0>)

[21,    40] loss_total: 41.895 loss_supervised_contrasive:  14.594 loss_intent :3006.861 
skip_time: 0
total : 980
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : plug_type
pair class : ['plug_type']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['jump_start', 'spending_history', 'rollover_401k', 'distance', 'redeem_rewards', 'update_playlist', 'order', 'restaurant_suggestion', 'order_checks', 'what_can_i_ask_you', 'mpg', 'plug_type', 'sync_device', 'ingredients_list', 'transactions', 'plug_type']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5283, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [15]
current labels : transactions
pair class : ['transactions']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [10]
current labels : order_checks
pair class : ['order_checks']
mask: [False False False False False False False False False False  True False
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 10]
['replacement_card_duration', 'time', 'alarm', 'share_location', 'tell_joke', 'transactions', 'order_checks', 'income', 'w2', 'translate', 'order_checks', 'transfer', 'apr', 'are_you_a_bot', 'pto_balance', 'transactions']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0702, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : where_are_you_from
pair class : ['where_are_you_from']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['international_fees', 'where_are_you_from', 'timezone', 'change_speed', 'pto_balance', 'accept_reservations', 'income', 'order_status', 'insurance', 'taxes', 'what_is_your_name', 'calendar_update', 'oil_change_how', 'reminder_update', 'account_blocked', 'where_are_you_from']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5393, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [10]
current labels : new_card
pair class : ['new_card']
mask: [False False False False False False False False False False  True False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [8]
current labels : lost_luggage
pair class : ['lost_luggage']
mask: [False False False False False False False False  True False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : shopping_list_update
pair class : ['shopping_list_update']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [10, 8, 15]
['new_card', 'play_music', 'whisper_mode', 'change_language', 'lost_luggage', 'next_holiday', 'calendar_update', 'w2', 'lost_luggage', 'cook_time', 'new_card', 'shopping_list_update', 'order', 'roll_dice', 'oil_change_when', 'shopping_list_update']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.1095, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 13
current skips : [15]
current labels : book_hotel
pair class : ['book_hotel']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['rollover_401k', 'payday', 'tell_joke', 'fun_fact', 'international_fees', 'text', 'next_holiday', 'time', 'card_declined', 'meal_suggestion', 'credit_score', 'how_old_are_you', 'are_you_a_bot', 'book_hotel', 'freeze_account', 'book_hotel']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5004, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [11]
current labels : credit_score
pair class : ['credit_score']
mask: [False False False False False False False False False False False  True
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [14]
current labels : tire_pressure
pair class : ['tire_pressure']
mask: [False False False False False False False False False False False False
 False False  True False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : schedule_maintenance
pair class : ['schedule_maintenance']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [11, 14, 15]
['bill_due', 'maybe', 'credit_score', 'change_ai_name', 'spending_history', 'tire_pressure', 'who_made_you', 'uber', 'schedule_maintenance', 'accept_reservations', 'tire_change', 'credit_score', 'restaurant_reservation', 'next_song', 'tire_pressure', 'schedule_maintenance']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.1108, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [13]
current labels : restaurant_reviews
pair class : ['restaurant_reviews']
mask: [False False False False False False False False False False False False
 False  True]
count: 14
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [13]
['jump_start', 'goodbye', 'interest_rate', 'transfer', 'direct_deposit', 'freeze_account', 'reminder_update', 'fun_fact', 'nutrition_info', 'last_maintenance', 'card_declined', 'restaurant_reviews', 'do_you_have_pets', 'restaurant_reviews']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([14, 14])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.2967, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [10]
current labels : tire_pressure
pair class : ['tire_pressure']
mask: [False False False False False False False False False False  True False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : greeting
pair class : ['greeting']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [6]
current labels : redeem_rewards
pair class : ['redeem_rewards']
mask: [False False False False False False  True False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [13]
current labels : definition
pair class : ['definition']
mask: [False False False False False False False False False False False False
 False  True False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 4
pairs see from labels :  4
All skippings : [10, 15, 6, 13]
['bill_due', 'tire_pressure', 'greeting', 'redeem_rewards', 'definition', 'mpg', 'redeem_rewards', 'are_you_a_bot', 'play_music', 'carry_on', 'tire_pressure', 'spending_history', 'lost_luggage', 'definition', 'text', 'greeting']
---------------------------------------------
concatenate got h_i : torch.Size([4, 768])
concatenate got h_j :  torch.Size([4, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([4, 1])
pos_sim.shape : torch.Size([4])
len(neg) : 4
loss_s_cl: tensor(0.6899, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [15]
current labels : carry_on
pair class : ['carry_on']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [8]
current labels : tell_joke
pair class : ['tell_joke']
mask: [False False False False False False False False  True False False False
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 8]
['carry_on', 'what_are_your_hobbies', 'todo_list', 'calendar_update', 'oil_change_how', 'tell_joke', 'alarm', 'credit_limit_change', 'tell_joke', 'order_checks', 'measurement_conversion', 'travel_suggestion', 'fun_fact', 'goodbye', 'pto_used', 'carry_on']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0699, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [8]
current labels : yes
pair class : ['yes']
mask: [False False False False False False False False  True False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : tire_change
pair class : ['tire_change']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [8, 15]
['shopping_list_update', 'account_blocked', 'yes', 'pay_bill', 'reminder_update', 'weather', 'balance', 'direct_deposit', 'yes', 'distance', 'no', 'tire_change', 'tire_pressure', 'last_maintenance', 'routing', 'tire_change']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0702, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [15]
current labels : credit_score
pair class : ['credit_score']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [13]
current labels : last_maintenance
pair class : ['last_maintenance']
mask: [False False False False False False False False False False False False
 False  True False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 13]
['credit_score', 'transfer', 'recipe', 'last_maintenance', 'lost_luggage', 'who_do_you_work_for', 'fun_fact', 'flight_status', 'find_phone', 'change_user_name', 'rewards_balance', 'share_location', 'cancel', 'last_maintenance', 'card_declined', 'credit_score']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0734, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [15]
current labels : shopping_list
pair class : ['shopping_list']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['reminder', 'rollover_401k', 'order_status', 'freeze_account', 'change_ai_name', 'carry_on', 'shopping_list', 'meal_suggestion', 'expiration_date', 'whisper_mode', 'distance', 'account_blocked', 'lost_luggage', 'application_status', 'transactions', 'shopping_list']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5304, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : calendar
pair class : ['calendar']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['rollover_401k', 'insurance_change', 'where_are_you_from', 'taxes', 'replacement_card_duration', 'pto_balance', 'weather', 'reminder', 'uber', 'change_volume', 'pto_used', 'who_made_you', 'calendar', 'interest_rate', 'update_playlist', 'calendar']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5228, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : oil_change_when
pair class : ['oil_change_when']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['restaurant_suggestion', 'meal_suggestion', 'restaurant_reviews', 'what_are_your_hobbies', 'gas_type', 'pto_used', 'user_name', 'todo_list_update', 'change_speed', 'oil_change_when', 'uber', 'taxes', 'timer', 'reminder', 'tire_change', 'oil_change_when']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5371, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : gas_type
pair class : ['gas_type']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [11]
current labels : report_lost_card
pair class : ['report_lost_card']
mask: [False False False False False False False False False False False  True
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 11]
['update_playlist', 'measurement_conversion', 'gas_type', 'spending_history', 'report_lost_card', 'restaurant_reviews', 'meal_suggestion', 'exchange_rate', 'what_is_your_name', 'confirm_reservation', 'direct_deposit', 'report_lost_card', 'credit_limit_change', 'share_location', 'tell_joke', 'gas_type']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0661, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : how_busy
pair class : ['how_busy']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['travel_suggestion', 'replacement_card_duration', 'how_busy', 'restaurant_reservation', 'shopping_list', 'carry_on', 'roll_dice', 'w2', 'meeting_schedule', 'goodbye', 'payday', 'pay_bill', 'uber', 'date', 'application_status', 'how_busy']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5391, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : order_checks
pair class : ['order_checks']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['schedule_maintenance', 'restaurant_suggestion', 'apr', 'change_language', 'calendar', 'do_you_have_pets', 'pto_balance', 'timer', 'last_maintenance', 'balance', 'yes', 'account_blocked', 'taxes', 'todo_list', 'order_checks', 'order_checks']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5410, grad_fn=<DivBackward0>)

[22,    10] loss_total: 41.251 loss_supervised_contrasive:  3.664 loss_intent :751.733 
skip_time: 0
total : 997
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [15]
current labels : meaning_of_life
pair class : ['meaning_of_life']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['income', 'jump_start', 'what_song', 'meaning_of_life', 'shopping_list_update', 'schedule_maintenance', 'translate', 'credit_score', 'transfer', 'oil_change_when', 'find_phone', 'tire_change', 'distance', 'calendar', 'w2', 'meaning_of_life']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5238, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [15]
current labels : pin_change
pair class : ['pin_change']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['change_user_name', 'reminder_update', 'shopping_list', 'pto_request', 'flip_coin', 'food_last', 'insurance', 'pin_change', 'spelling', 'restaurant_suggestion', 'credit_limit', 'travel_alert', 'pto_used', 'goodbye', 'pto_balance', 'pin_change']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5415, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [10]
current labels : how_old_are_you
pair class : ['how_old_are_you']
mask: [False False False False False False False False False False  True False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : fun_fact
pair class : ['fun_fact']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [14]
current labels : current_location
pair class : ['current_location']
mask: [False False False False False False False False False False False False
 False False  True False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [10, 15, 14]
['cook_time', 'balance', 'timezone', 'directions', 'smart_home', 'thank_you', 'text', 'how_old_are_you', 'payday', 'translate', 'how_old_are_you', 'fun_fact', 'current_location', 'lost_luggage', 'current_location', 'fun_fact']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.1139, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [12]
current labels : reminder_update
pair class : ['reminder_update']
mask: [False False False False False False False False False False False False
  True False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [15]
current labels : who_made_you
pair class : ['who_made_you']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [12, 15]
['make_call', 'reset_settings', 'oil_change_how', 'ingredients_list', 'measurement_conversion', 'shopping_list', 'timezone', 'reminder_update', 'todo_list_update', 'card_declined', 'who_made_you', 'spelling', 'reminder_update', 'sync_device', 'repeat', 'who_made_you']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0760, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : international_visa
pair class : ['international_visa']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['tire_change', 'whisper_mode', 'change_accent', 'goodbye', 'international_visa', 'what_can_i_ask_you', 'damaged_card', 'ingredients_list', 'what_song', 'find_phone', 'min_payment', 'insurance', 'reset_settings', 'vaccines', 'confirm_reservation', 'international_visa']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5133, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : where_are_you_from
pair class : ['where_are_you_from']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [13]
current labels : who_do_you_work_for
pair class : ['who_do_you_work_for']
mask: [False False False False False False False False False False False False
 False  True False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [12]
current labels : flip_coin
pair class : ['flip_coin']
mask: [False False False False False False False False False False False False
  True False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [15, 13, 12]
['book_flight', 'where_are_you_from', 'do_you_have_pets', 'travel_notification', 'new_card', 'report_fraud', 'pto_request', 'bill_due', 'who_do_you_work_for', 'flip_coin', 'traffic', 'greeting', 'flip_coin', 'who_do_you_work_for', 'confirm_reservation', 'where_are_you_from']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.1126, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [10]
current labels : ingredient_substitution
pair class : ['ingredient_substitution']
mask: [False False False False False False False False False False  True False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [15]
current labels : taxes
pair class : ['taxes']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [12]
current labels : plug_type
pair class : ['plug_type']
mask: [False False False False False False False False False False False False
  True False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [10, 15, 12]
['shopping_list_update', 'ingredient_substitution', 'play_music', 'order', 'new_card', 'taxes', 'mpg', 'weather', 'nutrition_info', 'plug_type', 'ingredient_substitution', 'are_you_a_bot', 'plug_type', 'gas', 'text', 'taxes']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.1104, grad_fn=<DivBackward0>)

----
masking label debug : ['rewards_balance' 'rewards_balance']
current labels  rewards_balance
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 2
current skips : [14 15]
current labels : rewards_balance
pair class : ['rewards_balance' 'rewards_balance']
mask: [False False False False False False False False False False False False
 False False  True  True]
count: 16
numbers of pairs one label : 2
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [14, 15]
['cancel', 'report_fraud', 'rewards_balance', 'routing', 'reminder', 'new_card', 'current_location', 'schedule_meeting', 'credit_score', 'what_song', 'transfer', 'who_do_you_work_for', 'confirm_reservation', 'international_fees', 'rewards_balance', 'rewards_balance']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0734, grad_fn=<DivBackward0>)

----
masking label debug : ['insurance_change' 'insurance_change']
current labels  insurance_change
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 6
current skips : [10 15]
current labels : insurance_change
pair class : ['insurance_change' 'insurance_change']
mask: [False False False False False False False False False False  True False
 False False False  True]
count: 16
numbers of pairs one label : 2
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [13]
current labels : definition
pair class : ['definition']
mask: [False False False False False False False False False False False False
 False  True False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [10, 15, 13]
['shopping_list', 'apr', 'cancel', 'nutrition_info', 'share_location', 'greeting', 'insurance_change', 'meaning_of_life', 'direct_deposit', 'weather', 'insurance_change', 'definition', 'yes', 'definition', 'timezone', 'insurance_change']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.1115, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : lost_luggage
pair class : ['lost_luggage']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['international_fees', 'sync_device', 'order_status', 'flight_status', 'apr', 'calculator', 'book_hotel', 'uber', 'greeting', 'mpg', 'book_flight', 'reminder_update', 'lost_luggage', 'tell_joke', 'routing', 'lost_luggage']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5423, grad_fn=<DivBackward0>)

[22,    20] loss_total: 40.658 loss_supervised_contrasive:  6.736 loss_intent :1503.453 
skip_time: 0
total : 1007
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : schedule_maintenance
pair class : ['schedule_maintenance']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['schedule_meeting', 'where_are_you_from', 'travel_suggestion', 'make_call', 'schedule_maintenance', 'insurance_change', 'car_rental', 'thank_you', 'meal_suggestion', 'account_blocked', 'tell_joke', 'w2', 'carry_on', 'order', 'expiration_date', 'schedule_maintenance']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5166, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [10]
current labels : smart_home
pair class : ['smart_home']
mask: [False False False False False False False False False False  True False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : how_busy
pair class : ['how_busy']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [10, 15]
['pay_bill', 'translate', 'meal_suggestion', 'oil_change_when', 'smart_home', 'change_accent', 'meaning_of_life', 'timezone', 'apr', 'yes', 'smart_home', 'how_busy', 'rollover_401k', 'tire_pressure', 'spelling', 'how_busy']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0859, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [14]
current labels : gas_type
pair class : ['gas_type']
mask: [False False False False False False False False False False False False
 False False  True False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : bill_balance
pair class : ['bill_balance']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [14, 15]
['transfer', 'alarm', 'pto_request', 'oil_change_how', 'change_language', 'order_status', 'meeting_schedule', 'gas_type', 'card_declined', 'restaurant_reservation', 'damaged_card', 'travel_suggestion', 'bill_balance', 'change_accent', 'gas_type', 'bill_balance']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0692, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [8]
current labels : time
pair class : ['time']
mask: [False False False False False False False False  True False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [15]
current labels : repeat
pair class : ['repeat']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [8, 15]
['travel_suggestion', 'time', 'what_is_your_name', 'repeat', 'nutrition_info', 'bill_balance', 'what_can_i_ask_you', 'meaning_of_life', 'time', 'date', 'insurance', 'mpg', 'flight_status', 'income', 'cook_time', 'repeat']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0746, grad_fn=<DivBackward0>)

----
masking label debug : ['do_you_have_pets' 'do_you_have_pets']
current labels  do_you_have_pets
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 0
current skips : [ 6 15]
current labels : do_you_have_pets
pair class : ['do_you_have_pets' 'do_you_have_pets']
mask: [False False False False False False  True False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 2
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [9]
current labels : accept_reservations
pair class : ['accept_reservations']
mask: [False False False False False False False False False  True False False
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [6, 15, 9]
['do_you_have_pets', 'meaning_of_life', 'todo_list_update', 'change_language', 'next_song', 'restaurant_reviews', 'do_you_have_pets', 'accept_reservations', 'pin_change', 'accept_reservations', 'spending_history', 'definition', 'payday', 'food_last', 'recipe', 'do_you_have_pets']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.1101, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [15]
current labels : shopping_list_update
pair class : ['shopping_list_update']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['card_declined', 'next_holiday', 'credit_limit_change', 'interest_rate', 'change_accent', 'shopping_list_update', 'tire_pressure', 'alarm', 'make_call', 'pay_bill', 'credit_score', 'nutrition_info', 'mpg', 'traffic', 'date', 'shopping_list_update']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5128, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [7]
current labels : food_last
pair class : ['food_last']
mask: [False False False False False False False  True False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : direct_deposit
pair class : ['direct_deposit']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [7, 15]
['last_maintenance', 'traffic', 'food_last', 'pto_request_status', 'make_call', 'restaurant_suggestion', 'change_ai_name', 'food_last', 'car_rental', 'direct_deposit', 'no', 'bill_balance', 'schedule_meeting', 'update_playlist', 'whisper_mode', 'direct_deposit']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0754, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [2]
current labels : change_speed
pair class : ['change_speed']
mask: [False False  True False False False False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : order
pair class : ['order']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [2, 15]
['smart_home', 'change_speed', 'change_speed', 'restaurant_reservation', 'oil_change_how', 'rewards_balance', 'damaged_card', 'cancel', 'exchange_rate', 'flight_status', 'meeting_schedule', 'transactions', 'alarm', 'pto_request', 'order', 'order']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0723, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [11]
current labels : maybe
pair class : ['maybe']
mask: [False False False False False False False False False False False  True
 False False False False]
count: 16
numbers of pairs one label : 1
----
masking label debug : ['sync_device' 'sync_device']
current labels  sync_device
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 7
current skips : [13 15]
current labels : sync_device
pair class : ['sync_device' 'sync_device']
mask: [False False False False False False False False False False False False
 False  True False  True]
count: 16
numbers of pairs one label : 2
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [11, 13, 15]
['weather', 'maybe', 'what_are_your_hobbies', 'next_song', 'apr', 'restaurant_reservation', 'how_old_are_you', 'sync_device', 'rewards_balance', 'change_volume', 'timer', 'maybe', 'share_location', 'sync_device', 'order_checks', 'sync_device']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.1087, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [11]
current labels : report_fraud
pair class : ['report_fraud']
mask: [False False False False False False False False False False False  True
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [15]
current labels : international_visa
pair class : ['international_visa']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [11, 15]
['report_fraud', 'insurance_change', 'user_name', 'traffic', 'transactions', 'international_visa', 'next_song', 'spelling', 'roll_dice', 'calendar_update', 'directions', 'report_fraud', 'no', 'maybe', 'change_speed', 'international_visa']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0729, grad_fn=<DivBackward0>)

[22,    30] loss_total: 40.156 loss_supervised_contrasive:  9.306 loss_intent :2255.179 
skip_time: 0
total : 1017
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [11]
current labels : next_holiday
pair class : ['next_holiday']
mask: [False False False False False False False False False False False  True
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [6]
current labels : recipe
pair class : ['recipe']
mask: [False False False False False False  True False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : what_can_i_ask_you
pair class : ['what_can_i_ask_you']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [11, 6, 15]
['plug_type', 'are_you_a_bot', 'change_user_name', 'next_holiday', 'international_visa', 'recipe', 'recipe', 'next_song', 'current_location', 'what_can_i_ask_you', 'translate', 'next_holiday', 'credit_limit_change', 'min_payment', 'spending_history', 'what_can_i_ask_you']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.1152, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [7]
current labels : are_you_a_bot
pair class : ['are_you_a_bot']
mask: [False False False False False False False  True False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : balance
pair class : ['balance']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [7, 15]
['cook_time', 'change_speed', 'confirm_reservation', 'calendar', 'vaccines', 'spelling', 'are_you_a_bot', 'are_you_a_bot', 'card_declined', 'how_busy', 'damaged_card', 'change_volume', 'balance', 'flight_status', 'rollover_401k', 'balance']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0692, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [5]
current labels : calculator
pair class : ['calculator']
mask: [False False False False False  True False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : repeat
pair class : ['repeat']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [5, 15]
['update_playlist', 'pto_balance', 'calculator', 'freeze_account', 'repeat', 'calculator', 'calendar_update', 'calories', 'account_blocked', 'directions', 'change_ai_name', 'damaged_card', 'goodbye', 'interest_rate', 'new_card', 'repeat']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0716, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : what_are_your_hobbies
pair class : ['what_are_your_hobbies']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['whisper_mode', 'how_old_are_you', 'book_flight', 'timer', 'thank_you', 'pto_request_status', 'improve_credit_score', 'taxes', 'what_are_your_hobbies', 'directions', 'smart_home', 'bill_balance', 'user_name', 'schedule_meeting', 'distance', 'what_are_your_hobbies']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5297, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [15]
current labels : oil_change_when
pair class : ['oil_change_when']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['who_made_you', 'play_music', 'what_is_your_name', 'pto_request_status', 'find_phone', 'oil_change_when', 'travel_alert', 'application_status', 'exchange_rate', 'expiration_date', 'credit_score', 'freeze_account', 'what_are_your_hobbies', 'repeat', 'meeting_schedule', 'oil_change_when']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5264, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : insurance
pair class : ['insurance']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [8]
current labels : min_payment
pair class : ['min_payment']
mask: [False False False False False False False False  True False False False
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 8]
['restaurant_suggestion', 'timer', 'insurance', 'jump_start', 'min_payment', 'exchange_rate', 'tire_change', 'roll_dice', 'min_payment', 'direct_deposit', 'vaccines', 'calendar', 'where_are_you_from', 'income', 'sync_device', 'insurance']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0706, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 13
current skips : [15]
current labels : cook_time
pair class : ['cook_time']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['next_song', 'do_you_have_pets', 'min_payment', 'time', 'food_last', 'transactions', 'report_lost_card', 'travel_notification', 'international_fees', 'car_rental', 'date', 'restaurant_reviews', 'gas', 'cook_time', 'credit_limit', 'cook_time']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5234, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : credit_limit
pair class : ['credit_limit']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [3]
current labels : travel_alert
pair class : ['travel_alert']
mask: [False False False  True False False False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 3]
['insurance', 'credit_limit', 'travel_alert', 'travel_alert', 'how_busy', 'cancel', 'exchange_rate', 'jump_start', 'income', 'calories', 'redeem_rewards', 'order_status', 'interest_rate', 'todo_list', 'timezone', 'credit_limit']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0762, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : reset_settings
pair class : ['reset_settings']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [8]
current labels : change_ai_name
pair class : ['change_ai_name']
mask: [False False False False False False False False  True False False False
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 8]
['schedule_meeting', 'reset_settings', 'change_user_name', 'international_visa', 'calendar_update', 'change_ai_name', 'fun_fact', 'travel_alert', 'change_ai_name', 'accept_reservations', 'pto_used', 'plug_type', 'reminder', 'replacement_card_duration', 'pin_change', 'reset_settings']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0729, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [15]
current labels : play_music
pair class : ['play_music']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['improve_credit_score', 'rollover_401k', 'book_hotel', 'current_location', 'cancel_reservation', 'play_music', 'text', 'change_volume', 'international_visa', 'pto_request_status', 'nutrition_info', 'oil_change_how', 'change_user_name', 'credit_limit', 'gas', 'play_music']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.6162, grad_fn=<DivBackward0>)

[22,    40] loss_total: 40.953 loss_supervised_contrasive:  12.673 loss_intent :3006.905 
skip_time: 0
total : 1027
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [7]
current labels : what_can_i_ask_you
pair class : ['what_can_i_ask_you']
mask: [False False False False False False False  True False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : redeem_rewards
pair class : ['redeem_rewards']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [7, 15]
['what_can_i_ask_you', 'redeem_rewards', 'traffic', 'change_language', 'who_made_you', 'pto_balance', 'new_card', 'what_can_i_ask_you', 'balance', 'improve_credit_score', 'reset_settings', 'fun_fact', 'translate', 'order', 'cancel_reservation', 'redeem_rewards']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0646, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [9]
current labels : ingredient_substitution
pair class : ['ingredient_substitution']
mask: [False False False False False False False False False  True False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : accept_reservations
pair class : ['accept_reservations']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [9, 15]
['application_status', 'make_call', 'shopping_list_update', 'ingredient_substitution', 'user_name', 'order_status', 'find_phone', 'travel_notification', 'accept_reservations', 'ingredient_substitution', 'calories', 'book_hotel', 'todo_list', 'pay_bill', 'pto_request_status', 'accept_reservations']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0653, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [15]
current labels : what_song
pair class : ['what_song']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [14]
current labels : car_rental
pair class : ['car_rental']
mask: [False False False False False False False False False False False False
 False False  True False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 14]
['application_status', 'distance', 'credit_limit', 'change_language', 'transactions', 'what_song', 'measurement_conversion', 'car_rental', 'oil_change_when', 'improve_credit_score', 'payday', 'next_holiday', 'alarm', 'redeem_rewards', 'car_rental', 'what_song']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0641, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [9]
current labels : calories
pair class : ['calories']
mask: [False False False False False False False False False  True False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : user_name
pair class : ['user_name']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [9, 15]
['share_location', 'calories', 'flip_coin', 'todo_list_update', 'time', 'what_is_your_name', 'who_made_you', 'gas', 'user_name', 'calories', 'how_old_are_you', 'freeze_account', 'ingredients_list', 'play_music', 'improve_credit_score', 'user_name']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0738, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [14]
current labels : expiration_date
pair class : ['expiration_date']
mask: [False False False False False False False False False False False False
 False False  True False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [11]
current labels : roll_dice
pair class : ['roll_dice']
mask: [False False False False False False False False False False False  True
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [15]
current labels : replacement_card_duration
pair class : ['replacement_card_duration']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [14, 11, 15]
['expiration_date', 'next_holiday', 'roll_dice', 'time', 'text', 'schedule_maintenance', 'update_playlist', 'replacement_card_duration', 'what_song', 'ingredients_list', 'thank_you', 'roll_dice', 'order_checks', 'vaccines', 'expiration_date', 'replacement_card_duration']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.1130, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [12]
current labels : pin_change
pair class : ['pin_change']
mask: [False False False False False False False False False False False False
  True False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : international_fees
pair class : ['international_fees']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [12, 15]
['calculator', 'replacement_card_duration', 'where_are_you_from', 'date', 'travel_notification', 'who_do_you_work_for', 'pin_change', 'no', 'international_fees', 'cook_time', 'jump_start', 'transfer', 'pin_change', 'recipe', 'report_lost_card', 'international_fees']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0752, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [9]
current labels : w2
pair class : ['w2']
mask: [False False False False False False False False False  True False False
 False False]
count: 14
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [13]
current labels : vaccines
pair class : ['vaccines']
mask: [False False False False False False False False False False False False
 False  True]
count: 14
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [9, 13]
['gas_type', 'reset_settings', 'report_lost_card', 'change_volume', 'w2', 'vaccines', 'jump_start', 'travel_notification', 'accept_reservations', 'w2', 'thank_you', 'pto_request', 'schedule_maintenance', 'vaccines']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([14, 14])
after summing res.shape : torch.Size([])
res.shape : torch.Size([14, 14])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.9393, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : improve_credit_score
pair class : ['improve_credit_score']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['text', 'distance', 'make_call', 'meeting_schedule', 'redeem_rewards', 'jump_start', 'carry_on', 'what_can_i_ask_you', 'pto_request', 'maybe', 'change_ai_name', 'improve_credit_score', 'update_playlist', 'freeze_account', 'expiration_date', 'improve_credit_score']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5184, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [15]
current labels : flip_coin
pair class : ['flip_coin']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [14]
current labels : definition
pair class : ['definition']
mask: [False False False False False False False False False False False False
 False False  True False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 14]
['timer', 'next_song', 'report_fraud', 'translate', 'plug_type', 'travel_alert', 'flip_coin', 'gas', 'who_made_you', 'tire_change', 'what_can_i_ask_you', 'replacement_card_duration', 'definition', 'measurement_conversion', 'definition', 'flip_coin']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0703, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [15]
current labels : rewards_balance
pair class : ['rewards_balance']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['meaning_of_life', 'book_flight', 'recipe', 'user_name', 'transfer', 'no', 'pto_used', 'rewards_balance', 'account_blocked', 'timezone', 'yes', 'timer', 'directions', 'fun_fact', 'insurance', 'rewards_balance']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5359, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [8]
current labels : tell_joke
pair class : ['tell_joke']
mask: [False False False False False False False False  True False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [15]
current labels : measurement_conversion
pair class : ['measurement_conversion']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [8, 15]
['payday', 'schedule_meeting', 'jump_start', 'timezone', 'tell_joke', 'traffic', 'order_status', 'measurement_conversion', 'tell_joke', 'who_made_you', 'cancel', 'smart_home', 'change_language', 'where_are_you_from', 'reminder', 'measurement_conversion']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0741, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : next_song
pair class : ['next_song']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['where_are_you_from', 'calendar_update', 'next_song', 'new_card', 'weather', 'timer', 'damaged_card', 'restaurant_reservation', 'change_ai_name', 'bill_balance', 'international_fees', 'order_status', 'who_do_you_work_for', 'confirm_reservation', 'spending_history', 'next_song']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5313, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [11]
current labels : calculator
pair class : ['calculator']
mask: [False False False False False False False False False False False  True
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : replacement_card_duration
pair class : ['replacement_card_duration']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [11, 15]
['gas_type', 'calculator', 'lost_luggage', 'redeem_rewards', 'greeting', 'make_call', 'order', 'who_made_you', 'replacement_card_duration', 'restaurant_reviews', 'routing', 'calculator', 'insurance_change', 'vaccines', 'report_lost_card', 'replacement_card_duration']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0725, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [5]
current labels : spelling
pair class : ['spelling']
mask: [False False False False False  True False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [15]
current labels : oil_change_how
pair class : ['oil_change_how']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [5, 15]
['income', 'cancel_reservation', 'travel_alert', 'spelling', 'smart_home', 'spelling', 'timer', 'balance', 'new_card', 'who_made_you', 'oil_change_how', 'cook_time', 'how_old_are_you', 'report_lost_card', 'todo_list_update', 'oil_change_how']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0755, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 13
current skips : [15]
current labels : greeting
pair class : ['greeting']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['routing', 'what_song', 'card_declined', 'shopping_list', 'rewards_balance', 'direct_deposit', 'car_rental', 'how_busy', 'restaurant_reviews', 'order_checks', 'no', 'repeat', 'food_last', 'greeting', 'what_are_your_hobbies', 'greeting']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5413, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : cancel_reservation
pair class : ['cancel_reservation']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['transactions', 'reminder', 'application_status', 'cancel', 'greeting', 'ingredient_substitution', 'report_lost_card', 'pto_used', 'next_song', 'credit_limit_change', 'restaurant_reviews', 'cancel_reservation', 'last_maintenance', 'distance', 'flight_status', 'cancel_reservation']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5414, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [15]
current labels : user_name
pair class : ['user_name']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['no', 'roll_dice', 'taxes', 'user_name', 'change_speed', 'date', 'change_ai_name', 'tire_pressure', 'next_holiday', 'pto_request_status', 'make_call', 'cancel', 'gas', 'shopping_list_update', 'insurance', 'user_name']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5356, grad_fn=<DivBackward0>)

[23,    10] loss_total: 41.735 loss_supervised_contrasive:  4.150 loss_intent :751.716 
skip_time: 0
total : 1044
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [15]
current labels : are_you_a_bot
pair class : ['are_you_a_bot']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['text', 'rewards_balance', 'reminder', 'insurance_change', 'what_are_your_hobbies', 'expiration_date', 'are_you_a_bot', 'oil_change_when', 'meal_suggestion', 'greeting', 'routing', 'taxes', 'payday', 'yes', 'play_music', 'are_you_a_bot']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5087, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [15]
current labels : weather
pair class : ['weather']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['weather', 'freeze_account', 'schedule_maintenance', 'credit_limit', 'accept_reservations', 'how_busy', 'ingredients_list', 'pay_bill', 'recipe', 'update_playlist', 'alarm', 'yes', 'pin_change', 'change_volume', 'share_location', 'weather']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5259, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : income
pair class : ['income']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['schedule_maintenance', 'definition', 'make_call', 'rewards_balance', 'bill_due', 'nutrition_info', 'interest_rate', 'report_fraud', 'w2', 'date', 'measurement_conversion', 'taxes', 'maybe', 'min_payment', 'income', 'income']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5364, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [13]
current labels : change_user_name
pair class : ['change_user_name']
mask: [False False False False False False False False False False False False
 False  True False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : income
pair class : ['income']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [13, 15]
['card_declined', 'balance', 'date', 'smart_home', 'ingredients_list', 'tire_change', 'tell_joke', 'pin_change', 'translate', 'reminder', 'change_user_name', 'income', 'fun_fact', 'change_user_name', 'lost_luggage', 'income']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0765, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [13]
current labels : how_old_are_you
pair class : ['how_old_are_you']
mask: [False False False False False False False False False False False False
 False  True False False]
count: 16
numbers of pairs one label : 1
----
masking label debug : ['translate' 'translate']
current labels  translate
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 8
current skips : [12 15]
current labels : translate
pair class : ['translate' 'translate']
mask: [False False False False False False False False False False False False
  True False False  True]
count: 16
numbers of pairs one label : 2
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [13, 12, 15]
['ingredient_substitution', 'definition', 'gas', 'apr', 'international_fees', 'change_user_name', 'how_old_are_you', 'change_volume', 'translate', 'plug_type', 'travel_notification', 'flight_status', 'translate', 'how_old_are_you', 'book_flight', 'translate']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.1116, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : do_you_have_pets
pair class : ['do_you_have_pets']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['next_holiday', 'do_you_have_pets', 'balance', 'jump_start', 'pto_balance', 'shopping_list_update', 'spelling', 'international_fees', 'transfer', 'alarm', 'rollover_401k', 'goodbye', 'cook_time', 'interest_rate', 'travel_suggestion', 'do_you_have_pets']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5272, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : uber
pair class : ['uber']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [12]
current labels : last_maintenance
pair class : ['last_maintenance']
mask: [False False False False False False False False False False False False
  True False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 12]
['meal_suggestion', 'whisper_mode', 'uber', 'how_busy', 'meeting_schedule', 'alarm', 'cancel_reservation', 'last_maintenance', 'change_accent', 'timezone', 'pto_request', 'current_location', 'last_maintenance', 'food_last', 'share_location', 'uber']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0699, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [15]
current labels : balance
pair class : ['balance']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['update_playlist', 'order_checks', 'shopping_list', 'weather', 'what_are_your_hobbies', 'schedule_maintenance', 'meaning_of_life', 'balance', 'book_flight', 'traffic', 'what_is_your_name', 'spelling', 'car_rental', 'calendar', 'insurance_change', 'balance']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5280, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [15]
current labels : todo_list
pair class : ['todo_list']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [7]
current labels : play_music
pair class : ['play_music']
mask: [False False False False False False False  True False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [10]
current labels : reset_settings
pair class : ['reset_settings']
mask: [False False False False False False False False False False  True False
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [15, 7, 10]
['todo_list', 'whisper_mode', 'who_made_you', 'travel_notification', 'travel_suggestion', 'translate', 'play_music', 'play_music', 'change_language', 'reset_settings', 'reset_settings', 'calendar_update', 'order', 'mpg', 'vaccines', 'todo_list']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.1177, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [11]
current labels : book_hotel
pair class : ['book_hotel']
mask: [False False False False False False False False False False False  True
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [15]
current labels : min_payment
pair class : ['min_payment']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [11, 15]
['tire_pressure', 'oil_change_when', 'what_is_your_name', 'reset_settings', 'fun_fact', 'food_last', 'application_status', 'book_hotel', 'restaurant_reservation', 'restaurant_reviews', 'min_payment', 'book_hotel', 'new_card', 'vaccines', 'order', 'min_payment']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0897, grad_fn=<DivBackward0>)

[23,    20] loss_total: 41.194 loss_supervised_contrasive:  7.759 loss_intent :1503.417 
skip_time: 0
total : 1054
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [9]
current labels : change_volume
pair class : ['change_volume']
mask: [False False False False False False False False False  True False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [15]
current labels : weather
pair class : ['weather']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [9, 15]
['change_volume', 'pay_bill', 'share_location', 'schedule_meeting', 'recipe', 'taxes', 'weather', 'bill_balance', 'calories', 'change_volume', 'tire_change', 'what_song', 'international_visa', 'plug_type', 'gas_type', 'weather']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0761, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [11]
current labels : credit_score
pair class : ['credit_score']
mask: [False False False False False False False False False False False  True
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 13
current skips : [15]
current labels : change_speed
pair class : ['change_speed']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [11, 15]
['ingredients_list', 'nutrition_info', 'credit_score', 'w2', 'direct_deposit', 'directions', 'restaurant_reservation', 'traffic', 'recipe', 'meal_suggestion', 'travel_alert', 'credit_score', 'time', 'change_speed', 'pay_bill', 'change_speed']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0764, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [6]
current labels : schedule_meeting
pair class : ['schedule_meeting']
mask: [False False False False False False  True False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
----
masking label debug : ['confirm_reservation' 'confirm_reservation']
current labels  confirm_reservation
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 8
current skips : [14 15]
current labels : confirm_reservation
pair class : ['confirm_reservation' 'confirm_reservation']
mask: [False False False False False False False False False False False False
 False False  True  True]
count: 16
numbers of pairs one label : 2
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [6, 14, 15]
['rewards_balance', 'uber', 'pto_request_status', 'travel_suggestion', 'schedule_meeting', 'accept_reservations', 'schedule_meeting', 'tire_pressure', 'confirm_reservation', 'do_you_have_pets', 'calendar', 'pin_change', 'calculator', 'bill_balance', 'confirm_reservation', 'confirm_reservation']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.1144, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : travel_notification
pair class : ['travel_notification']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['reset_settings', 'user_name', 'pto_used', 'freeze_account', 'apr', 'spending_history', 'exchange_rate', 'meeting_schedule', 'travel_notification', 'what_are_your_hobbies', 'order_checks', 'insurance_change', 'application_status', 'lost_luggage', 'nutrition_info', 'travel_notification']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5306, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : what_can_i_ask_you
pair class : ['what_can_i_ask_you']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['freeze_account', 'bill_balance', 'maybe', 'sync_device', 'damaged_card', 'insurance', 'spending_history', 'fun_fact', 'user_name', 'what_can_i_ask_you', 'current_location', 'income', 'pto_balance', 'pto_request', 'account_blocked', 'what_can_i_ask_you']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5243, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : pto_request_status
pair class : ['pto_request_status']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['how_old_are_you', 'credit_limit', 'international_fees', 'restaurant_suggestion', 'smart_home', 'last_maintenance', 'what_song', 'card_declined', 'pto_request_status', 'credit_limit_change', 'sync_device', 'spending_history', 'meaning_of_life', 'w2', 'next_holiday', 'pto_request_status']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5384, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [15]
current labels : change_accent
pair class : ['change_accent']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['account_blocked', 'jump_start', 'text', 'change_accent', 'payday', 'international_fees', 'time', 'transactions', 'tire_pressure', 'reminder_update', 'accept_reservations', 'meal_suggestion', 'plug_type', 'todo_list', 'freeze_account', 'change_accent']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5277, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [5]
current labels : current_location
pair class : ['current_location']
mask: [False False False False False  True False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : change_volume
pair class : ['change_volume']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [5, 15]
['change_user_name', 'time', 'current_location', 'carry_on', 'alarm', 'current_location', 'expiration_date', 'pto_request', 'application_status', 'change_volume', 'mpg', 'how_busy', 'change_ai_name', 'update_playlist', 'report_fraud', 'change_volume']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0866, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [5]
current labels : are_you_a_bot
pair class : ['are_you_a_bot']
mask: [False False False False False  True False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : replacement_card_duration
pair class : ['replacement_card_duration']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [11]
current labels : change_accent
pair class : ['change_accent']
mask: [False False False False False False False False False False False  True
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [5, 15, 11]
['redeem_rewards', 'tire_change', 'ingredients_list', 'mpg', 'are_you_a_bot', 'are_you_a_bot', 'sync_device', 'shopping_list_update', 'replacement_card_duration', 'meaning_of_life', 'change_accent', 'change_accent', 'meeting_schedule', 'todo_list_update', 'thank_you', 'replacement_card_duration']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.1083, grad_fn=<DivBackward0>)

----
masking label debug : ['who_do_you_work_for' 'who_do_you_work_for']
current labels  who_do_you_work_for
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 0
current skips : [13 15]
current labels : who_do_you_work_for
pair class : ['who_do_you_work_for' 'who_do_you_work_for']
mask: [False False False False False False False False False False False False
 False  True False  True]
count: 16
numbers of pairs one label : 2
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [13, 15]
['who_do_you_work_for', 'play_music', 'transactions', 'how_old_are_you', 'weather', 'date', 'directions', 'restaurant_suggestion', 'order_status', 'alarm', 'pto_request', 'ingredient_substitution', 'tell_joke', 'who_do_you_work_for', 'nutrition_info', 'who_do_you_work_for']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0601, grad_fn=<DivBackward0>)

[23,    30] loss_total: 40.850 loss_supervised_contrasive:  11.023 loss_intent :2255.126 
skip_time: 0
total : 1064
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [15]
current labels : greeting
pair class : ['greeting']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [9]
current labels : do_you_have_pets
pair class : ['do_you_have_pets']
mask: [False False False False False False False False False  True False False
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 9]
['meeting_schedule', 'replacement_card_duration', 'pin_change', 'yes', 'rollover_401k', 'greeting', 'travel_suggestion', 'food_last', 'do_you_have_pets', 'do_you_have_pets', 'next_holiday', 'schedule_maintenance', 'restaurant_suggestion', 'taxes', 'oil_change_how', 'greeting']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0814, grad_fn=<DivBackward0>)

----
masking label debug : ['flip_coin' 'flip_coin']
current labels  flip_coin
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 11
current skips : [14 15]
current labels : flip_coin
pair class : ['flip_coin' 'flip_coin']
mask: [False False False False False False False False False False False False
 False False  True  True]
count: 16
numbers of pairs one label : 2
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [14, 15]
['travel_alert', 'order_checks', 'traffic', 'repeat', 'change_language', 'car_rental', 'interest_rate', 'play_music', 'oil_change_how', 'current_location', 'roll_dice', 'flip_coin', 'ingredient_substitution', 'new_card', 'flip_coin', 'flip_coin']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0697, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [14]
current labels : international_visa
pair class : ['international_visa']
mask: [False False False False False False False False False False False False
 False False  True False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : spending_history
pair class : ['spending_history']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [14, 15]
['direct_deposit', 'interest_rate', 'damaged_card', 'cancel', 'goodbye', 'reset_settings', 'pin_change', 'international_visa', 'spending_history', 'food_last', 'gas', 'change_ai_name', 'exchange_rate', 'recipe', 'international_visa', 'spending_history']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0725, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [3]
current labels : goodbye
pair class : ['goodbye']
mask: [False False False  True False False False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [15]
current labels : flight_status
pair class : ['flight_status']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [3, 15]
['goodbye', 'find_phone', 'direct_deposit', 'goodbye', 'pto_balance', 'calendar_update', 'flight_status', 'timer', 'insurance', 'credit_score', 'oil_change_how', 'order_status', 'rollover_401k', 'redeem_rewards', 'credit_limit_change', 'flight_status']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0711, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : account_blocked
pair class : ['account_blocked']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['reminder_update', 'exchange_rate', 'where_are_you_from', 'find_phone', 'damaged_card', 'measurement_conversion', 'uber', 'oil_change_when', 'change_speed', 'international_visa', 'thank_you', 'account_blocked', 'are_you_a_bot', 'next_holiday', 'goodbye', 'account_blocked']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5172, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [15]
current labels : change_accent
pair class : ['change_accent']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [10]
current labels : improve_credit_score
pair class : ['improve_credit_score']
mask: [False False False False False False False False False False  True False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [14]
current labels : todo_list_update
pair class : ['todo_list_update']
mask: [False False False False False False False False False False False False
 False False  True False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [15, 10, 14]
['change_accent', 'share_location', 'timezone', 'find_phone', 'income', 'what_is_your_name', 'make_call', 'improve_credit_score', 'todo_list_update', 'gas_type', 'improve_credit_score', 'pto_used', 'min_payment', 'repeat', 'todo_list_update', 'change_accent']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.1115, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [15]
current labels : oil_change_how
pair class : ['oil_change_how']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [14]
current labels : gas_type
pair class : ['gas_type']
mask: [False False False False False False False False False False False False
 False False  True False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 14]
['oil_change_how', 'who_do_you_work_for', 'insurance', 'exchange_rate', 'travel_suggestion', 'time', 'restaurant_reservation', 'gas_type', 'calories', 'damaged_card', 'restaurant_reviews', 'todo_list_update', 'rollover_401k', 'book_hotel', 'gas_type', 'oil_change_how']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0680, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [15]
current labels : maybe
pair class : ['maybe']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['payday', 'accept_reservations', 'min_payment', 'meal_suggestion', 'thank_you', 'maybe', 'car_rental', 'change_speed', 'what_can_i_ask_you', 'calories', 'change_user_name', 'apr', 'credit_limit', 'what_is_your_name', 'directions', 'maybe']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5390, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : no
pair class : ['no']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['gas', 'routing', 'credit_limit', 'sync_device', 'yes', 'order', 'what_can_i_ask_you', 'lost_luggage', 'no', 'restaurant_suggestion', 'application_status', 'thank_you', 'reminder_update', 'todo_list', 'update_playlist', 'no']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.6254, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : repeat
pair class : ['repeat']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['ingredient_substitution', 'what_is_your_name', 'next_song', 'exchange_rate', 'pto_balance', 'whisper_mode', 'change_language', 'international_visa', 'calculator', 'apr', 'cook_time', 'vaccines', 'repeat', 'transfer', 'min_payment', 'repeat']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5206, grad_fn=<DivBackward0>)

[23,    40] loss_total: 40.952 loss_supervised_contrasive:  14.391 loss_intent :3006.814 
skip_time: 0
total : 1074
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : tire_change
pair class : ['tire_change']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['cancel_reservation', 'what_song', 'account_blocked', 'bill_due', 'uber', 'spelling', 'car_rental', 'roll_dice', 'flight_status', 'repeat', 'pto_request_status', 'tire_change', 'accept_reservations', 'calories', 'distance', 'tire_change']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5262, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : what_are_your_hobbies
pair class : ['what_are_your_hobbies']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['mpg', 'calendar', 'direct_deposit', 'meaning_of_life', 'shopping_list', 'whisper_mode', 'improve_credit_score', 'pay_bill', 'what_are_your_hobbies', 'shopping_list_update', 'oil_change_when', 'time', 'how_busy', 'travel_notification', 'todo_list', 'what_are_your_hobbies']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5381, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : schedule_meeting
pair class : ['schedule_meeting']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
----
masking label debug : ['bill_due' 'bill_due']
current labels  bill_due
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 2
current skips : [4 6]
current labels : bill_due
pair class : ['bill_due' 'bill_due']
mask: [False False False False  True False  True False False False False False
 False False False False]
count: 16
numbers of pairs one label : 2
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [15, 4, 6]
['card_declined', 'schedule_meeting', 'bill_due', 'where_are_you_from', 'bill_due', 'book_flight', 'bill_due', 'travel_notification', 'improve_credit_score', 'last_maintenance', 'confirm_reservation', 'mpg', 'definition', 'pay_bill', 'roll_dice', 'schedule_meeting']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.1117, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : nutrition_info
pair class : ['nutrition_info']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['cook_time', 'restaurant_suggestion', 'lost_luggage', 'report_lost_card', 'calculator', 'travel_alert', 'text', 'measurement_conversion', 'ingredients_list', 'nutrition_info', 'reminder_update', 'thank_you', 'interest_rate', 'no', 'carry_on', 'nutrition_info']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5240, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : maybe
pair class : ['maybe']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['shopping_list', 'todo_list', 'maybe', 'tell_joke', 'distance', 'vaccines', 'share_location', 'reminder_update', 'plug_type', 'report_lost_card', 'pto_balance', 'new_card', 'redeem_rewards', 'smart_home', 'credit_limit_change', 'maybe']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5283, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : whisper_mode
pair class : ['whisper_mode']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['cancel_reservation', 'traffic', 'whisper_mode', 'order_status', 'shopping_list', 'credit_limit', 'calories', 'w2', 'carry_on', 'text', 'uber', 'where_are_you_from', 'change_language', 'distance', 'change_speed', 'whisper_mode']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5375, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [13]
current labels : restaurant_reservation
pair class : ['restaurant_reservation']
mask: [False False False False False False False False False False False False
 False  True]
count: 14
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [13]
['insurance_change', 'calendar', 'card_declined', 'report_fraud', 'are_you_a_bot', 'user_name', 'transfer', 'replacement_card_duration', 'do_you_have_pets', 'expiration_date', 'order_checks', 'reminder', 'restaurant_reservation', 'restaurant_reservation']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([14, 14])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.2490, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : bill_balance
pair class : ['bill_balance']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['change_volume', 'min_payment', 'are_you_a_bot', 'mpg', 'ingredient_substitution', 'rewards_balance', 'taxes', 'flight_status', 'bill_balance', 'alarm', 'reminder', 'book_flight', 'spelling', 'distance', 'what_is_your_name', 'bill_balance']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5219, grad_fn=<DivBackward0>)

----
masking label debug : ['time' 'time']
current labels  time
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 3
current skips : [6 8]
current labels : time
pair class : ['time' 'time']
mask: [False False False False False False  True False  True False False False
 False False False False]
count: 16
numbers of pairs one label : 2
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : restaurant_reservation
pair class : ['restaurant_reservation']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [6, 8, 15]
['who_made_you', 'weather', 'apr', 'time', 'pto_used', 'measurement_conversion', 'time', 'vaccines', 'time', 'pin_change', 'no', 'direct_deposit', 'restaurant_reservation', 'transactions', 'sync_device', 'restaurant_reservation']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.1086, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : change_accent
pair class : ['change_accent']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['user_name', 'measurement_conversion', 'current_location', 'calories', 'definition', 'exchange_rate', 'travel_suggestion', 'new_card', 'calculator', 'whisper_mode', 'todo_list_update', 'gas_type', 'change_accent', 'flight_status', 'account_blocked', 'change_accent']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5317, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : no
pair class : ['no']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [13]
current labels : yes
pair class : ['yes']
mask: [False False False False False False False False False False False False
 False  True False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [12]
current labels : shopping_list_update
pair class : ['shopping_list_update']
mask: [False False False False False False False False False False False False
  True False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [15, 13, 12]
['translate', 'no', 'vaccines', 'yes', 'shopping_list_update', 'carry_on', 'sync_device', 'todo_list', 'meeting_schedule', 'car_rental', 'date', 'balance', 'shopping_list_update', 'yes', 'alarm', 'no']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.1143, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [15]
current labels : schedule_meeting
pair class : ['schedule_meeting']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['recipe', 'directions', 'find_phone', 'schedule_meeting', 'how_old_are_you', 'vaccines', 'book_flight', 'measurement_conversion', 'change_volume', 'confirm_reservation', 'definition', 'balance', 'restaurant_reviews', 'jump_start', 'application_status', 'schedule_meeting']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5315, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [11]
current labels : change_speed
pair class : ['change_speed']
mask: [False False False False False False False False False False False  True
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : timer
pair class : ['timer']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [11, 15]
['order_checks', 'who_do_you_work_for', 'translate', 'pto_used', 'what_is_your_name', 'mpg', 'routing', 'international_visa', 'report_fraud', 'new_card', 'change_speed', 'change_speed', 'timer', 'order_status', 'make_call', 'timer']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0721, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [2]
current labels : report_lost_card
pair class : ['report_lost_card']
mask: [False False  True False False False False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : expiration_date
pair class : ['expiration_date']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [2, 15]
['who_do_you_work_for', 'report_lost_card', 'report_lost_card', 'change_volume', 'food_last', 'traffic', 'pto_request_status', 'how_busy', 'expiration_date', 'todo_list', 'smart_home', 'restaurant_suggestion', 'cancel_reservation', 'routing', 'accept_reservations', 'expiration_date']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0691, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : todo_list_update
pair class : ['todo_list_update']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['current_location', 'reminder_update', 'where_are_you_from', 'change_accent', 'todo_list_update', 'change_user_name', 'oil_change_how', 'calories', 'meal_suggestion', 'damaged_card', 'fun_fact', 'rollover_401k', 'order_status', 'interest_rate', 'do_you_have_pets', 'todo_list_update']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5447, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [9]
current labels : insurance
pair class : ['insurance']
mask: [False False False False False False False False False  True False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [15]
current labels : next_holiday
pair class : ['next_holiday']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [9, 15]
['what_can_i_ask_you', 'change_speed', 'travel_notification', 'account_blocked', 'insurance', 'next_holiday', 'weather', 'meeting_schedule', 'car_rental', 'insurance', 'income', 'accept_reservations', 'goodbye', 'fun_fact', 'international_visa', 'next_holiday']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0720, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [1]
current labels : tire_pressure
pair class : ['tire_pressure']
mask: [False  True False False False False False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : international_fees
pair class : ['international_fees']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [1, 15]
['tire_pressure', 'tire_pressure', 'pto_request_status', 'gas', 'schedule_meeting', 'insurance_change', 'smart_home', 'spelling', 'next_holiday', 'freeze_account', 'sync_device', 'calculator', 'date', 'measurement_conversion', 'international_fees', 'international_fees']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0691, grad_fn=<DivBackward0>)

[24,    10] loss_total: 40.849 loss_supervised_contrasive:  3.264 loss_intent :751.705 
skip_time: 0
total : 1091
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [9]
current labels : cancel
pair class : ['cancel']
mask: [False False False False False False False False False  True False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [5]
current labels : tell_joke
pair class : ['tell_joke']
mask: [False False False False False  True False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [15]
current labels : change_user_name
pair class : ['change_user_name']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [9, 5, 15]
['schedule_maintenance', 'transfer', 'transactions', 'cancel', 'tell_joke', 'tell_joke', 'lost_luggage', 'change_user_name', 'text', 'cancel', 'what_song', 'share_location', 'schedule_meeting', 'gas', 'calendar_update', 'change_user_name']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.1121, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [15]
current labels : calculator
pair class : ['calculator']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['rollover_401k', 'schedule_maintenance', 'share_location', 'rewards_balance', 'carry_on', 'calculator', 'bill_due', 'credit_score', 'tell_joke', 'weather', 'travel_notification', 'pto_balance', 'calendar', 'damaged_card', 'flip_coin', 'calculator']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5566, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [3]
current labels : what_is_your_name
pair class : ['what_is_your_name']
mask: [False False False  True False False False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : pto_request
pair class : ['pto_request']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [3, 15]
['what_is_your_name', 'pto_request', 'reminder', 'what_is_your_name', 'income', 'shopping_list_update', 'credit_score', 'traffic', 'restaurant_reviews', 'credit_limit_change', 'mpg', 'make_call', 'next_song', 'definition', 'timezone', 'pto_request']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0701, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [14]
current labels : payday
pair class : ['payday']
mask: [False False False False False False False False False False False False
 False False  True False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : report_lost_card
pair class : ['report_lost_card']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [14, 15]
['who_made_you', 'shopping_list', 'pto_request_status', 'improve_credit_score', 'exchange_rate', 'timezone', 'payday', 'meal_suggestion', 'greeting', 'directions', 'car_rental', 'credit_limit_change', 'report_lost_card', 'gas_type', 'payday', 'report_lost_card']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0737, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [15]
current labels : improve_credit_score
pair class : ['improve_credit_score']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['improve_credit_score', 'carry_on', 'gas', 'weather', 'reset_settings', 'plug_type', 'maybe', 'spending_history', 'who_made_you', 'uber', 'min_payment', 'fun_fact', 'insurance', 'replacement_card_duration', 'order_checks', 'improve_credit_score']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5518, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : transfer
pair class : ['transfer']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['flip_coin', 'mpg', 'lost_luggage', 'jump_start', 'transfer', 'timezone', 'smart_home', 'restaurant_suggestion', 'new_card', 'timer', 'recipe', 'weather', 'order_checks', 'restaurant_reservation', 'routing', 'transfer']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5296, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : no
pair class : ['no']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['current_location', 'freeze_account', 'pin_change', 'recipe', 'restaurant_reservation', 'apr', 'roll_dice', 'do_you_have_pets', 'restaurant_suggestion', 'no', 'jump_start', 'share_location', 'report_fraud', 'oil_change_how', 'meaning_of_life', 'no']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5203, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : update_playlist
pair class : ['update_playlist']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['improve_credit_score', 'change_volume', 'redeem_rewards', 'calories', 'are_you_a_bot', 'cook_time', 'accept_reservations', 'timezone', 'lost_luggage', 'update_playlist', 'w2', 'gas', 'current_location', 'card_declined', 'gas_type', 'update_playlist']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5363, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [8]
current labels : confirm_reservation
pair class : ['confirm_reservation']
mask: [False False False False False False False False  True False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : damaged_card
pair class : ['damaged_card']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [8, 15]
['card_declined', 'sync_device', 'vaccines', 'nutrition_info', 'confirm_reservation', 'transactions', 'tire_change', 'whisper_mode', 'confirm_reservation', 'ingredients_list', 'restaurant_suggestion', 'update_playlist', 'tell_joke', 'next_song', 'damaged_card', 'damaged_card']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0738, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [8]
current labels : how_old_are_you
pair class : ['how_old_are_you']
mask: [False False False False False False False False  True False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : travel_notification
pair class : ['travel_notification']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [8, 15]
['income', 'play_music', 'maybe', 'how_old_are_you', 'expiration_date', 'change_speed', 'calendar', 'find_phone', 'how_old_are_you', 'travel_notification', 'restaurant_reservation', 'pto_request', 'min_payment', 'schedule_meeting', 'pto_balance', 'travel_notification']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0719, grad_fn=<DivBackward0>)

[24,    20] loss_total: 41.295 loss_supervised_contrasive:  6.973 loss_intent :1503.406 
skip_time: 0
total : 1101
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 13
current skips : [15]
current labels : spelling
pair class : ['spelling']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['travel_notification', 'make_call', 'pay_bill', 'international_fees', 'who_made_you', 'food_last', 'restaurant_suggestion', 'time', 'travel_suggestion', 'meal_suggestion', 'book_flight', 'roll_dice', 'reminder', 'spelling', 'credit_limit_change', 'spelling']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5481, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [15]
current labels : change_language
pair class : ['change_language']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['change_ai_name', 'application_status', 'calculator', 'balance', 'pto_balance', 'restaurant_reservation', 'change_language', 'replacement_card_duration', 'redeem_rewards', 'gas_type', 'report_lost_card', 'order_status', 'ingredient_substitution', 'tire_change', 'card_declined', 'change_language']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5192, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [14]
current labels : change_accent
pair class : ['change_accent']
mask: [False False False False False False False False False False False False
 False False  True False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : bill_due
pair class : ['bill_due']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [14, 15]
['change_accent', 'timer', 'greeting', 'travel_alert', 'uber', 'transactions', 'meeting_schedule', 'translate', 'bill_due', 'tire_change', 'schedule_maintenance', 'apr', 'oil_change_how', 'ingredient_substitution', 'change_accent', 'bill_due']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0777, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [2]
current labels : tire_pressure
pair class : ['tire_pressure']
mask: [False False  True False False False False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [13]
current labels : todo_list_update
pair class : ['todo_list_update']
mask: [False False False False False False False False False False False False
 False  True False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [15]
current labels : timer
pair class : ['timer']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [2, 13, 15]
['tire_pressure', 'change_language', 'tire_pressure', 'play_music', 'uber', 'goodbye', 'todo_list_update', 'jump_start', 'what_are_your_hobbies', 'reminder', 'timer', 'last_maintenance', 'calendar_update', 'todo_list_update', 'maybe', 'timer']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.1162, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [13]
current labels : w2
pair class : ['w2']
mask: [False False False False False False False False False False False False
 False  True False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : insurance_change
pair class : ['insurance_change']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [13, 15]
['whisper_mode', 'cook_time', 'w2', 'oil_change_when', 'balance', 'reminder', 'shopping_list', 'update_playlist', 'bill_due', 'insurance_change', 'last_maintenance', 'account_blocked', 'text', 'w2', 'reset_settings', 'insurance_change']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0689, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [12]
current labels : pto_request
pair class : ['pto_request']
mask: [False False False False False False False False False False False False
  True False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 13
current skips : [15]
current labels : payday
pair class : ['payday']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [12, 15]
['freeze_account', 'tire_change', 'pto_request', 'travel_alert', 'change_user_name', 'cook_time', 'reset_settings', 'distance', 'calendar', 'shopping_list_update', 'last_maintenance', 'cancel_reservation', 'pto_request', 'payday', 'food_last', 'payday']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0732, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : goodbye
pair class : ['goodbye']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['min_payment', 'credit_limit', 'order_checks', 'distance', 'repeat', 'plug_type', 'what_is_your_name', 'spending_history', 'change_language', 'text', 'whisper_mode', 'goodbye', 'reminder_update', 'pto_used', 'last_maintenance', 'goodbye']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5429, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [13]
current labels : book_hotel
pair class : ['book_hotel']
mask: [False False False False False False False False False False False False
 False  True False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [15]
current labels : interest_rate
pair class : ['interest_rate']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [13, 15]
['shopping_list', 'recipe', 'report_fraud', 'do_you_have_pets', 'book_hotel', 'credit_limit', 'timezone', 'change_ai_name', 'where_are_you_from', 'directions', 'interest_rate', 'book_flight', 'rollover_401k', 'book_hotel', 'todo_list_update', 'interest_rate']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0752, grad_fn=<DivBackward0>)

----
masking label debug : ['are_you_a_bot' 'are_you_a_bot']
current labels  are_you_a_bot
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 9
current skips : [14 15]
current labels : are_you_a_bot
pair class : ['are_you_a_bot' 'are_you_a_bot']
mask: [False False False False False False False False False False False False
 False False  True  True]
count: 16
numbers of pairs one label : 2
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [14, 15]
['greeting', 'find_phone', 'calendar', 'travel_suggestion', 'reminder_update', 'change_language', 'nutrition_info', 'bill_balance', 'rewards_balance', 'are_you_a_bot', 'goodbye', 'spending_history', 'report_lost_card', 'last_maintenance', 'are_you_a_bot', 'are_you_a_bot']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0612, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [14]
current labels : what_are_your_hobbies
pair class : ['what_are_your_hobbies']
mask: [False False False False False False False False False False False False
 False False  True False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : change_volume
pair class : ['change_volume']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [14, 15]
['user_name', 'spending_history', 'schedule_maintenance', 'restaurant_reviews', 'maybe', 'repeat', 'what_are_your_hobbies', 'accept_reservations', 'change_volume', 'shopping_list_update', 'where_are_you_from', 'income', 'text', 'directions', 'what_are_your_hobbies', 'change_volume']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0694, grad_fn=<DivBackward0>)

[24,    30] loss_total: 40.600 loss_supervised_contrasive:  9.988 loss_intent :2255.102 
skip_time: 0
total : 1111
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [15]
current labels : vaccines
pair class : ['vaccines']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [12]
current labels : oil_change_how
pair class : ['oil_change_how']
mask: [False False False False False False False False False False False False
  True False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 12]
['vaccines', 'tire_pressure', 'calendar_update', 'pay_bill', 'how_busy', 'roll_dice', 'pto_used', 'next_song', 'oil_change_how', 'what_song', 'date', 'credit_limit', 'oil_change_how', 'calories', 'where_are_you_from', 'vaccines']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0662, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : nutrition_info
pair class : ['nutrition_info']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['order', 'smart_home', 'nutrition_info', 'ingredients_list', 'routing', 'no', 'bill_balance', 'pin_change', 'change_speed', 'meal_suggestion', 'rewards_balance', 'thank_you', 'credit_score', 'play_music', 'application_status', 'nutrition_info']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5383, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : find_phone
pair class : ['find_phone']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [13]
current labels : redeem_rewards
pair class : ['redeem_rewards']
mask: [False False False False False False False False False False False False
 False  True False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 13]
['interest_rate', 'pin_change', 'distance', 'change_ai_name', 'find_phone', 'order', 'replacement_card_duration', 'redeem_rewards', 'directions', 'oil_change_when', 'definition', 'current_location', 'time', 'redeem_rewards', 'pay_bill', 'find_phone']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0731, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : new_card
pair class : ['new_card']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['no', 'interest_rate', 'update_playlist', 'yes', 'taxes', 'account_blocked', 'exchange_rate', 'share_location', 'what_can_i_ask_you', 'uber', 'meaning_of_life', 'new_card', 'traffic', 'redeem_rewards', 'pay_bill', 'new_card']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5299, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [11]
current labels : change_ai_name
pair class : ['change_ai_name']
mask: [False False False False False False False False False False False  True
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : date
pair class : ['date']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [11, 15]
['ingredients_list', 'expiration_date', 'who_do_you_work_for', 'payday', 'bill_balance', 'change_ai_name', 'meaning_of_life', 'card_declined', 'taxes', 'date', 'todo_list', 'change_ai_name', 'direct_deposit', 'interest_rate', 'meeting_schedule', 'date']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0721, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [15]
current labels : confirm_reservation
pair class : ['confirm_reservation']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['confirm_reservation', 'roll_dice', 'oil_change_when', 'thank_you', 'order_status', 'how_old_are_you', 'order_checks', 'who_do_you_work_for', 'exchange_rate', 'sync_device', 'pto_used', 'travel_suggestion', 'balance', 'replacement_card_duration', 'meeting_schedule', 'confirm_reservation']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5309, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : shopping_list
pair class : ['shopping_list']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [11]
current labels : traffic
pair class : ['traffic']
mask: [False False False False False False False False False False False  True
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 11]
['travel_alert', 'income', 'report_fraud', 'transfer', 'shopping_list', 'taxes', 'what_are_your_hobbies', 'traffic', 'next_holiday', 'direct_deposit', 'plug_type', 'traffic', 'book_hotel', 'restaurant_reviews', 'schedule_meeting', 'shopping_list']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0686, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [11]
current labels : transfer
pair class : ['transfer']
mask: [False False False False False False False False False False False  True
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : how_old_are_you
pair class : ['how_old_are_you']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [11, 15]
['thank_you', 'w2', 'taxes', 'repeat', 'book_flight', 'transfer', 'insurance', 'credit_limit', 'card_declined', 'how_old_are_you', 'mpg', 'transfer', 'freeze_account', 'bill_due', 'food_last', 'how_old_are_you']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0640, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [8]
current labels : spelling
pair class : ['spelling']
mask: [False False False False False False False False  True False False False
 False False False False]
count: 16
numbers of pairs one label : 1
----
masking label debug : ['next_holiday' 'next_holiday']
current labels  next_holiday
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 6
current skips : [10 15]
current labels : next_holiday
pair class : ['next_holiday' 'next_holiday']
mask: [False False False False False False False False False False  True False
 False False False  True]
count: 16
numbers of pairs one label : 2
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [8, 10, 15]
['calculator', 'car_rental', 'change_language', 'spelling', 'damaged_card', 'expiration_date', 'next_holiday', 'change_user_name', 'spelling', 'flip_coin', 'next_holiday', 'carry_on', 'what_can_i_ask_you', 'meaning_of_life', 'international_fees', 'next_holiday']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.1114, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [6]
current labels : cancel
pair class : ['cancel']
mask: [False False False False False False  True False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : pto_balance
pair class : ['pto_balance']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [6, 15]
['calendar_update', 'recipe', 'book_hotel', 'flip_coin', 'cancel', 'next_song', 'cancel', 'ingredient_substitution', 'what_can_i_ask_you', 'pto_balance', 'translate', 'international_visa', 'food_last', 'reminder_update', 'application_status', 'pto_balance']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0774, grad_fn=<DivBackward0>)

[24,    40] loss_total: 40.598 loss_supervised_contrasive:  13.002 loss_intent :3006.802 
skip_time: 0
total : 1121
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : translate
pair class : ['translate']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['definition', 'translate', 'calendar_update', 'min_payment', 'meal_suggestion', 'routing', 'greeting', 'change_accent', 'yes', 'insurance_change', 'flip_coin', 'where_are_you_from', 'cancel_reservation', 'ingredients_list', 'lost_luggage', 'translate']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5335, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [6]
current labels : reset_settings
pair class : ['reset_settings']
mask: [False False False False False False  True False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [15]
current labels : schedule_maintenance
pair class : ['schedule_maintenance']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [6, 15]
['order', 'international_visa', 'account_blocked', 'what_song', 'reset_settings', 'damaged_card', 'reset_settings', 'schedule_maintenance', 'travel_suggestion', 'shopping_list', 'credit_limit', 'make_call', 'freeze_account', 'tell_joke', 'roll_dice', 'schedule_maintenance']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0733, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [15]
current labels : flight_status
pair class : ['flight_status']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['bill_due', 'measurement_conversion', 'w2', 'pto_request', 'what_are_your_hobbies', 'carry_on', 'cancel_reservation', 'flight_status', 'make_call', 'thank_you', 'what_song', 'rewards_balance', 'credit_score', 'are_you_a_bot', 'credit_limit_change', 'flight_status']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5346, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : fun_fact
pair class : ['fun_fact']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['improve_credit_score', 'fun_fact', 'car_rental', 'who_do_you_work_for', 'todo_list', 'who_made_you', 'how_busy', 'text', 'nutrition_info', 'thank_you', 'application_status', 'alarm', 'international_visa', 'transactions', 'insurance_change', 'fun_fact']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5356, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : distance
pair class : ['distance']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['calendar', 'user_name', 'insurance_change', 'apr', 'pay_bill', 'oil_change_when', 'rollover_401k', 'how_busy', 'alarm', 'travel_notification', 'flight_status', 'distance', 'date', 'cancel', 'international_fees', 'distance']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5482, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [12]
current labels : do_you_have_pets
pair class : ['do_you_have_pets']
mask: [False False False False False False False False False False False False
  True False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [15]
current labels : fun_fact
pair class : ['fun_fact']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [12, 15]
['cancel_reservation', 'travel_alert', 'jump_start', 'next_song', 'flight_status', 'tire_change', 'do_you_have_pets', 'fun_fact', 'plug_type', 'smart_home', 'cook_time', 'share_location', 'do_you_have_pets', 'insurance', 'lost_luggage', 'fun_fact']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0695, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [6]
current labels : pto_request_status
pair class : ['pto_request_status']
mask: [False False False False False False  True False False False False False
 False False]
count: 14
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [13]
current labels : bill_balance
pair class : ['bill_balance']
mask: [False False False False False False False False False False False False
 False  True]
count: 14
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [6, 13]
['direct_deposit', 'improve_credit_score', 'rollover_401k', 'pto_request_status', 'cook_time', 'order_status', 'pto_request_status', 'accept_reservations', 'maybe', 'bill_balance', 'calories', 'pto_balance', 'order', 'bill_balance']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([14, 14])
after summing res.shape : torch.Size([])
res.shape : torch.Size([14, 14])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.9429, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : change_accent
pair class : ['change_accent']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['uber', 'change_language', 'plug_type', 'pay_bill', 'application_status', 'sync_device', 'repeat', 'rewards_balance', 'alarm', 'book_hotel', 'what_song', 'restaurant_reviews', 'calories', 'weather', 'change_accent', 'change_accent']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5267, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [6]
current labels : spending_history
pair class : ['spending_history']
mask: [False False False False False False  True False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : new_card
pair class : ['new_card']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [6, 15]
['income', 'timer', 'recipe', 'spending_history', 'new_card', 'report_fraud', 'spending_history', 'flip_coin', 'redeem_rewards', 'exchange_rate', 'freeze_account', 'gas_type', 'calculator', 'mpg', 'oil_change_how', 'new_card']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0741, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : new_card
pair class : ['new_card']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['date', 'meal_suggestion', 'no', 'gas', 'change_language', 'bill_due', 'reminder', 'next_holiday', 'todo_list', 'new_card', 'who_made_you', 'share_location', 'directions', 'schedule_meeting', 'transfer', 'new_card']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5355, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [14]
current labels : traffic
pair class : ['traffic']
mask: [False False False False False False False False False False False False
 False False  True False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : new_card
pair class : ['new_card']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [14, 15]
['change_ai_name', 'change_language', 'apr', 'lost_luggage', 'oil_change_when', 'thank_you', 'traffic', 'plug_type', 'new_card', 'whisper_mode', 'maybe', 'tire_pressure', 'update_playlist', 'direct_deposit', 'traffic', 'new_card']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0710, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [10]
current labels : smart_home
pair class : ['smart_home']
mask: [False False False False False False False False False False  True False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [15]
current labels : restaurant_reviews
pair class : ['restaurant_reviews']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [10, 15]
['w2', 'plug_type', 'weather', 'what_are_your_hobbies', 'smart_home', 'traffic', 'restaurant_reviews', 'date', 'application_status', 'what_song', 'smart_home', 'whisper_mode', 'change_speed', 'tire_change', 'share_location', 'restaurant_reviews']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0746, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [15]
current labels : interest_rate
pair class : ['interest_rate']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['payday', 'weather', 'what_song', 'pto_balance', 'book_flight', 'insurance_change', 'travel_notification', 'current_location', 'translate', 'repeat', 'interest_rate', 'calendar', 'confirm_reservation', 'book_hotel', 'next_holiday', 'interest_rate']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5232, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [15]
current labels : tell_joke
pair class : ['tell_joke']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['international_fees', 'order_checks', 'definition', 'tell_joke', 'rewards_balance', 'how_old_are_you', 'calendar', 'rollover_401k', 'what_are_your_hobbies', 'interest_rate', 'pto_request', 'restaurant_reviews', 'mpg', 'expiration_date', 'reminder', 'tell_joke']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5323, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : oil_change_when
pair class : ['oil_change_when']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['date', 'application_status', 'schedule_meeting', 'tire_pressure', 'oil_change_when', 'reset_settings', 'pto_used', 'find_phone', 'ingredient_substitution', 'measurement_conversion', 'accept_reservations', 'bill_balance', 'payday', 'who_made_you', 'change_volume', 'oil_change_when']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5335, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : direct_deposit
pair class : ['direct_deposit']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [12]
current labels : vaccines
pair class : ['vaccines']
mask: [False False False False False False False False False False False False
  True False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 12]
['expiration_date', 'direct_deposit', 'recipe', 'pto_request', 'w2', 'bill_balance', 'fun_fact', 'report_fraud', 'translate', 'pto_request_status', 'vaccines', 'international_fees', 'vaccines', 'account_blocked', 'interest_rate', 'direct_deposit']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0746, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : card_declined
pair class : ['card_declined']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['pto_request_status', 'cook_time', 'application_status', 'what_are_your_hobbies', 'tire_change', 'order_checks', 'improve_credit_score', 'text', 'translate', 'vaccines', 'min_payment', 'uber', 'card_declined', 'international_visa', 'next_holiday', 'card_declined']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5365, grad_fn=<DivBackward0>)

[25,    10] loss_total: 41.733 loss_supervised_contrasive:  4.148 loss_intent :751.700 
skip_time: 0
total : 1138
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : are_you_a_bot
pair class : ['are_you_a_bot']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['calendar_update', 'travel_suggestion', 'ingredient_substitution', 'time', 'nutrition_info', 'cancel', 'timezone', 'shopping_list', 'todo_list_update', 'find_phone', 'insurance', 'are_you_a_bot', 'calculator', 'bill_balance', 'freeze_account', 'are_you_a_bot']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4924, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : travel_suggestion
pair class : ['travel_suggestion']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['travel_alert', 'bill_balance', 'smart_home', 'report_lost_card', 'taxes', 'transactions', 'carry_on', 'oil_change_when', 'redeem_rewards', 'uber', 'restaurant_reviews', 'travel_suggestion', 'order', 'do_you_have_pets', 'pto_balance', 'travel_suggestion']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5315, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [8]
current labels : damaged_card
pair class : ['damaged_card']
mask: [False False False False False False False False  True False False False
 False False False False]
count: 16
numbers of pairs one label : 1
----
masking label debug : ['credit_limit' 'credit_limit']
current labels  credit_limit
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 3
current skips : [ 9 15]
current labels : credit_limit
pair class : ['credit_limit' 'credit_limit']
mask: [False False False False False False False False False  True False False
 False False False  True]
count: 16
numbers of pairs one label : 2
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [8, 9, 15]
['damaged_card', 'who_do_you_work_for', 'restaurant_suggestion', 'credit_limit', 'lost_luggage', 'calendar_update', 'tire_pressure', 'greeting', 'damaged_card', 'credit_limit', 'confirm_reservation', 'food_last', 'reset_settings', 'alarm', 'transactions', 'credit_limit']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.1133, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : expiration_date
pair class : ['expiration_date']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['new_card', 'alarm', 'expiration_date', 'timer', 'min_payment', 'plug_type', 'pin_change', 'change_user_name', 'routing', 'shopping_list_update', 'spelling', 'who_do_you_work_for', 'insurance', 'maybe', 'pay_bill', 'expiration_date']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5336, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [15]
current labels : shopping_list
pair class : ['shopping_list']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['travel_notification', 'cook_time', 'goodbye', 'rollover_401k', 'book_flight', 'card_declined', 'restaurant_reservation', 'shopping_list', 'yes', 'timer', 'credit_limit', 'vaccines', 'redeem_rewards', 'what_are_your_hobbies', 'fun_fact', 'shopping_list']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5378, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : cook_time
pair class : ['cook_time']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['gas', 'shopping_list_update', 'cook_time', 'sync_device', 'w2', 'change_ai_name', 'schedule_maintenance', 'change_language', 'damaged_card', 'next_holiday', 'insurance', 'definition', 'replacement_card_duration', 'reminder_update', 'translate', 'cook_time']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5295, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [15]
current labels : report_fraud
pair class : ['report_fraud']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['meal_suggestion', 'confirm_reservation', 'calendar', 'car_rental', 'book_flight', 'mpg', 'flip_coin', 'travel_suggestion', 'international_visa', 'accept_reservations', 'report_fraud', 'shopping_list_update', 'how_old_are_you', 'pay_bill', 'transfer', 'report_fraud']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5227, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [4]
current labels : where_are_you_from
pair class : ['where_are_you_from']
mask: [False False False False  True False False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [15]
current labels : definition
pair class : ['definition']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [10]
current labels : repeat
pair class : ['repeat']
mask: [False False False False False False False False False False  True False
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [4, 15, 10]
['account_blocked', 'food_last', 'who_do_you_work_for', 'where_are_you_from', 'where_are_you_from', 'how_busy', 'definition', 'repeat', 'cook_time', 'payday', 'repeat', 'who_made_you', 'user_name', 'thank_you', 'calendar', 'definition']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.1132, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [15]
current labels : book_flight
pair class : ['book_flight']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['pto_used', 'bill_due', 'restaurant_suggestion', 'book_flight', 'flight_status', 'next_song', 'min_payment', 'timer', 'order_checks', 'order_status', 'report_fraud', 'taxes', 'pay_bill', 'make_call', 'last_maintenance', 'book_flight']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5540, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [8]
current labels : meeting_schedule
pair class : ['meeting_schedule']
mask: [False False False False False False False False  True False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [4]
current labels : meaning_of_life
pair class : ['meaning_of_life']
mask: [False False False False  True False False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [15]
current labels : measurement_conversion
pair class : ['measurement_conversion']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [7]
current labels : no
pair class : ['no']
mask: [False False False False False False False  True False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 4
pairs see from labels :  4
All skippings : [8, 4, 15, 7]
['meeting_schedule', 'pto_request_status', 'meaning_of_life', 'redeem_rewards', 'meaning_of_life', 'measurement_conversion', 'no', 'no', 'meeting_schedule', 'order_status', 'reminder_update', 'w2', 'text', 'tell_joke', 'transactions', 'measurement_conversion']
---------------------------------------------
concatenate got h_i : torch.Size([4, 768])
concatenate got h_j :  torch.Size([4, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([4, 1])
pos_sim.shape : torch.Size([4])
len(neg) : 4
loss_s_cl: tensor(0.6901, grad_fn=<DivBackward0>)

[25,    20] loss_total: 41.747 loss_supervised_contrasive:  8.310 loss_intent :1503.413 
skip_time: 0
total : 1148
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : credit_limit_change
pair class : ['credit_limit_change']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['spending_history', 'directions', 'no', 'what_can_i_ask_you', 'cancel_reservation', 'shopping_list', 'play_music', 'travel_alert', 'gas_type', 'next_song', 'pto_balance', 'restaurant_reviews', 'do_you_have_pets', 'shopping_list_update', 'credit_limit_change', 'credit_limit_change']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5242, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : calories
pair class : ['calories']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['timezone', 'traffic', 'direct_deposit', 'fun_fact', 'travel_notification', 'taxes', 'tell_joke', 'find_phone', 'change_user_name', 'schedule_maintenance', 'spelling', 'calories', 'calendar_update', 'replacement_card_duration', 'application_status', 'calories']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5418, grad_fn=<DivBackward0>)

----
masking label debug : ['income' 'income']
current labels  income
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 2
current skips : [ 9 12]
current labels : income
pair class : ['income' 'income']
mask: [False False False False False False False False False  True False False
  True False False False]
count: 16
numbers of pairs one label : 2
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [6]
current labels : time
pair class : ['time']
mask: [False False False False False False  True False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : improve_credit_score
pair class : ['improve_credit_score']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 4
pairs see from labels :  4
All skippings : [9, 12, 6, 15]
['ingredient_substitution', 'what_is_your_name', 'income', 'update_playlist', 'exchange_rate', 'time', 'time', 'food_last', 'book_hotel', 'income', 'reset_settings', 'car_rental', 'income', 'international_visa', 'improve_credit_score', 'improve_credit_score']
---------------------------------------------
concatenate got h_i : torch.Size([4, 768])
concatenate got h_j :  torch.Size([4, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([4, 1])
pos_sim.shape : torch.Size([4])
len(neg) : 4
loss_s_cl: tensor(0.6897, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : pto_request
pair class : ['pto_request']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [9]
current labels : spelling
pair class : ['spelling']
mask: [False False False False False False False False False  True False False
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 9]
['no', 'pto_request', 'current_location', 'are_you_a_bot', 'credit_limit_change', 'insurance', 'travel_notification', 'carry_on', 'spelling', 'spelling', 'spending_history', 'nutrition_info', 'credit_score', 'transfer', 'calendar_update', 'pto_request']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0758, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [3]
current labels : definition
pair class : ['definition']
mask: [False False False  True False False False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : improve_credit_score
pair class : ['improve_credit_score']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [3, 15]
['definition', 'improve_credit_score', 'freeze_account', 'definition', 'reminder', 'jump_start', 'flight_status', 'tell_joke', 'cancel_reservation', 'goodbye', 'schedule_maintenance', 'order_status', 'pto_used', 'bill_due', 'pto_request', 'improve_credit_score']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0784, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [15]
current labels : todo_list_update
pair class : ['todo_list_update']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['ingredient_substitution', 'change_accent', 'what_song', 'credit_limit', 'card_declined', 'todo_list_update', 'change_speed', 'whisper_mode', 'meeting_schedule', 'min_payment', 'exchange_rate', 'calories', 'where_are_you_from', 'gas_type', 'distance', 'todo_list_update']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5250, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 13
current skips : [15]
current labels : user_name
pair class : ['user_name']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['next_song', 'change_volume', 'where_are_you_from', 'recipe', 'nutrition_info', 'what_song', 'time', 'what_can_i_ask_you', 'credit_limit_change', 'international_visa', 'change_accent', 'book_hotel', 'exchange_rate', 'user_name', 'gas', 'user_name']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5318, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [15]
current labels : account_blocked
pair class : ['account_blocked']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['directions', 'pto_used', 'apr', 'thank_you', 'translate', 'carry_on', 'yes', 'restaurant_reservation', 'interest_rate', 'current_location', 'account_blocked', 'lost_luggage', 'order', 'change_speed', 'who_do_you_work_for', 'account_blocked']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5292, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : report_lost_card
pair class : ['report_lost_card']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [6]
current labels : are_you_a_bot
pair class : ['are_you_a_bot']
mask: [False False False False False False  True False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [10]
current labels : current_location
pair class : ['current_location']
mask: [False False False False False False False False False False  True False
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [15, 6, 10]
['user_name', 'calculator', 'routing', 'transactions', 'report_lost_card', 'are_you_a_bot', 'are_you_a_bot', 'shopping_list', 'current_location', 'replacement_card_duration', 'current_location', 'uber', 'gas', 'change_language', 'find_phone', 'report_lost_card']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.1088, grad_fn=<DivBackward0>)

----
masking label debug : ['oil_change_how' 'oil_change_how']
current labels  oil_change_how
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 4
current skips : [ 5 15]
current labels : oil_change_how
pair class : ['oil_change_how' 'oil_change_how']
mask: [False False False False False  True False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 2
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [5, 15]
['car_rental', 'how_old_are_you', 'user_name', 'last_maintenance', 'oil_change_how', 'oil_change_how', 'taxes', 'jump_start', 'shopping_list_update', 'meeting_schedule', 'change_volume', 'whisper_mode', 'gas_type', 'book_flight', 'transactions', 'oil_change_how']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0699, grad_fn=<DivBackward0>)

[25,    30] loss_total: 41.153 loss_supervised_contrasive:  11.877 loss_intent :2255.132 
skip_time: 0
total : 1158
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [10]
current labels : timezone
pair class : ['timezone']
mask: [False False False False False False False False False False  True False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : expiration_date
pair class : ['expiration_date']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [10, 15]
['report_lost_card', 'accept_reservations', 'share_location', 'distance', 'how_busy', 'international_visa', 'timezone', 'tire_change', 'travel_suggestion', 'freeze_account', 'timezone', 'credit_score', 'expiration_date', 'goodbye', 'greeting', 'expiration_date']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0755, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [15]
current labels : rewards_balance
pair class : ['rewards_balance']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['rewards_balance', 'how_busy', 'do_you_have_pets', 'travel_suggestion', 'nutrition_info', 'restaurant_reservation', 'direct_deposit', 'sync_device', 'apr', 'credit_score', 'calories', 'improve_credit_score', 'schedule_meeting', 'plug_type', 'reset_settings', 'rewards_balance']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5328, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [4]
current labels : rollover_401k
pair class : ['rollover_401k']
mask: [False False False False  True False False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [15]
current labels : international_fees
pair class : ['international_fees']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [12]
current labels : insurance_change
pair class : ['insurance_change']
mask: [False False False False False False False False False False False False
  True False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [4, 15, 12]
['play_music', 'pto_request', 'rollover_401k', 'credit_limit_change', 'rollover_401k', 'international_fees', 'schedule_meeting', 'oil_change_when', 'credit_score', 'insurance_change', 'order_checks', 'change_user_name', 'insurance_change', 'order', 'last_maintenance', 'international_fees']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.1110, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [14]
current labels : flight_status
pair class : ['flight_status']
mask: [False False False False False False False False False False False False
 False False  True False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [15]
current labels : schedule_meeting
pair class : ['schedule_meeting']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [14, 15]
['flight_status', 'roll_dice', 'lost_luggage', 'min_payment', 'credit_limit', 'play_music', 'insurance_change', 'reminder', 'meaning_of_life', 'next_song', 'schedule_meeting', 'accept_reservations', 'cancel', 'whisper_mode', 'flight_status', 'schedule_meeting']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0757, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [15]
current labels : fun_fact
pair class : ['fun_fact']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['fun_fact', 'how_old_are_you', 'roll_dice', 'text', 'confirm_reservation', 'schedule_maintenance', 'how_busy', 'gas_type', 'goodbye', 'who_do_you_work_for', 'bill_balance', 'directions', 'tire_pressure', 'meaning_of_life', 'maybe', 'fun_fact']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5383, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : replacement_card_duration
pair class : ['replacement_card_duration']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['jump_start', 'meal_suggestion', 'thank_you', 'international_fees', 'play_music', 'next_holiday', 'tire_change', 'todo_list', 'flight_status', 'replacement_card_duration', 'repeat', 'transfer', 'smart_home', 'make_call', 'who_made_you', 'replacement_card_duration']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5343, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : jump_start
pair class : ['jump_start']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['flip_coin', 'todo_list', 'yes', 'exchange_rate', 'ingredients_list', 'traffic', 'report_lost_card', 'change_volume', 'what_is_your_name', 'meal_suggestion', 'new_card', 'jump_start', 'cancel', 'damaged_card', 'apr', 'jump_start']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5516, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [9]
current labels : calculator
pair class : ['calculator']
mask: [False False False False False False False False False  True False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : update_playlist
pair class : ['update_playlist']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [9, 15]
['make_call', 'calculator', 'what_can_i_ask_you', 'where_are_you_from', 'maybe', 'travel_alert', 'last_maintenance', 'food_last', 'update_playlist', 'calculator', 'routing', 'cancel', 'measurement_conversion', 'mpg', 'uber', 'update_playlist']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0729, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [15]
current labels : jump_start
pair class : ['jump_start']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['reminder', 'tire_pressure', 'pin_change', 'jump_start', 'reminder_update', 'recipe', 'rollover_401k', 'date', 'make_call', 'account_blocked', 'roll_dice', 'income', 'order_checks', 'cancel', 'food_last', 'jump_start']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5455, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [11]
current labels : carry_on
pair class : ['carry_on']
mask: [False False False False False False False False False False False  True
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : yes
pair class : ['yes']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [11, 15]
['carry_on', 'apr', 'yes', 'travel_notification', 'todo_list', 'cancel_reservation', 'oil_change_when', 'what_is_your_name', 'next_song', 'credit_limit_change', 'weather', 'carry_on', 'time', 'payday', 'balance', 'yes']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0739, grad_fn=<DivBackward0>)

[25,    40] loss_total: 41.296 loss_supervised_contrasive:  15.589 loss_intent :3006.826 
skip_time: 0
total : 1168
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [10]
current labels : text
pair class : ['text']
mask: [False False False False False False False False False False  True False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : calendar_update
pair class : ['calendar_update']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [10, 15]
['greeting', 'tell_joke', 'change_ai_name', 'roll_dice', 'spending_history', 'car_rental', 'order', 'tire_change', 'what_can_i_ask_you', 'text', 'text', 'calories', 'mpg', 'sync_device', 'calendar_update', 'calendar_update']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0781, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [15]
current labels : taxes
pair class : ['taxes']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['alarm', 'make_call', 'flip_coin', 'what_is_your_name', 'update_playlist', 'are_you_a_bot', 'taxes', 'restaurant_suggestion', 'date', 'order', 'balance', 'credit_score', 'bill_due', 'shopping_list', 'cancel_reservation', 'taxes']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5271, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [12]
current labels : change_ai_name
pair class : ['change_ai_name']
mask: [False False False False False False False False False False False False
  True False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [4]
current labels : reminder_update
pair class : ['reminder_update']
mask: [False False False False  True False False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : ingredient_substitution
pair class : ['ingredient_substitution']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [12, 4, 15]
['change_ai_name', 'change_speed', 'reminder_update', 'reset_settings', 'reminder_update', 'damaged_card', 'redeem_rewards', 'greeting', 'car_rental', 'alarm', 'yes', 'cook_time', 'change_ai_name', 'pin_change', 'ingredient_substitution', 'ingredient_substitution']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.1137, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [11]
current labels : pto_balance
pair class : ['pto_balance']
mask: [False False False False False False False False False False False  True
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : balance
pair class : ['balance']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [11, 15]
['pin_change', 'interest_rate', 'todo_list_update', 'sync_device', 'pto_balance', 'change_accent', 'gas', 'book_hotel', 'fun_fact', 'balance', 'update_playlist', 'pto_balance', 'improve_credit_score', 'meeting_schedule', 'restaurant_suggestion', 'balance']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0733, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [15]
current labels : restaurant_reservation
pair class : ['restaurant_reservation']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [13]
current labels : change_user_name
pair class : ['change_user_name']
mask: [False False False False False False False False False False False False
 False  True False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 13]
['measurement_conversion', 'transfer', 'how_old_are_you', 'account_blocked', 'todo_list_update', 'do_you_have_pets', 'share_location', 'todo_list', 'distance', 'thank_you', 'restaurant_reservation', 'change_user_name', 'meaning_of_life', 'change_user_name', 'w2', 'restaurant_reservation']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0639, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [15]
current labels : nutrition_info
pair class : ['nutrition_info']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['find_phone', 'goodbye', 'last_maintenance', 'direct_deposit', 'what_is_your_name', 'nutrition_info', 'rewards_balance', 'todo_list_update', 'pto_request_status', 'timezone', 'payday', 'replacement_card_duration', 'freeze_account', 'ingredients_list', 'meal_suggestion', 'nutrition_info']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5410, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [13]
current labels : pto_used
pair class : ['pto_used']
mask: [False False False False False False False False False False False False
 False  True]
count: 14
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [13]
['card_declined', 'pto_used', 'do_you_have_pets', 'travel_alert', 'restaurant_reservation', 'balance', 'routing', 'greeting', 'lost_luggage', 'user_name', 'vaccines', 'pin_change', 'pto_request_status', 'pto_used']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([14, 14])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.2675, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : update_playlist
pair class : ['update_playlist']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [14]
current labels : schedule_maintenance
pair class : ['schedule_maintenance']
mask: [False False False False False False False False False False False False
 False False  True False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 14]
['do_you_have_pets', 'who_made_you', 'exchange_rate', 'shopping_list', 'card_declined', 'what_is_your_name', 'confirm_reservation', 'routing', 'tell_joke', 'update_playlist', 'schedule_maintenance', 'change_accent', 'user_name', 'date', 'schedule_maintenance', 'update_playlist']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0746, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [15]
current labels : schedule_maintenance
pair class : ['schedule_maintenance']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['bill_due', 'min_payment', 'change_user_name', 'carry_on', 'gas', 'pto_balance', 'todo_list_update', 'cook_time', 'account_blocked', 'application_status', 'schedule_maintenance', 'how_busy', 'report_fraud', 'timezone', 'oil_change_when', 'schedule_maintenance']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5239, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [15]
current labels : min_payment
pair class : ['min_payment']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['damaged_card', 'timer', 'redeem_rewards', 'are_you_a_bot', 'directions', 'min_payment', 'date', 'application_status', 'international_visa', 'tire_change', 'report_fraud', 'make_call', 'meal_suggestion', 'pto_used', 'ingredient_substitution', 'min_payment']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5417, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [15]
current labels : redeem_rewards
pair class : ['redeem_rewards']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['restaurant_reservation', 'fun_fact', 'restaurant_reviews', 'income', 'routing', 'insurance_change', 'gas_type', 'redeem_rewards', 'credit_limit_change', 'text', 'weather', 'reminder_update', 'traffic', 'rewards_balance', 'pto_used', 'redeem_rewards']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5449, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [6]
current labels : who_do_you_work_for
pair class : ['who_do_you_work_for']
mask: [False False False False False False  True False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [10]
current labels : pin_change
pair class : ['pin_change']
mask: [False False False False False False False False False False  True False
 False False False False]
count: 16
numbers of pairs one label : 1
----
masking label debug : ['book_hotel' 'book_hotel']
current labels  book_hotel
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 8
current skips : [12 15]
current labels : book_hotel
pair class : ['book_hotel' 'book_hotel']
mask: [False False False False False False False False False False False False
  True False False  True]
count: 16
numbers of pairs one label : 2
the number of pairs for entire batch: 4
pairs see from labels :  4
All skippings : [6, 10, 12, 15]
['who_do_you_work_for', 'pin_change', 'tire_change', 'text', 'interest_rate', 'smart_home', 'who_do_you_work_for', 'car_rental', 'book_hotel', 'international_visa', 'pin_change', 'credit_limit', 'book_hotel', 'redeem_rewards', 'who_made_you', 'book_hotel']
---------------------------------------------
concatenate got h_i : torch.Size([4, 768])
concatenate got h_j :  torch.Size([4, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([4, 1])
pos_sim.shape : torch.Size([4])
len(neg) : 4
loss_s_cl: tensor(0.6901, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [15]
current labels : car_rental
pair class : ['car_rental']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['spending_history', 'change_user_name', 'meeting_schedule', 'recipe', 'taxes', 'car_rental', 'change_speed', 'update_playlist', 'uber', 'change_volume', 'oil_change_when', 'report_fraud', 'direct_deposit', 'reminder', 'balance', 'car_rental']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5353, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [11]
current labels : change_language
pair class : ['change_language']
mask: [False False False False False False False False False False False  True
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : gas
pair class : ['gas']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [11, 15]
['order_status', 'new_card', 'change_volume', 'order_checks', 'translate', 'tell_joke', 'change_language', 'sync_device', 'calendar', 'ingredient_substitution', 'restaurant_reviews', 'change_language', 'gas', 'play_music', 'where_are_you_from', 'gas']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0671, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [15]
current labels : yes
pair class : ['yes']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['timezone', 'cook_time', 'todo_list', 'yes', 'flight_status', 'jump_start', 'pay_bill', 'carry_on', 'rewards_balance', 'shopping_list', 'transfer', 'recipe', 'pto_request', 'income', 'taxes', 'yes']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5283, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [15]
current labels : travel_alert
pair class : ['travel_alert']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
----
masking label debug : ['change_ai_name' 'change_ai_name']
current labels  change_ai_name
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 6
current skips : [ 7 10]
current labels : change_ai_name
pair class : ['change_ai_name' 'change_ai_name']
mask: [False False False False False False False  True False False  True False
 False False False False]
count: 16
numbers of pairs one label : 2
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [15, 7, 10]
['freeze_account', 'timezone', 'oil_change_when', 'where_are_you_from', 'roll_dice', 'travel_alert', 'change_ai_name', 'change_ai_name', 'credit_limit_change', 'apr', 'change_ai_name', 'do_you_have_pets', 'are_you_a_bot', 'smart_home', 'measurement_conversion', 'travel_alert']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.1167, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : card_declined
pair class : ['card_declined']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['time', 'car_rental', 'calendar', 'change_user_name', 'what_are_your_hobbies', 'rewards_balance', 'cancel', 'who_do_you_work_for', 'change_speed', 'lost_luggage', 'gas', 'international_fees', 'card_declined', 'book_flight', 'how_busy', 'card_declined']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5394, grad_fn=<DivBackward0>)

[26,    10] loss_total: 41.501 loss_supervised_contrasive:  3.916 loss_intent :751.690 
skip_time: 0
total : 1185
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : travel_suggestion
pair class : ['travel_suggestion']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['taxes', 'current_location', 'change_ai_name', 'date', 'calculator', 'direct_deposit', 'mpg', 'update_playlist', 'report_fraud', 'share_location', 'restaurant_reviews', 'travel_suggestion', 'schedule_meeting', 'bill_balance', 'improve_credit_score', 'travel_suggestion']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5407, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [15]
current labels : order
pair class : ['order']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['change_accent', 'expiration_date', 'replacement_card_duration', 'flight_status', 'change_ai_name', 'order', 'maybe', 'credit_limit_change', 'improve_credit_score', 'sync_device', 'nutrition_info', 'fun_fact', 'next_song', 'uber', 'min_payment', 'order']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5243, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [7]
current labels : report_lost_card
pair class : ['report_lost_card']
mask: [False False False False False False False  True False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : what_can_i_ask_you
pair class : ['what_can_i_ask_you']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [7, 15]
['goodbye', 'current_location', 'order_status', 'cancel', 'rewards_balance', 'order', 'report_lost_card', 'report_lost_card', 'what_can_i_ask_you', 'calories', 'next_song', 'travel_notification', 'what_song', 'todo_list', 'balance', 'what_can_i_ask_you']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0711, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : confirm_reservation
pair class : ['confirm_reservation']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['routing', 'credit_limit', 'meeting_schedule', 'find_phone', 'greeting', 'schedule_meeting', 'calendar', 'tire_pressure', 'insurance_change', 'confirm_reservation', 'plug_type', 'insurance', 'travel_alert', 'next_song', 'last_maintenance', 'confirm_reservation']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5358, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [15]
current labels : bill_due
pair class : ['bill_due']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['direct_deposit', 'cancel', 'schedule_maintenance', 'insurance', 'expiration_date', 'weather', 'pto_request_status', 'damaged_card', 'time', 'todo_list', 'bill_due', 'thank_you', 'routing', 'new_card', 'jump_start', 'bill_due']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5750, grad_fn=<DivBackward0>)

----
masking label debug : ['shopping_list_update' 'shopping_list_update']
current labels  shopping_list_update
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 0
current skips : [ 5 15]
current labels : shopping_list_update
pair class : ['shopping_list_update' 'shopping_list_update']
mask: [False False False False False  True False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 2
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [5, 15]
['shopping_list_update', 'who_made_you', 'user_name', 'income', 'time', 'shopping_list_update', 'gas', 'what_are_your_hobbies', 'rewards_balance', 'accept_reservations', 'reset_settings', 'insurance_change', 'repeat', 'international_fees', 'meal_suggestion', 'shopping_list_update']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0697, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [7]
current labels : pay_bill
pair class : ['pay_bill']
mask: [False False False False False False False  True False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : exchange_rate
pair class : ['exchange_rate']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [7, 15]
['pay_bill', 'exchange_rate', 'todo_list_update', 'travel_suggestion', 'mpg', 'book_hotel', 'no', 'pay_bill', 'reminder_update', 'freeze_account', 'vaccines', 'thank_you', 'next_holiday', 'lost_luggage', 'payday', 'exchange_rate']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0652, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : yes
pair class : ['yes']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['order_status', 'application_status', 'payday', 'car_rental', 'restaurant_suggestion', 'gas', 'international_fees', 'pto_used', 'make_call', 'interest_rate', 'income', 'yes', 'how_busy', 'pto_request', 'pto_request_status', 'yes']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5522, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : no
pair class : ['no']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['report_lost_card', 'min_payment', 'what_can_i_ask_you', 'calendar_update', 'no', 'are_you_a_bot', 'pto_request_status', 'find_phone', 'carry_on', 'expiration_date', 'timer', 'directions', 'sync_device', 'order', 'repeat', 'no']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5182, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : transfer
pair class : ['transfer']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['pto_balance', 'payday', 'balance', 'ingredient_substitution', 'transfer', 'book_flight', 'card_declined', 'tire_change', 'change_language', 'timer', 'replacement_card_duration', 'share_location', 'what_song', 'credit_limit_change', 'nutrition_info', 'transfer']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5387, grad_fn=<DivBackward0>)

[26,    20] loss_total: 42.083 loss_supervised_contrasive:  8.415 loss_intent :1503.375 
skip_time: 0
total : 1195
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : flip_coin
pair class : ['flip_coin']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['pto_request_status', 'international_fees', 'maybe', 'pto_request', 'calories', 'tell_joke', 'mpg', 'bill_balance', 'find_phone', 'time', 'do_you_have_pets', 'flip_coin', 'where_are_you_from', 'recipe', 'spending_history', 'flip_coin']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5363, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [6]
current labels : reminder_update
pair class : ['reminder_update']
mask: [False False False False False False  True False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : next_holiday
pair class : ['next_holiday']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [6, 15]
['what_is_your_name', 'last_maintenance', 'spelling', 'translate', 'reminder_update', 'traffic', 'reminder_update', 'travel_suggestion', 'next_holiday', 'accept_reservations', 'order_status', 'fun_fact', 'definition', 'flight_status', 'insurance_change', 'next_holiday']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0979, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : bill_balance
pair class : ['bill_balance']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['roll_dice', 'book_flight', 'international_visa', 'interest_rate', 'bill_balance', 'alarm', 'travel_suggestion', 'account_blocked', 'insurance', 'transactions', 'calculator', 'order', 'application_status', 'meaning_of_life', 'tire_change', 'bill_balance']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5463, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [13]
current labels : text
pair class : ['text']
mask: [False False False False False False False False False False False False
 False  True False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : gas_type
pair class : ['gas_type']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [13, 15]
['definition', 'expiration_date', 'measurement_conversion', 'change_accent', 'food_last', 'shopping_list_update', 'whisper_mode', 'goodbye', 'text', 'gas_type', 'flip_coin', 'credit_limit', 'pto_request', 'text', 'improve_credit_score', 'gas_type']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0727, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : oil_change_how
pair class : ['oil_change_how']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['whisper_mode', 'mpg', 'share_location', 'play_music', 'repeat', 'change_speed', 'travel_notification', 'calendar_update', 'greeting', 'insurance', 'accept_reservations', 'oil_change_how', 'apr', 'next_holiday', 'do_you_have_pets', 'oil_change_how']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5370, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [10]
current labels : yes
pair class : ['yes']
mask: [False False False False False False False False False False  True False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [12]
current labels : rollover_401k
pair class : ['rollover_401k']
mask: [False False False False False False False False False False False False
  True False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 13
current skips : [15]
current labels : todo_list_update
pair class : ['todo_list_update']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [10, 12, 15]
['todo_list', 'roll_dice', 'what_are_your_hobbies', 'uber', 'calculator', 'tire_change', 'yes', 'thank_you', 'rollover_401k', 'reminder_update', 'yes', 'apr', 'rollover_401k', 'todo_list_update', 'timer', 'todo_list_update']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.1105, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [9]
current labels : cook_time
pair class : ['cook_time']
mask: [False False False False False False False False False  True False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [8]
current labels : sync_device
pair class : ['sync_device']
mask: [False False False False False False False False  True False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : min_payment
pair class : ['min_payment']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [9, 8, 15]
['time', 'what_are_your_hobbies', 'cook_time', 'sync_device', 'change_user_name', 'what_is_your_name', 'w2', 'user_name', 'sync_device', 'cook_time', 'shopping_list_update', 'measurement_conversion', 'min_payment', 'oil_change_when', 'weather', 'min_payment']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.1069, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : freeze_account
pair class : ['freeze_account']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['share_location', 'tire_pressure', 'translate', 'calendar_update', 'greeting', 'damaged_card', 'text', 'timer', 'spending_history', 'change_speed', 'travel_notification', 'freeze_account', 'calories', 'what_can_i_ask_you', 'change_volume', 'freeze_account']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5384, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [9]
current labels : are_you_a_bot
pair class : ['are_you_a_bot']
mask: [False False False False False False False False False  True False False
 False False False False]
count: 16
numbers of pairs one label : 1
----
masking label debug : ['pin_change' 'pin_change']
current labels  pin_change
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 5
current skips : [12 15]
current labels : pin_change
pair class : ['pin_change' 'pin_change']
mask: [False False False False False False False False False False False False
  True False False  True]
count: 16
numbers of pairs one label : 2
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [9, 12, 15]
['ingredients_list', 'are_you_a_bot', 'credit_score', 'what_are_your_hobbies', 'share_location', 'pin_change', 'uber', 'smart_home', 'ingredient_substitution', 'are_you_a_bot', 'new_card', 'reminder', 'pin_change', 'calculator', 'book_flight', 'pin_change']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.1149, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [8]
current labels : ingredients_list
pair class : ['ingredients_list']
mask: [False False False False False False False False  True False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : mpg
pair class : ['mpg']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [8, 15]
['nutrition_info', 'meeting_schedule', 'spelling', 'ingredients_list', 'find_phone', 'order_checks', 'pto_request', 'what_can_i_ask_you', 'ingredients_list', 'mpg', 'recipe', 'todo_list_update', 'directions', 'redeem_rewards', 'credit_limit_change', 'mpg']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0649, grad_fn=<DivBackward0>)

[26,    30] loss_total: 40.758 loss_supervised_contrasive:  11.588 loss_intent :2255.076 
skip_time: 0
total : 1205
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [15]
current labels : alarm
pair class : ['alarm']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [9]
current labels : goodbye
pair class : ['goodbye']
mask: [False False False False False False False False False  True False False
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 9]
['alarm', 'change_language', 'calories', 'whisper_mode', 'w2', 'pay_bill', 'goodbye', 'food_last', 'taxes', 'goodbye', 'credit_score', 'current_location', 'interest_rate', 'maybe', 'cancel_reservation', 'alarm']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0730, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [15]
current labels : damaged_card
pair class : ['damaged_card']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['direct_deposit', 'last_maintenance', 'balance', 'damaged_card', 'report_fraud', 'date', 'pto_balance', 'card_declined', 'carry_on', 'rollover_401k', 'current_location', 'insurance', 'travel_alert', 'alarm', 'how_old_are_you', 'damaged_card']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5573, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : change_user_name
pair class : ['change_user_name']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['account_blocked', 'traffic', 'income', 'distance', 'exchange_rate', 'make_call', 'reset_settings', 'directions', 'report_lost_card', 'restaurant_reservation', 'definition', 'change_user_name', 'no', 'international_visa', 'shopping_list_update', 'change_user_name']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5207, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [15]
current labels : redeem_rewards
pair class : ['redeem_rewards']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [12]
current labels : alarm
pair class : ['alarm']
mask: [False False False False False False False False False False False False
  True False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 12]
['redeem_rewards', 'vaccines', 'play_music', 'tire_pressure', 'who_do_you_work_for', 'new_card', 'how_busy', 'recipe', 'alarm', 'update_playlist', 'flight_status', 'travel_alert', 'alarm', 'reset_settings', 'distance', 'redeem_rewards']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0704, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : transactions
pair class : ['transactions']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['reminder', 'credit_limit', 'next_song', 'thank_you', 'transactions', 'cancel', 'what_song', 'flip_coin', 'calendar', 'direct_deposit', 'spending_history', 'next_holiday', 'meeting_schedule', 'cancel_reservation', 'order_checks', 'transactions']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5412, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [5]
current labels : confirm_reservation
pair class : ['confirm_reservation']
mask: [False False False False False  True False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : restaurant_reviews
pair class : ['restaurant_reviews']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [5, 15]
['restaurant_suggestion', 'vaccines', 'oil_change_when', 'confirm_reservation', 'restaurant_reviews', 'confirm_reservation', 'card_declined', 'repeat', 'distance', 'todo_list', 'flight_status', 'calendar_update', 'order_checks', 'international_fees', 'apr', 'restaurant_reviews']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0787, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : credit_limit
pair class : ['credit_limit']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['w2', 'credit_score', 'spelling', 'book_flight', 'credit_limit', 'schedule_maintenance', 'travel_suggestion', 'payday', 'food_last', 'cancel_reservation', 'definition', 'yes', 'bill_due', 'where_are_you_from', 'oil_change_how', 'credit_limit']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5592, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : next_holiday
pair class : ['next_holiday']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['how_old_are_you', 'next_holiday', 'who_made_you', 'gas_type', 'weather', 'lost_luggage', 'tell_joke', 'transfer', 'meal_suggestion', 'confirm_reservation', 'pto_balance', 'exchange_rate', 'restaurant_reservation', 'last_maintenance', 'reset_settings', 'next_holiday']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5286, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [15]
current labels : bill_balance
pair class : ['bill_balance']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['play_music', 'pto_balance', 'nutrition_info', 'tire_pressure', 'car_rental', 'bill_balance', 'restaurant_reservation', 'pay_bill', 'freeze_account', 'application_status', 'change_speed', 'vaccines', 'next_song', 'accept_reservations', 'do_you_have_pets', 'bill_balance']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5368, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [12]
current labels : traffic
pair class : ['traffic']
mask: [False False False False False False False False False False False False
  True False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [15]
current labels : plug_type
pair class : ['plug_type']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [12, 15]
['definition', 'thank_you', 'improve_credit_score', 'traffic', 'oil_change_how', 'balance', 'calories', 'plug_type', 'international_visa', 'weather', 'restaurant_suggestion', 'credit_score', 'traffic', 'ingredients_list', 'shopping_list', 'plug_type']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0758, grad_fn=<DivBackward0>)

[26,    40] loss_total: 41.738 loss_supervised_contrasive:  15.742 loss_intent :3006.760 
skip_time: 0
total : 1215
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [15]
current labels : interest_rate
pair class : ['interest_rate']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['shopping_list', 'user_name', 'transactions', 'interest_rate', 'calendar', 'book_hotel', 'new_card', 'meaning_of_life', 'report_lost_card', 'flip_coin', 'what_song', 'cancel_reservation', 'timezone', 'insurance_change', 'travel_notification', 'interest_rate']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5333, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [15]
current labels : roll_dice
pair class : ['roll_dice']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['goodbye', 'lost_luggage', 'fun_fact', 'bill_balance', 'no', 'roll_dice', 'cancel_reservation', 'meaning_of_life', 'restaurant_suggestion', 'change_accent', 'whisper_mode', 'user_name', 'schedule_meeting', 'plug_type', 'apr', 'roll_dice']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5340, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [15]
current labels : lost_luggage
pair class : ['lost_luggage']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [12]
current labels : plug_type
pair class : ['plug_type']
mask: [False False False False False False False False False False False False
  True False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 12]
['change_accent', 'restaurant_reservation', 'last_maintenance', 'make_call', 'bill_due', 'lost_luggage', 'current_location', 'w2', 'replacement_card_duration', 'timezone', 'plug_type', 'update_playlist', 'plug_type', 'what_can_i_ask_you', 'calendar_update', 'lost_luggage']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0734, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : who_made_you
pair class : ['who_made_you']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['find_phone', 'meal_suggestion', 'who_made_you', 'fun_fact', 'gas_type', 'reminder', 'food_last', 'restaurant_suggestion', 'uber', 'translate', 'pin_change', 'how_busy', 'rollover_401k', 'change_language', 'whisper_mode', 'who_made_you']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5524, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [1]
current labels : transactions
pair class : ['transactions']
mask: [False  True False False False False False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [15]
current labels : freeze_account
pair class : ['freeze_account']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [1, 15]
['transactions', 'transactions', 'where_are_you_from', 'order_checks', 'measurement_conversion', 'spending_history', 'freeze_account', 'greeting', 'how_old_are_you', 'order_status', 'improve_credit_score', 'change_volume', 'smart_home', 'food_last', 'damaged_card', 'freeze_account']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0756, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 13
current skips : [15]
current labels : todo_list_update
pair class : ['todo_list_update']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['schedule_meeting', 'change_volume', 'pto_used', 'how_old_are_you', 'travel_notification', 'cook_time', 'w2', 'maybe', 'gas_type', 'rollover_401k', 'order', 'spelling', 'make_call', 'todo_list_update', 'taxes', 'todo_list_update']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5196, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [9]
current labels : oil_change_how
pair class : ['oil_change_how']
mask: [False False False False False False False False False  True False False
 False False]
count: 14
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [13]
current labels : pto_used
pair class : ['pto_used']
mask: [False False False False False False False False False False False False
 False  True]
count: 14
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [9, 13]
['shopping_list', 'replacement_card_duration', 'how_old_are_you', 'transfer', 'oil_change_how', 'nutrition_info', 'meaning_of_life', 'credit_score', 'meal_suggestion', 'oil_change_how', 'pto_used', 'payday', 'what_song', 'pto_used']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([14, 14])
after summing res.shape : torch.Size([])
res.shape : torch.Size([14, 14])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.9441, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [9]
current labels : travel_notification
pair class : ['travel_notification']
mask: [False False False False False False False False False  True False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : greeting
pair class : ['greeting']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [9, 15]
['travel_notification', 'confirm_reservation', 'change_language', 'report_lost_card', 'restaurant_reviews', 'exchange_rate', 'weather', 'goodbye', 'report_fraud', 'travel_notification', 'credit_limit_change', 'thank_you', 'greeting', 'translate', 'nutrition_info', 'greeting']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0877, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [7]
current labels : car_rental
pair class : ['car_rental']
mask: [False False False False False False False  True False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [10]
current labels : pay_bill
pair class : ['pay_bill']
mask: [False False False False False False False False False False  True False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [14]
current labels : timezone
pair class : ['timezone']
mask: [False False False False False False False False False False False False
 False False  True False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : what_is_your_name
pair class : ['what_is_your_name']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 4
pairs see from labels :  4
All skippings : [7, 10, 14, 15]
['taxes', 'gas_type', 'smart_home', 'car_rental', 'payday', 'pto_used', 'schedule_maintenance', 'car_rental', 'pay_bill', 'timezone', 'pay_bill', 'what_is_your_name', 'uber', 'who_made_you', 'timezone', 'what_is_your_name']
---------------------------------------------
concatenate got h_i : torch.Size([4, 768])
concatenate got h_j :  torch.Size([4, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([4, 1])
pos_sim.shape : torch.Size([4])
len(neg) : 4
loss_s_cl: tensor(0.6897, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : meal_suggestion
pair class : ['meal_suggestion']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [4]
current labels : next_holiday
pair class : ['next_holiday']
mask: [False False False False  True False False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 4]
['credit_score', 'meal_suggestion', 'transfer', 'next_holiday', 'next_holiday', 'lost_luggage', 'change_user_name', 'spelling', 'book_hotel', 'book_flight', 'cancel', 'share_location', 'whisper_mode', 'goodbye', 'insurance_change', 'meal_suggestion']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0740, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : order_status
pair class : ['order_status']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['do_you_have_pets', 'order_status', 'directions', 'tell_joke', 'damaged_card', 'shopping_list', 'greeting', 'payday', 'who_made_you', 'no', 'insurance', 'ingredient_substitution', 'what_can_i_ask_you', 'account_blocked', 'tire_change', 'order_status']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5303, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : calendar_update
pair class : ['calendar_update']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['time', 'restaurant_suggestion', 'calendar_update', 'income', 'restaurant_reservation', 'application_status', 'routing', 'tire_change', 'pto_request_status', 'repeat', 'what_song', 'calculator', 'spelling', 'how_old_are_you', 'plug_type', 'calendar_update']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5378, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [15]
current labels : change_accent
pair class : ['change_accent']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['change_accent', 'payday', 'travel_suggestion', 'new_card', 'directions', 'pto_request', 'alarm', 'card_declined', 'mpg', 'order_status', 'traffic', 'report_lost_card', 'next_song', 'meal_suggestion', 'credit_limit', 'change_accent']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5243, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [9]
current labels : pto_request_status
pair class : ['pto_request_status']
mask: [False False False False False False False False False  True False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : report_fraud
pair class : ['report_fraud']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [9, 15]
['how_busy', 'car_rental', 'pto_request_status', 'improve_credit_score', 'ingredients_list', 'credit_limit', 'thank_you', 'rewards_balance', 'reset_settings', 'pto_request_status', 'smart_home', 'direct_deposit', 'report_fraud', 'calendar', 'play_music', 'report_fraud']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0646, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : jump_start
pair class : ['jump_start']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['translate', 'roll_dice', 'todo_list_update', 'improve_credit_score', 'jump_start', 'next_song', 'definition', 'gas', 'restaurant_reviews', 'timer', 'credit_limit', 'tire_change', 'book_flight', 'cancel', 'how_old_are_you', 'jump_start']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5362, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : apr
pair class : ['apr']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [11]
current labels : maybe
pair class : ['maybe']
mask: [False False False False False False False False False False False  True
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 11]
['balance', 'apr', 'todo_list', 'cancel', 'calendar_update', 'what_are_your_hobbies', 'order', 'maybe', 'gas', 'current_location', 'oil_change_when', 'maybe', 'spelling', 'recipe', 'change_user_name', 'apr']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0807, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : transactions
pair class : ['transactions']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['food_last', 'apr', 'insurance', 'nutrition_info', 'travel_notification', 'schedule_meeting', 'what_can_i_ask_you', 'todo_list_update', 'transactions', 'do_you_have_pets', 'travel_alert', 'vaccines', 'change_accent', 'reset_settings', 'travel_suggestion', 'transactions']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5334, grad_fn=<DivBackward0>)

[27,    10] loss_total: 41.251 loss_supervised_contrasive:  3.666 loss_intent :751.700 
skip_time: 0
total : 1232
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : pin_change
pair class : ['pin_change']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['credit_limit', 'who_do_you_work_for', 'todo_list', 'cook_time', 'schedule_meeting', 'text', 'expiration_date', 'travel_suggestion', 'damaged_card', 'pin_change', 'insurance', 'carry_on', 'report_fraud', 'change_speed', 'recipe', 'pin_change']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5427, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [14]
current labels : bill_balance
pair class : ['bill_balance']
mask: [False False False False False False False False False False False False
 False False  True False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 13
current skips : [15]
current labels : shopping_list_update
pair class : ['shopping_list_update']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [14, 15]
['make_call', 'calendar', 'flight_status', 'international_fees', 'traffic', 'bill_balance', 'w2', 'text', 'update_playlist', 'meeting_schedule', 'travel_suggestion', 'mpg', 'user_name', 'shopping_list_update', 'bill_balance', 'shopping_list_update']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0732, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [15]
current labels : cancel_reservation
pair class : ['cancel_reservation']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [8]
current labels : play_music
pair class : ['play_music']
mask: [False False False False False False False False  True False False False
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 8]
['cancel_reservation', 'book_hotel', 'play_music', 'smart_home', 'no', 'calories', 'repeat', 'report_lost_card', 'play_music', 'change_volume', 'share_location', 'redeem_rewards', 'meeting_schedule', 'time', 'gas_type', 'cancel_reservation']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0770, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : freeze_account
pair class : ['freeze_account']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [14]
current labels : update_playlist
pair class : ['update_playlist']
mask: [False False False False False False False False False False False False
 False False  True False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 14]
['accept_reservations', 'freeze_account', 'book_flight', 'routing', 'measurement_conversion', 'text', 'calendar_update', 'change_speed', 'cancel_reservation', 'time', 'whisper_mode', 'update_playlist', 'reminder', 'yes', 'update_playlist', 'freeze_account']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0730, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 13
current skips : [15]
current labels : whisper_mode
pair class : ['whisper_mode']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['weather', 'definition', 'schedule_meeting', 'change_user_name', 'gas_type', 'schedule_maintenance', 'bill_due', 'fun_fact', 'ingredients_list', 'do_you_have_pets', 'exchange_rate', 'new_card', 'vaccines', 'whisper_mode', 'redeem_rewards', 'whisper_mode']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5816, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [10]
current labels : reminder
pair class : ['reminder']
mask: [False False False False False False False False False False  True False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [9]
current labels : min_payment
pair class : ['min_payment']
mask: [False False False False False False False False False  True False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : travel_suggestion
pair class : ['travel_suggestion']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [10, 9, 15]
['pto_request', 'reminder', 'rollover_401k', 'no', 'restaurant_reservation', 'direct_deposit', 'schedule_maintenance', 'fun_fact', 'min_payment', 'min_payment', 'reminder', 'what_is_your_name', 'travel_suggestion', 'food_last', 'bill_due', 'travel_suggestion']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.1153, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [10]
current labels : card_declined
pair class : ['card_declined']
mask: [False False False False False False False False False False  True False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [15]
current labels : timer
pair class : ['timer']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [10, 15]
['nutrition_info', 'cook_time', 'carry_on', 'card_declined', 'who_do_you_work_for', 'timer', 'new_card', 'freeze_account', 'credit_score', 'interest_rate', 'card_declined', 'make_call', 'tire_change', 'bill_due', 'how_busy', 'timer']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0754, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : traffic
pair class : ['traffic']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['shopping_list', 'credit_score', 'w2', 'share_location', 'spending_history', 'international_visa', 'meaning_of_life', 'jump_start', 'schedule_maintenance', 'traffic', 'lost_luggage', 'restaurant_reservation', 'timer', 'yes', 'thank_you', 'traffic']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5312, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [15]
current labels : tell_joke
pair class : ['tell_joke']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['tell_joke', 'freeze_account', 'change_volume', 'shopping_list', 'roll_dice', 'timezone', 'directions', 'balance', 'account_blocked', 'maybe', 'todo_list', 'shopping_list_update', 'pto_request', 'schedule_meeting', 'restaurant_suggestion', 'tell_joke']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5462, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [15]
current labels : goodbye
pair class : ['goodbye']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [13]
current labels : change_accent
pair class : ['change_accent']
mask: [False False False False False False False False False False False False
 False  True False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 13]
['pto_request_status', 'nutrition_info', 'uber', 'pto_used', 'fun_fact', 'new_card', 'goodbye', 'rewards_balance', 'meal_suggestion', 'todo_list_update', 'who_made_you', 'change_accent', 'change_speed', 'change_accent', 'transactions', 'goodbye']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0718, grad_fn=<DivBackward0>)

[27,    20] loss_total: 40.954 loss_supervised_contrasive:  7.035 loss_intent :1503.406 
skip_time: 0
total : 1242
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [15]
current labels : roll_dice
pair class : ['roll_dice']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['next_song', 'change_ai_name', 'reminder', 'confirm_reservation', 'freeze_account', 'roll_dice', 'maybe', 'directions', 'change_volume', 'improve_credit_score', 'find_phone', 'order_checks', 'vaccines', 'income', 'redeem_rewards', 'roll_dice']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5324, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [15]
current labels : international_visa
pair class : ['international_visa']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['what_are_your_hobbies', 'plug_type', 'transactions', 'international_visa', 'travel_notification', 'how_old_are_you', 'cancel_reservation', 'distance', 'measurement_conversion', 'greeting', 'expiration_date', 'whisper_mode', 'restaurant_reservation', 'insurance_change', 'make_call', 'international_visa']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5240, grad_fn=<DivBackward0>)

----
masking label debug : ['book_hotel' 'book_hotel']
current labels  book_hotel
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 9
current skips : [13 15]
current labels : book_hotel
pair class : ['book_hotel' 'book_hotel']
mask: [False False False False False False False False False False False False
 False  True False  True]
count: 16
numbers of pairs one label : 2
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [13, 15]
['exchange_rate', 'nutrition_info', 'timer', 'where_are_you_from', 'distance', 'order_checks', 'pay_bill', 'next_holiday', 'alarm', 'book_hotel', 'find_phone', 'carry_on', 'share_location', 'book_hotel', 'fun_fact', 'book_hotel']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0735, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [15]
current labels : who_do_you_work_for
pair class : ['who_do_you_work_for']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['share_location', 'oil_change_how', 'meaning_of_life', 'update_playlist', 'application_status', 'bill_balance', 'change_ai_name', 'who_do_you_work_for', 'how_busy', 'change_user_name', 'apr', 'fun_fact', 'last_maintenance', 'definition', 'timezone', 'who_do_you_work_for']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5447, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [15]
current labels : expiration_date
pair class : ['expiration_date']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['flip_coin', 'calendar', 'date', 'expiration_date', 'reset_settings', 'greeting', 'accept_reservations', 'pto_balance', 'cook_time', 'what_are_your_hobbies', 'are_you_a_bot', 'confirm_reservation', 'goodbye', 'ingredient_substitution', 'measurement_conversion', 'expiration_date']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5307, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : thank_you
pair class : ['thank_you']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['what_are_your_hobbies', 'thank_you', 'bill_balance', 'recipe', 'play_music', 'insurance_change', 'transactions', 'min_payment', 'food_last', 'tire_pressure', 'distance', 'calendar', 'do_you_have_pets', 'traffic', 'calendar_update', 'thank_you']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5474, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [14]
current labels : meaning_of_life
pair class : ['meaning_of_life']
mask: [False False False False False False False False False False False False
 False False  True False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : last_maintenance
pair class : ['last_maintenance']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [14, 15]
['recipe', 'change_ai_name', 'meaning_of_life', 'international_fees', 'insurance', 'change_speed', 'application_status', 'rollover_401k', 'translate', 'last_maintenance', 'routing', 'how_old_are_you', 'credit_score', 'travel_alert', 'meaning_of_life', 'last_maintenance']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0750, grad_fn=<DivBackward0>)

----
masking label debug : ['travel_alert' 'travel_alert']
current labels  travel_alert
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 0
current skips : [13 15]
current labels : travel_alert
pair class : ['travel_alert' 'travel_alert']
mask: [False False False False False False False False False False False False
 False  True False  True]
count: 16
numbers of pairs one label : 2
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [13, 15]
['travel_alert', 'min_payment', 'expiration_date', 'what_can_i_ask_you', 'text', 'income', 'timezone', 'change_volume', 'weather', 'shopping_list', 'vaccines', 'rollover_401k', 'w2', 'travel_alert', 'find_phone', 'travel_alert']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0712, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 13
current skips : [15]
current labels : jump_start
pair class : ['jump_start']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['text', 'payday', 'what_is_your_name', 'distance', 'carry_on', 'are_you_a_bot', 'direct_deposit', 'pin_change', 'spending_history', 'tell_joke', 'redeem_rewards', 'maybe', 'taxes', 'jump_start', 'w2', 'jump_start']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5356, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [15]
current labels : roll_dice
pair class : ['roll_dice']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['lost_luggage', 'restaurant_suggestion', 'apr', 'exchange_rate', 'change_ai_name', 'spelling', 'roll_dice', 'reminder_update', 'car_rental', 'reset_settings', 'new_card', 'oil_change_how', 'card_declined', 'account_blocked', 'shopping_list_update', 'roll_dice']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5289, grad_fn=<DivBackward0>)

[27,    30] loss_total: 42.081 loss_supervised_contrasive:  11.531 loss_intent :2255.103 
skip_time: 0
total : 1252
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : book_flight
pair class : ['book_flight']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['credit_limit_change', 'flight_status', 'oil_change_when', 'what_song', 'order', 'pto_used', 'plug_type', 'international_fees', 'pto_request_status', 'sync_device', 'current_location', 'book_flight', 'restaurant_reviews', 'gas_type', 'change_speed', 'book_flight']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5321, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [7]
current labels : ingredients_list
pair class : ['ingredients_list']
mask: [False False False False False False False  True False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [13]
current labels : accept_reservations
pair class : ['accept_reservations']
mask: [False False False False False False False False False False False False
 False  True False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : alarm
pair class : ['alarm']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [7, 13, 15]
['pto_balance', 'tire_change', 'who_do_you_work_for', 'recipe', 'translate', 'ingredients_list', 'yes', 'ingredients_list', 'transfer', 'report_lost_card', 'accept_reservations', 'card_declined', 'alarm', 'accept_reservations', 'international_fees', 'alarm']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.1111, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : current_location
pair class : ['current_location']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['tell_joke', 'cook_time', 'lost_luggage', 'restaurant_suggestion', 'jump_start', 'update_playlist', 'tire_pressure', 'reset_settings', 'routing', 'definition', 'replacement_card_duration', 'reminder_update', 'current_location', 'rollover_401k', 'spelling', 'current_location']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5851, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [15]
current labels : international_visa
pair class : ['international_visa']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['uber', 'are_you_a_bot', 'change_accent', 'what_can_i_ask_you', 'flight_status', 'todo_list_update', 'timer', 'cancel_reservation', 'account_blocked', 'flip_coin', 'international_visa', 'schedule_maintenance', 'plug_type', 'last_maintenance', 'translate', 'international_visa']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5448, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [8]
current labels : interest_rate
pair class : ['interest_rate']
mask: [False False False False False False False False  True False False False
 False False False False]
count: 16
numbers of pairs one label : 1
----
masking label debug : ['calories' 'calories']
current labels  calories
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 5
current skips : [10 15]
current labels : calories
pair class : ['calories' 'calories']
mask: [False False False False False False False False False False  True False
 False False False  True]
count: 16
numbers of pairs one label : 2
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [8, 10, 15]
['tire_pressure', 'interest_rate', 'order_checks', 'order_status', 'order', 'calories', 'date', 'bill_due', 'interest_rate', 'pin_change', 'calories', 'direct_deposit', 'redeem_rewards', 'gas', 'insurance_change', 'calories']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.1169, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : credit_score
pair class : ['credit_score']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['credit_limit', 'sync_device', 'flip_coin', 'international_visa', 'apr', 'restaurant_suggestion', 'what_is_your_name', 'vaccines', 'oil_change_when', 'order_checks', 'reminder', 'pay_bill', 'pto_request', 'do_you_have_pets', 'credit_score', 'credit_score']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5499, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [15]
current labels : change_language
pair class : ['change_language']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [13]
current labels : credit_limit_change
pair class : ['credit_limit_change']
mask: [False False False False False False False False False False False False
 False  True False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 13]
['next_song', 'restaurant_reviews', 'meeting_schedule', 'what_song', 'pin_change', 'shopping_list_update', 'todo_list', 'change_language', 'w2', 'last_maintenance', 'credit_limit_change', 'ingredient_substitution', 'sync_device', 'credit_limit_change', 'balance', 'change_language']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0785, grad_fn=<DivBackward0>)

----
masking label debug : ['oil_change_when' 'oil_change_when']
current labels  oil_change_when
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 2
current skips : [11 15]
current labels : oil_change_when
pair class : ['oil_change_when' 'oil_change_when']
mask: [False False False False False False False False False False False  True
 False False False  True]
count: 16
numbers of pairs one label : 2
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [11, 15]
['time', 'directions', 'oil_change_when', 'pin_change', 'transactions', 'schedule_meeting', 'tire_pressure', 'credit_limit_change', 'pto_used', 'are_you_a_bot', 'no', 'oil_change_when', 'calculator', 'exchange_rate', 'distance', 'oil_change_when']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0757, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : transfer
pair class : ['transfer']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['calculator', 'change_user_name', 'goodbye', 'tell_joke', 'international_fees', 'replacement_card_duration', 'user_name', 'roll_dice', 'pto_used', 'international_visa', 'account_blocked', 'freeze_account', 'car_rental', 'weather', 'transfer', 'transfer']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5279, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [15]
current labels : reminder_update
pair class : ['reminder_update']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['yes', 'bill_balance', 'interest_rate', 'make_call', 'gas_type', 'reminder_update', 'oil_change_how', 'greeting', 'how_busy', 'bill_due', 'measurement_conversion', 'calories', 'routing', 'min_payment', 'alarm', 'reminder_update']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5315, grad_fn=<DivBackward0>)

[27,    40] loss_total: 41.550 loss_supervised_contrasive:  15.496 loss_intent :3006.792 
skip_time: 0
total : 1262
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : sync_device
pair class : ['sync_device']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['expiration_date', 'last_maintenance', 'current_location', 'how_old_are_you', 'change_ai_name', 'reminder_update', 'taxes', 'restaurant_reservation', 'date', 'play_music', 'improve_credit_score', 'sync_device', 'cancel', 'pay_bill', 'pto_balance', 'sync_device']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5342, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [4]
current labels : where_are_you_from
pair class : ['where_are_you_from']
mask: [False False False False  True False False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
----
masking label debug : ['who_made_you' 'who_made_you']
current labels  who_made_you
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 10
current skips : [14 15]
current labels : who_made_you
pair class : ['who_made_you' 'who_made_you']
mask: [False False False False False False False False False False False False
 False False  True  True]
count: 16
numbers of pairs one label : 2
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [4, 14, 15]
['confirm_reservation', 'weather', 'where_are_you_from', 'repeat', 'where_are_you_from', 'calculator', 'rewards_balance', 'time', 'book_hotel', 'oil_change_how', 'who_made_you', 'make_call', 'sync_device', 'change_language', 'who_made_you', 'who_made_you']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.1140, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [15]
current labels : meeting_schedule
pair class : ['meeting_schedule']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [9]
current labels : application_status
pair class : ['application_status']
mask: [False False False False False False False False False  True False False
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 9]
['meeting_schedule', 'damaged_card', 'whisper_mode', 'restaurant_reviews', 'todo_list', 'pto_balance', 'food_last', 'current_location', 'application_status', 'application_status', 'user_name', 'income', 'calculator', 'direct_deposit', 'todo_list_update', 'meeting_schedule']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0826, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [12]
current labels : uber
pair class : ['uber']
mask: [False False False False False False False False False False False False
  True False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [9]
current labels : transfer
pair class : ['transfer']
mask: [False False False False False False False False False  True False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [15]
current labels : change_volume
pair class : ['change_volume']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [12, 9, 15]
['jump_start', 'change_language', 'payday', 'ingredient_substitution', 'replacement_card_duration', 'pto_request', 'next_song', 'uber', 'transfer', 'transfer', 'change_volume', 'find_phone', 'uber', 'what_is_your_name', 'taxes', 'change_volume']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.1111, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [11]
current labels : report_fraud
pair class : ['report_fraud']
mask: [False False False False False False False False False False False  True
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : flip_coin
pair class : ['flip_coin']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [11, 15]
['report_fraud', 'find_phone', 'oil_change_how', 'ingredients_list', 'what_are_your_hobbies', 'flight_status', 'spending_history', 'pto_balance', 'flip_coin', 'taxes', 'what_song', 'report_fraud', 'thank_you', 'mpg', 'next_holiday', 'flip_coin']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0734, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : order
pair class : ['order']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['lost_luggage', 'who_do_you_work_for', 'no', 'reminder_update', 'traffic', 'interest_rate', 'flight_status', 'alarm', 'meaning_of_life', 'damaged_card', 'accept_reservations', 'travel_alert', 'next_holiday', 'change_language', 'order', 'order']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5387, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [8]
current labels : repeat
pair class : ['repeat']
mask: [False False False False False False False False  True False False False
 False False]
count: 14
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [13]
current labels : definition
pair class : ['definition']
mask: [False False False False False False False False False False False False
 False  True]
count: 14
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [8, 13]
['measurement_conversion', 'improve_credit_score', 'meal_suggestion', 'yes', 'gas', 'insurance_change', 'repeat', 'how_busy', 'repeat', 'rewards_balance', 'confirm_reservation', 'definition', 'rollover_401k', 'definition']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([14, 14])
after summing res.shape : torch.Size([])
res.shape : torch.Size([14, 14])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.9426, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : spelling
pair class : ['spelling']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['change_ai_name', 'income', 'application_status', 'mpg', 'calories', 'maybe', 'oil_change_how', 'balance', 'timezone', 'card_declined', 'current_location', 'spelling', 'gas', 'report_fraud', 'pto_request', 'spelling']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5365, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [9]
current labels : account_blocked
pair class : ['account_blocked']
mask: [False False False False False False False False False  True False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [15]
current labels : tire_change
pair class : ['tire_change']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [9, 15]
['who_do_you_work_for', 'make_call', 'application_status', 'account_blocked', 'min_payment', 'traffic', 'carry_on', 'tire_change', 'expiration_date', 'account_blocked', 'change_speed', 'current_location', 'rewards_balance', 'restaurant_suggestion', 'w2', 'tire_change']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0723, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [6]
current labels : change_ai_name
pair class : ['change_ai_name']
mask: [False False False False False False  True False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [15]
current labels : what_are_your_hobbies
pair class : ['what_are_your_hobbies']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [6, 15]
['insurance_change', 'where_are_you_from', 'freeze_account', 'reset_settings', 'change_ai_name', 'oil_change_how', 'change_ai_name', 'card_declined', 'transactions', 'traffic', 'what_are_your_hobbies', 'apr', 'lost_luggage', 'date', 'what_is_your_name', 'what_are_your_hobbies']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0656, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [15]
current labels : timer
pair class : ['timer']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [5]
current labels : food_last
pair class : ['food_last']
mask: [False False False False False  True False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [13]
current labels : routing
pair class : ['routing']
mask: [False False False False False False False False False False False False
 False  True False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [15, 5, 13]
['timer', 'who_made_you', 'report_lost_card', 'food_last', 'reminder_update', 'food_last', 'repeat', 'w2', 'sync_device', 'last_maintenance', 'routing', 'play_music', 'schedule_meeting', 'routing', 'taxes', 'timer']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.1138, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : pto_request_status
pair class : ['pto_request_status']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['schedule_maintenance', 'todo_list_update', 'pto_request_status', 'change_accent', 'card_declined', 'weather', 'find_phone', 'report_fraud', 'play_music', 'rewards_balance', 'meaning_of_life', 'book_flight', 'payday', 'cancel_reservation', 'balance', 'pto_request_status']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5381, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [13]
current labels : what_are_your_hobbies
pair class : ['what_are_your_hobbies']
mask: [False False False False False False False False False False False False
 False  True False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [15]
current labels : pto_request
pair class : ['pto_request']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [13, 15]
['bill_balance', 'spelling', 'insurance', 'spending_history', 'shopping_list', 'replacement_card_duration', 'what_are_your_hobbies', 'play_music', 'plug_type', 'ingredients_list', 'pto_request', 'calendar_update', 'application_status', 'what_are_your_hobbies', 'next_holiday', 'pto_request']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0747, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : min_payment
pair class : ['min_payment']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['shopping_list', 'insurance', 'timezone', 'exchange_rate', 'bill_balance', 'translate', 'directions', 'book_hotel', 'last_maintenance', 'what_is_your_name', 'damaged_card', 'min_payment', 'accept_reservations', 'restaurant_reservation', 'nutrition_info', 'min_payment']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5380, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : recipe
pair class : ['recipe']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['pay_bill', 'alarm', 'interest_rate', 'income', 'travel_alert', 'change_user_name', 'exchange_rate', 'redeem_rewards', 'recipe', 'book_flight', 'restaurant_reviews', 'cook_time', 'rollover_401k', 'repeat', 'what_song', 'recipe']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5239, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [4]
current labels : international_visa
pair class : ['international_visa']
mask: [False False False False  True False False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [15]
current labels : tell_joke
pair class : ['tell_joke']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [14]
current labels : car_rental
pair class : ['car_rental']
mask: [False False False False False False False False False False False False
 False False  True False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [4, 15, 14]
['pto_request_status', 'international_visa', 'date', 'tell_joke', 'international_visa', 'car_rental', 'change_speed', 'goodbye', 'traffic', 'user_name', 'what_can_i_ask_you', 'rollover_401k', 'w2', 'replacement_card_duration', 'car_rental', 'tell_joke']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.1127, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [15]
current labels : bill_due
pair class : ['bill_due']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['directions', 'report_lost_card', 'insurance_change', 'who_made_you', 'gas_type', 'rollover_401k', 'roll_dice', 'bill_due', 'redeem_rewards', 'plug_type', 'expiration_date', 'vaccines', 'calories', 'how_busy', 'traffic', 'bill_due']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5299, grad_fn=<DivBackward0>)

[28,    10] loss_total: 41.195 loss_supervised_contrasive:  3.611 loss_intent :751.698 
skip_time: 0
total : 1279
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : improve_credit_score
pair class : ['improve_credit_score']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['directions', 'ingredient_substitution', 'last_maintenance', 'flip_coin', 'insurance', 'gas', 'mpg', 'payday', 'order_checks', 'improve_credit_score', 'update_playlist', 'shopping_list_update', 'pto_request', 'calendar', 'sync_device', 'improve_credit_score']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5217, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [11]
current labels : order_status
pair class : ['order_status']
mask: [False False False False False False False False False False False  True
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : cancel
pair class : ['cancel']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [11, 15]
['mpg', 'next_holiday', 'order_status', 'schedule_meeting', 'current_location', 'reset_settings', 'calories', 'time', 'food_last', 'thank_you', 'calendar_update', 'order_status', 'cancel', 'update_playlist', 'calendar', 'cancel']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0746, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [15]
current labels : yes
pair class : ['yes']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['gas_type', 'translate', 'transfer', 'yes', 'travel_notification', 'flip_coin', 'payday', 'todo_list', 'change_language', 'pay_bill', 'roll_dice', 'change_user_name', 'credit_limit_change', 'date', 'reminder', 'yes']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5372, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : todo_list
pair class : ['todo_list']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 13
current skips : [14]
current labels : carry_on
pair class : ['carry_on']
mask: [False False False False False False False False False False False False
 False False  True False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 14]
['meeting_schedule', 'tire_change', 'change_language', 'timer', 'todo_list', 'rewards_balance', 'restaurant_reviews', 'order_status', 'pto_request_status', 'uber', 'are_you_a_bot', 'pin_change', 'timezone', 'carry_on', 'carry_on', 'todo_list']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0670, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [8]
current labels : spelling
pair class : ['spelling']
mask: [False False False False False False False False  True False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [15]
current labels : rollover_401k
pair class : ['rollover_401k']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [14]
current labels : thank_you
pair class : ['thank_you']
mask: [False False False False False False False False False False False False
 False False  True False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [8, 15, 14]
['spelling', 'cancel_reservation', 'no', 'todo_list', 'uber', 'weather', 'calendar_update', 'rollover_401k', 'spelling', 'thank_you', 'make_call', 'how_old_are_you', 'tire_pressure', 'pin_change', 'thank_you', 'rollover_401k']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.1111, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [10]
current labels : improve_credit_score
pair class : ['improve_credit_score']
mask: [False False False False False False False False False False  True False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : income
pair class : ['income']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [10, 15]
['improve_credit_score', 'roll_dice', 'income', 'translate', 'whisper_mode', 'new_card', 'schedule_meeting', 'what_can_i_ask_you', 'travel_suggestion', 'calendar', 'improve_credit_score', 'maybe', 'goodbye', 'text', 'restaurant_suggestion', 'income']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0756, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [14]
current labels : change_volume
pair class : ['change_volume']
mask: [False False False False False False False False False False False False
 False False  True False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [8]
current labels : schedule_maintenance
pair class : ['schedule_maintenance']
mask: [False False False False False False False False  True False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [15]
current labels : restaurant_suggestion
pair class : ['restaurant_suggestion']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [14, 8, 15]
['reminder_update', 'shopping_list', 'calendar_update', 'change_volume', 'how_old_are_you', 'are_you_a_bot', 'schedule_maintenance', 'todo_list', 'schedule_maintenance', 'transactions', 'restaurant_suggestion', 'definition', 'pto_balance', 'cook_time', 'change_volume', 'restaurant_suggestion']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.1135, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 13
current skips : [15]
current labels : user_name
pair class : ['user_name']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['sync_device', 'next_song', 'uber', 'recipe', 'definition', 'book_flight', 'time', 'todo_list_update', 'how_old_are_you', 'w2', 'tell_joke', 'schedule_meeting', 'replacement_card_duration', 'user_name', 'calculator', 'user_name']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5281, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [7]
current labels : direct_deposit
pair class : ['direct_deposit']
mask: [False False False False False False False  True False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : yes
pair class : ['yes']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [7, 15]
['pay_bill', 'gas', 'confirm_reservation', 'new_card', 'change_accent', 'direct_deposit', 'date', 'direct_deposit', 'freeze_account', 'yes', 'what_song', 'measurement_conversion', 'current_location', 'international_fees', 'balance', 'yes']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0772, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [12]
current labels : confirm_reservation
pair class : ['confirm_reservation']
mask: [False False False False False False False False False False False False
  True False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : jump_start
pair class : ['jump_start']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [12, 15]
['measurement_conversion', 'pin_change', 'credit_limit', 'transfer', 'spending_history', 'confirm_reservation', 'reset_settings', 'shopping_list_update', 'bill_due', 'flight_status', 'credit_limit_change', 'meeting_schedule', 'confirm_reservation', 'fun_fact', 'jump_start', 'jump_start']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0741, grad_fn=<DivBackward0>)

[28,    20] loss_total: 40.503 loss_supervised_contrasive:  6.529 loss_intent :1503.395 
skip_time: 0
total : 1289
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : international_fees
pair class : ['international_fees']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [14]
current labels : credit_limit
pair class : ['credit_limit']
mask: [False False False False False False False False False False False False
 False False  True False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 14]
['weather', 'gas_type', 'pin_change', 'car_rental', 'international_fees', 'tire_change', 'account_blocked', 'shopping_list', 'measurement_conversion', 'book_hotel', 'accept_reservations', 'credit_limit', 'oil_change_how', 'what_song', 'credit_limit', 'international_fees']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0767, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : reset_settings
pair class : ['reset_settings']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['vaccines', 'cook_time', 'reset_settings', 'user_name', 'yes', 'distance', 'report_lost_card', 'change_user_name', 'gas', 'how_old_are_you', 'balance', 'greeting', 'repeat', 'share_location', 'order', 'reset_settings']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5330, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : change_ai_name
pair class : ['change_ai_name']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['travel_alert', 'direct_deposit', 'freeze_account', 'report_lost_card', 'where_are_you_from', 'pto_balance', 'exchange_rate', 'spelling', 'schedule_maintenance', 'change_ai_name', 'order', 'last_maintenance', 'travel_notification', 'insurance_change', 'income', 'change_ai_name']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5335, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : pto_request
pair class : ['pto_request']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['order_checks', 'ingredients_list', 'what_song', 'timer', 'new_card', 'transfer', 'travel_notification', 'calories', 'damaged_card', 'pto_request', 'plug_type', 'next_holiday', 'mpg', 'make_call', 'jump_start', 'pto_request']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5267, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : taxes
pair class : ['taxes']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['apr', 'measurement_conversion', 'restaurant_suggestion', 'ingredients_list', 'international_fees', 'no', 'oil_change_when', 'weather', 'tire_pressure', 'next_holiday', 'restaurant_reviews', 'change_speed', 'taxes', 'plug_type', 'reminder', 'taxes']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5316, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : bill_balance
pair class : ['bill_balance']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['time', 'replacement_card_duration', 'bill_balance', 'maybe', 'ingredient_substitution', 'rollover_401k', 'change_speed', 'todo_list_update', 'travel_alert', 'play_music', 'insurance', 'find_phone', 'pto_used', 'greeting', 'order_status', 'bill_balance']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5447, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [9]
current labels : travel_suggestion
pair class : ['travel_suggestion']
mask: [False False False False False False False False False  True False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [15]
current labels : pto_request_status
pair class : ['pto_request_status']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [9, 15]
['travel_notification', 'travel_suggestion', 'timezone', 'pto_request_status', 'redeem_rewards', 'smart_home', 'reminder_update', 'report_fraud', 'uber', 'travel_suggestion', 'min_payment', 'oil_change_how', 'card_declined', 'credit_score', 'oil_change_when', 'pto_request_status']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0763, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : next_holiday
pair class : ['next_holiday']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [13]
current labels : damaged_card
pair class : ['damaged_card']
mask: [False False False False False False False False False False False False
 False  True False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 13]
['no', 'next_holiday', 'order_checks', 'fun_fact', 'order', 'pay_bill', 'pto_balance', 'calendar', 'flight_status', 'damaged_card', 'change_language', 'freeze_account', 'where_are_you_from', 'damaged_card', 'cancel_reservation', 'next_holiday']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0737, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [15]
current labels : text
pair class : ['text']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [2]
current labels : transactions
pair class : ['transactions']
mask: [False False  True False False False False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [14]
current labels : food_last
pair class : ['food_last']
mask: [False False False False False False False False False False False False
 False False  True False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [10]
current labels : jump_start
pair class : ['jump_start']
mask: [False False False False False False False False False False  True False
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 4
pairs see from labels :  4
All skippings : [15, 2, 14, 10]
['text', 'transactions', 'transactions', 'shopping_list', 'food_last', 'smart_home', 'lost_luggage', 'jump_start', 'definition', 'recipe', 'jump_start', 'cancel', 'repeat', 'who_do_you_work_for', 'food_last', 'text']
---------------------------------------------
concatenate got h_i : torch.Size([4, 768])
concatenate got h_j :  torch.Size([4, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([4, 1])
pos_sim.shape : torch.Size([4])
len(neg) : 4
loss_s_cl: tensor(0.6922, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [14]
current labels : confirm_reservation
pair class : ['confirm_reservation']
mask: [False False False False False False False False False False False False
 False False  True False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : last_maintenance
pair class : ['last_maintenance']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [14, 15]
['reminder_update', 'accept_reservations', 'what_is_your_name', 'cook_time', 'shopping_list_update', 'flip_coin', 'translate', 'confirm_reservation', 'cancel', 'new_card', 'recipe', 'last_maintenance', 'oil_change_when', 'thank_you', 'confirm_reservation', 'last_maintenance']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0776, grad_fn=<DivBackward0>)

[28,    30] loss_total: 41.252 loss_supervised_contrasive:  10.195 loss_intent :2255.097 
skip_time: 0
total : 1299
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [10]
current labels : share_location
pair class : ['share_location']
mask: [False False False False False False False False False False  True False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : make_call
pair class : ['make_call']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [10, 15]
['fun_fact', 'flight_status', 'do_you_have_pets', 'accept_reservations', 'weather', 'share_location', 'routing', 'pto_balance', 'change_user_name', 'meal_suggestion', 'share_location', 'rewards_balance', 'vaccines', 'cancel', 'make_call', 'make_call']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0734, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [15]
current labels : credit_limit_change
pair class : ['credit_limit_change']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [14]
current labels : tire_pressure
pair class : ['tire_pressure']
mask: [False False False False False False False False False False False False
 False False  True False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 14]
['credit_limit_change', 'alarm', 'make_call', 'bill_due', 'where_are_you_from', 'tire_pressure', 'international_visa', 'rewards_balance', 'nutrition_info', 'account_blocked', 'transfer', 'fun_fact', 'calculator', 'sync_device', 'tire_pressure', 'credit_limit_change']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0801, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : international_visa
pair class : ['international_visa']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['timezone', 'gas_type', 'insurance', 'change_accent', 'expiration_date', 'meal_suggestion', 'update_playlist', 'international_fees', 'international_visa', 'flight_status', 'ingredients_list', 'damaged_card', 'change_speed', 'next_song', 'application_status', 'international_visa']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5439, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : current_location
pair class : ['current_location']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['alarm', 'are_you_a_bot', 'next_song', 'find_phone', 'order_checks', 'restaurant_reservation', 'date', 'book_hotel', 'uber', 'user_name', 'pin_change', 'carry_on', 'current_location', 'recipe', 'who_do_you_work_for', 'current_location']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5342, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : whisper_mode
pair class : ['whisper_mode']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['change_volume', 'whisper_mode', 'order', 'international_fees', 'bill_due', 'vaccines', 'do_you_have_pets', 'smart_home', 'nutrition_info', 'w2', 'report_lost_card', 'freeze_account', 'calendar', 'book_flight', 'account_blocked', 'whisper_mode']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5405, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : roll_dice
pair class : ['roll_dice']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['application_status', 'meeting_schedule', 'change_language', 'change_accent', 'roll_dice', 'direct_deposit', 'maybe', 'credit_limit', 'routing', 'flight_status', 'mpg', 'repeat', 'payday', 'are_you_a_bot', 'income', 'roll_dice']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5239, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [11]
current labels : improve_credit_score
pair class : ['improve_credit_score']
mask: [False False False False False False False False False False False  True
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [15]
current labels : yes
pair class : ['yes']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [12]
current labels : reminder
pair class : ['reminder']
mask: [False False False False False False False False False False False False
  True False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [11, 15, 12]
['pto_used', 'user_name', 'improve_credit_score', 'yes', 'spending_history', 'redeem_rewards', 'no', 'reminder', 'meal_suggestion', 'change_ai_name', 'nutrition_info', 'improve_credit_score', 'reminder', 'car_rental', 'smart_home', 'yes']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.1157, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : smart_home
pair class : ['smart_home']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['todo_list', 'tell_joke', 'greeting', 'taxes', 'pto_used', 'transactions', 'credit_limit_change', 'update_playlist', 'share_location', 'smart_home', 'change_language', 'translate', 'exchange_rate', 'min_payment', 'cancel', 'smart_home']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5846, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [15]
current labels : update_playlist
pair class : ['update_playlist']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['credit_score', 'transfer', 'what_song', 'update_playlist', 'spending_history', 'change_user_name', 'greeting', 'bill_balance', 'taxes', 'distance', 'car_rental', 'order_checks', 'calories', 'what_can_i_ask_you', 'tell_joke', 'update_playlist']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5369, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [12]
current labels : distance
pair class : ['distance']
mask: [False False False False False False False False False False False False
  True False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : restaurant_reviews
pair class : ['restaurant_reviews']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [12, 15]
['insurance_change', 'oil_change_when', 'distance', 'apr', 'book_flight', 'flip_coin', 'routing', 'pto_balance', 'travel_alert', 'credit_score', 'calculator', 'ingredients_list', 'distance', 'timer', 'restaurant_reviews', 'restaurant_reviews']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0719, grad_fn=<DivBackward0>)

[28,    40] loss_total: 41.645 loss_supervised_contrasive:  14.256 loss_intent :3006.779 
skip_time: 0
total : 1309
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [10]
current labels : ingredient_substitution
pair class : ['ingredient_substitution']
mask: [False False False False False False False False False False  True False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : reminder_update
pair class : ['reminder_update']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [10, 15]
['cancel_reservation', 'schedule_maintenance', 'fun_fact', 'flip_coin', 'distance', 'meeting_schedule', 'roll_dice', 'shopping_list_update', 'do_you_have_pets', 'ingredient_substitution', 'ingredient_substitution', 'goodbye', 'how_old_are_you', 'what_are_your_hobbies', 'reminder_update', 'reminder_update']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0728, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [5]
current labels : how_busy
pair class : ['how_busy']
mask: [False False False False False  True False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [15]
current labels : tire_change
pair class : ['tire_change']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [5, 15]
['how_busy', 'lost_luggage', 'do_you_have_pets', 'tire_change', 'order_status', 'how_busy', 'credit_score', 'time', 'restaurant_reservation', 'vaccines', 'meeting_schedule', 'expiration_date', 'interest_rate', 'meal_suggestion', 'meaning_of_life', 'tire_change']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.1158, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [15]
current labels : carry_on
pair class : ['carry_on']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['travel_alert', 'jump_start', 'next_song', 'thank_you', 'pto_used', 'restaurant_suggestion', 'credit_score', 'carry_on', 'restaurant_reservation', 'tire_pressure', 'oil_change_when', 'calendar_update', 'text', 'lost_luggage', 'new_card', 'carry_on']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5395, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : apr
pair class : ['apr']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['payday', 'cancel_reservation', 'apr', 'what_are_your_hobbies', 'credit_limit', 'insurance_change', 'timer', 'goodbye', 'what_is_your_name', 'ingredient_substitution', 'card_declined', 'whisper_mode', 'alarm', 'tire_change', 'replacement_card_duration', 'apr']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5314, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [5]
current labels : definition
pair class : ['definition']
mask: [False False False False False  True False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [10]
current labels : book_hotel
pair class : ['book_hotel']
mask: [False False False False False False False False False False  True False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : who_do_you_work_for
pair class : ['who_do_you_work_for']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [5, 10, 15]
['greeting', 'how_busy', 'definition', 'oil_change_how', 'pay_bill', 'definition', 'book_hotel', 'time', 'pto_request', 'meal_suggestion', 'book_hotel', 'pto_request_status', 'next_song', 'exchange_rate', 'who_do_you_work_for', 'who_do_you_work_for']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.1108, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [5]
current labels : directions
pair class : ['directions']
mask: [False False False False False  True False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
----
masking label debug : ['interest_rate' 'interest_rate']
current labels  interest_rate
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 12
current skips : [14 15]
current labels : interest_rate
pair class : ['interest_rate' 'interest_rate']
mask: [False False False False False False False False False False False False
 False False  True  True]
count: 16
numbers of pairs one label : 2
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [5, 14, 15]
['how_busy', 'traffic', 'directions', 'report_fraud', 'do_you_have_pets', 'directions', 'cook_time', 'taxes', 'whisper_mode', 'sync_device', 'shopping_list_update', 'play_music', 'interest_rate', 'share_location', 'interest_rate', 'interest_rate']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.1148, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [13]
current labels : meaning_of_life
pair class : ['meaning_of_life']
mask: [False False False False False False False False False False False False
 False  True]
count: 14
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [11]
current labels : todo_list_update
pair class : ['todo_list_update']
mask: [False False False False False False False False False False False  True
 False False]
count: 14
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [13, 11]
['who_made_you', 'travel_suggestion', 'alarm', 'what_is_your_name', 'text', 'where_are_you_from', 'yes', 'reminder', 'meaning_of_life', 'plug_type', 'todo_list_update', 'todo_list_update', 'balance', 'meaning_of_life']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([14, 14])
after summing res.shape : torch.Size([])
res.shape : torch.Size([14, 14])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(1.9400, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : pto_request_status
pair class : ['pto_request_status']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [5]
current labels : damaged_card
pair class : ['damaged_card']
mask: [False False False False False  True False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 5]
['pto_balance', 'pto_request_status', 'new_card', 'damaged_card', 'taxes', 'damaged_card', 'min_payment', 'play_music', 'oil_change_when', 'schedule_maintenance', 'flip_coin', 'international_visa', 'what_is_your_name', 'gas_type', 'reminder', 'pto_request_status']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0812, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 13
current skips : [15]
current labels : todo_list_update
pair class : ['todo_list_update']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['jump_start', 'smart_home', 'ingredient_substitution', 'apr', 'cancel_reservation', 'rollover_401k', 'goodbye', 'shopping_list', 'travel_alert', 'whisper_mode', 'replacement_card_duration', 'calendar', 'insurance_change', 'todo_list_update', 'expiration_date', 'todo_list_update']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5292, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [14]
current labels : transfer
pair class : ['transfer']
mask: [False False False False False False False False False False False False
 False False  True False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : report_lost_card
pair class : ['report_lost_card']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [14, 15]
['transfer', 'order_checks', 'redeem_rewards', 'rollover_401k', 'how_old_are_you', 'restaurant_suggestion', 'pay_bill', 'pto_balance', 'timezone', 'credit_score', 'make_call', 'calendar', 'report_lost_card', 'calendar_update', 'transfer', 'report_lost_card']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0756, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [15]
current labels : cook_time
pair class : ['cook_time']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['do_you_have_pets', 'plug_type', 'balance', 'carry_on', 'reset_settings', 'damaged_card', 'cook_time', 'what_song', 'who_do_you_work_for', 'definition', 'pto_request', 'travel_notification', 'recipe', 'alarm', 'make_call', 'cook_time']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5433, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [4]
current labels : pay_bill
pair class : ['pay_bill']
mask: [False False False False  True False False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : insurance
pair class : ['insurance']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [4, 15]
['pay_bill', 'report_lost_card', 'goodbye', 'uber', 'pay_bill', 'freeze_account', 'change_speed', 'food_last', 'insurance', 'whisper_mode', 'travel_suggestion', 'where_are_you_from', 'ingredient_substitution', 'reminder', 'shopping_list_update', 'insurance']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0817, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [15]
current labels : alarm
pair class : ['alarm']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['mpg', 'travel_suggestion', 'international_fees', 'interest_rate', 'nutrition_info', 'alarm', 'credit_score', 'book_hotel', 'maybe', 'pto_used', 'pin_change', 'routing', 'whisper_mode', 'yes', 'next_holiday', 'alarm']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5449, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [15]
current labels : credit_limit_change
pair class : ['credit_limit_change']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['credit_limit_change', 'shopping_list_update', 'taxes', 'payday', 'timer', 'definition', 'card_declined', 'greeting', 'order', 'calendar', 'cook_time', 'cancel_reservation', 'todo_list_update', 'maybe', 'calculator', 'credit_limit_change']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5406, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : meal_suggestion
pair class : ['meal_suggestion']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['order_checks', 'fun_fact', 'insurance', 'transactions', 'change_user_name', 'traffic', 'calculator', 'carry_on', 'car_rental', 'flip_coin', 'next_song', 'distance', 'alarm', 'tell_joke', 'meal_suggestion', 'meal_suggestion']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5211, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [5]
current labels : credit_limit
pair class : ['credit_limit']
mask: [False False False False False  True False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [10]
current labels : traffic
pair class : ['traffic']
mask: [False False False False False False False False False False  True False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : user_name
pair class : ['user_name']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [5, 10, 15]
['credit_limit', 'damaged_card', 'oil_change_when', 'calendar_update', 'min_payment', 'credit_limit', 'where_are_you_from', 'traffic', 'improve_credit_score', 'bill_balance', 'traffic', 'expiration_date', 'user_name', 'what_song', 'flight_status', 'user_name']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.1119, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : text
pair class : ['text']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['are_you_a_bot', 'pto_used', 'balance', 'order_checks', 'taxes', 'shopping_list', 'exchange_rate', 'payday', 'distance', 'no', 'sync_device', 'text', 'yes', 'expiration_date', 'credit_limit_change', 'text']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5375, grad_fn=<DivBackward0>)

[29,    10] loss_total: 41.641 loss_supervised_contrasive:  4.057 loss_intent :751.690 
skip_time: 0
total : 1326
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [15]
current labels : international_visa
pair class : ['international_visa']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [10]
current labels : income
pair class : ['income']
mask: [False False False False False False False False False False  True False
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 10]
['share_location', 'w2', 'plug_type', 'international_visa', 'transactions', 'income', 'freeze_account', 'change_language', 'gas_type', 'order_status', 'income', 'apr', 'replacement_card_duration', 'who_do_you_work_for', 'routing', 'international_visa']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0761, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : definition
pair class : ['definition']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['who_do_you_work_for', 'play_music', 'book_hotel', 'text', 'food_last', 'bill_due', 'travel_suggestion', 'reminder_update', 'change_user_name', 'definition', 'confirm_reservation', 'sync_device', 'what_are_your_hobbies', 'apr', 'change_ai_name', 'definition']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5293, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [15]
current labels : todo_list_update
pair class : ['todo_list_update']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['what_is_your_name', 'how_old_are_you', 'card_declined', 'accept_reservations', 'lost_luggage', 'translate', 'change_ai_name', 'bill_balance', 'change_speed', 'travel_alert', 'todo_list_update', 'improve_credit_score', 'what_can_i_ask_you', 'weather', 'next_song', 'todo_list_update']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5308, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : freeze_account
pair class : ['freeze_account']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['new_card', 'ingredient_substitution', 'timer', 'meal_suggestion', 'play_music', 'tell_joke', 'order', 'rewards_balance', 'flight_status', 'current_location', 'no', 'freeze_account', 'w2', 'reminder_update', 'who_do_you_work_for', 'freeze_account']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5338, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [15]
current labels : travel_notification
pair class : ['travel_notification']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['what_are_your_hobbies', 'smart_home', 'insurance', 'flight_status', 'cancel', 'calories', 'payday', 'shopping_list', 'change_speed', 'ingredient_substitution', 'travel_notification', 'order_checks', 'meal_suggestion', 'repeat', 'accept_reservations', 'travel_notification']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5425, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : reminder_update
pair class : ['reminder_update']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['roll_dice', 'apr', 'timer', 'current_location', 'maybe', 'yes', 'user_name', 'greeting', 'measurement_conversion', 'meeting_schedule', 'calculator', 'where_are_you_from', 'reminder_update', 'plug_type', 'gas', 'reminder_update']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5301, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [12]
current labels : redeem_rewards
pair class : ['redeem_rewards']
mask: [False False False False False False False False False False False False
  True False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [8]
current labels : gas
pair class : ['gas']
mask: [False False False False False False False False  True False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [15]
current labels : bill_due
pair class : ['bill_due']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [12, 8, 15]
['redeem_rewards', 'gas', 'who_made_you', 'tire_pressure', 'change_user_name', 'bill_due', 'goodbye', 'what_are_your_hobbies', 'gas', 'update_playlist', 'repeat', 'international_fees', 'redeem_rewards', 'directions', 'meeting_schedule', 'bill_due']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.1129, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 13
current skips : [15]
current labels : lost_luggage
pair class : ['lost_luggage']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['food_last', 'timezone', 'play_music', 'exchange_rate', 'pto_request_status', 'restaurant_reviews', 'do_you_have_pets', 'meaning_of_life', 'thank_you', 'credit_limit', 'tire_pressure', 'freeze_account', 'insurance', 'lost_luggage', 'credit_score', 'lost_luggage']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5434, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [15]
current labels : greeting
pair class : ['greeting']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['what_is_your_name', 'text', 'change_user_name', 'greeting', 'cancel_reservation', 'what_are_your_hobbies', 'change_ai_name', 'tire_change', 'exchange_rate', 'vaccines', 'how_old_are_you', 'insurance_change', 'translate', 'oil_change_how', 'schedule_maintenance', 'greeting']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5323, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : user_name
pair class : ['user_name']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['roll_dice', 'user_name', 'bill_due', 'do_you_have_pets', 'order_status', 'travel_suggestion', 'smart_home', 'uber', 'recipe', 'restaurant_reservation', 'pto_request', 'min_payment', 'gas_type', 'measurement_conversion', 'direct_deposit', 'user_name']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5411, grad_fn=<DivBackward0>)

[29,    20] loss_total: 42.331 loss_supervised_contrasive:  8.804 loss_intent :1503.371 
skip_time: 0
total : 1336
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [15]
current labels : timezone
pair class : ['timezone']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['traffic', 'direct_deposit', 'find_phone', 'travel_notification', 'pto_used', 'taxes', 'timezone', 'where_are_you_from', 'accept_reservations', 'shopping_list', 'jump_start', 'meal_suggestion', 'application_status', 'cancel', 'reset_settings', 'timezone']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5402, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : freeze_account
pair class : ['freeze_account']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['user_name', 'freeze_account', 'cancel', 'change_volume', 'lost_luggage', 'fun_fact', 'pto_request', 'calendar_update', 'how_busy', 'expiration_date', 'restaurant_reviews', 'schedule_meeting', 'pto_request_status', 'order', 'restaurant_reservation', 'freeze_account']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5364, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : international_visa
pair class : ['international_visa']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['roll_dice', 'translate', 'international_visa', 'smart_home', 'share_location', 'calendar_update', 'spending_history', 'what_can_i_ask_you', 'tire_change', 'repeat', 'calories', 'change_volume', 'card_declined', 'last_maintenance', 'schedule_maintenance', 'international_visa']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5555, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [5]
current labels : pto_balance
pair class : ['pto_balance']
mask: [False False False False False  True False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : date
pair class : ['date']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [5, 15]
['car_rental', 'recipe', 'pto_balance', 'mpg', 'vaccines', 'pto_balance', 'interest_rate', 'book_hotel', 'update_playlist', 'lost_luggage', 'maybe', 'reminder', 'date', 'timezone', 'rewards_balance', 'date']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0769, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : change_accent
pair class : ['change_accent']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['find_phone', 'change_accent', 'tire_change', 'user_name', 'bill_due', 'insurance_change', 'shopping_list_update', 'cancel_reservation', 'jump_start', 'travel_alert', 'fun_fact', 'spelling', 'are_you_a_bot', 'carry_on', 'application_status', 'change_accent']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5402, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : book_flight
pair class : ['book_flight']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['change_language', 'plug_type', 'translate', 'meaning_of_life', 'distance', 'confirm_reservation', 'mpg', 'calculator', 'nutrition_info', 'rewards_balance', 'application_status', 'make_call', 'book_flight', 'insurance', 'tell_joke', 'book_flight']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5453, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [4]
current labels : directions
pair class : ['directions']
mask: [False False False False  True False False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : lost_luggage
pair class : ['lost_luggage']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [4, 15]
['directions', 'last_maintenance', 'date', 'direct_deposit', 'directions', 'change_ai_name', 'next_song', 'what_can_i_ask_you', 'spelling', 'who_do_you_work_for', 'what_are_your_hobbies', 'confirm_reservation', 'lost_luggage', 'routing', 'make_call', 'lost_luggage']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0760, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [15]
current labels : rollover_401k
pair class : ['rollover_401k']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['sync_device', 'who_made_you', 'how_busy', 'rollover_401k', 'income', 'pin_change', 'fun_fact', 'cook_time', 'change_language', 'report_fraud', 'pto_used', 'calendar', 'account_blocked', 'rewards_balance', 'thank_you', 'rollover_401k']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5255, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [10]
current labels : interest_rate
pair class : ['interest_rate']
mask: [False False False False False False False False False False  True False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 11
current skips : [15]
current labels : goodbye
pair class : ['goodbye']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [10, 15]
['restaurant_reservation', 'ingredients_list', 'timezone', 'mpg', 'routing', 'pto_request_status', 'weather', 'interest_rate', 'find_phone', 'shopping_list', 'interest_rate', 'goodbye', 'maybe', 'who_made_you', 'todo_list_update', 'goodbye']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0753, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : meaning_of_life
pair class : ['meaning_of_life']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [9]
current labels : next_holiday
pair class : ['next_holiday']
mask: [False False False False False False False False False  True False False
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 9]
['international_visa', 'meaning_of_life', 'next_holiday', 'play_music', 'bill_balance', 'change_accent', 'how_busy', 'tire_pressure', 'do_you_have_pets', 'next_holiday', 'credit_score', 'update_playlist', 'gas', 'cancel_reservation', 'who_made_you', 'meaning_of_life']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0739, grad_fn=<DivBackward0>)

[29,    30] loss_total: 41.739 loss_supervised_contrasive:  12.958 loss_intent :2255.060 
skip_time: 0
total : 1346
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [1]
current labels : min_payment
pair class : ['min_payment']
mask: [False  True False False False False False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 13
current skips : [15]
current labels : yes
pair class : ['yes']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [1, 15]
['min_payment', 'min_payment', 'food_last', 'weather', 'restaurant_suggestion', 'gas', 'flight_status', 'rollover_401k', 'redeem_rewards', 'ingredients_list', 'update_playlist', 'no', 'report_fraud', 'yes', 'change_volume', 'yes']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0839, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [15]
current labels : ingredients_list
pair class : ['ingredients_list']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [9]
current labels : report_fraud
pair class : ['report_fraud']
mask: [False False False False False False False False False  True False False
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 9]
['ingredients_list', 'nutrition_info', 'meal_suggestion', 'recipe', 'flip_coin', 'report_fraud', 'carry_on', 'oil_change_how', 'meeting_schedule', 'report_fraud', 'jump_start', 'plug_type', 'todo_list', 'expiration_date', 'direct_deposit', 'ingredients_list']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0731, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [5]
current labels : oil_change_when
pair class : ['oil_change_when']
mask: [False False False False False  True False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [15]
current labels : car_rental
pair class : ['car_rental']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [5, 15]
['oil_change_when', 'distance', 'next_song', 'calories', 'date', 'oil_change_when', 'reset_settings', 'account_blocked', 'travel_suggestion', 'oil_change_how', 'car_rental', 'w2', 'insurance_change', 'measurement_conversion', 'ingredients_list', 'car_rental']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0750, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : find_phone
pair class : ['find_phone']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['oil_change_how', 'find_phone', 'recipe', 'schedule_meeting', 'calculator', 'cook_time', 'text', 'restaurant_reviews', 'smart_home', 'measurement_conversion', 'oil_change_when', 'are_you_a_bot', 'pto_balance', 'date', 'timer', 'find_phone']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5436, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : tire_pressure
pair class : ['tire_pressure']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['make_call', 'pto_request_status', 'thank_you', 'last_maintenance', 'change_accent', 'change_speed', 'uber', 'greeting', 'tire_pressure', 'cancel', 'transactions', 'transfer', 'application_status', 'measurement_conversion', 'current_location', 'tire_pressure']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5366, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : meaning_of_life
pair class : ['meaning_of_life']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['improve_credit_score', 'goodbye', 'meaning_of_life', 'damaged_card', 'restaurant_suggestion', 'new_card', 'calendar_update', 'schedule_meeting', 'book_flight', 'international_visa', 'gas_type', 'todo_list_update', 'whisper_mode', 'confirm_reservation', 'change_language', 'meaning_of_life']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5343, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [14]
current labels : credit_limit_change
pair class : ['credit_limit_change']
mask: [False False False False False False False False False False False False
 False False  True False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [6]
current labels : vaccines
pair class : ['vaccines']
mask: [False False False False False False  True False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 13
current skips : [15]
current labels : flip_coin
pair class : ['flip_coin']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [14, 6, 15]
['credit_limit_change', 'meeting_schedule', 'nutrition_info', 'vaccines', 'how_busy', 'bill_balance', 'vaccines', 'weather', 'rollover_401k', 'spending_history', 'alarm', 'international_fees', 'time', 'flip_coin', 'credit_limit_change', 'flip_coin']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.1139, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : jump_start
pair class : ['jump_start']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['w2', 'balance', 'schedule_maintenance', 'time', 'jump_start', 'change_volume', 'mpg', 'ingredients_list', 'timer', 'are_you_a_bot', 'transfer', 'spelling', 'income', 'whisper_mode', 'replacement_card_duration', 'jump_start']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5417, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [15]
current labels : vaccines
pair class : ['vaccines']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [11]
current labels : pin_change
pair class : ['pin_change']
mask: [False False False False False False False False False False False  True
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 11]
['traffic', 'apr', 'restaurant_suggestion', 'vaccines', 'pin_change', 'where_are_you_from', 'account_blocked', 'todo_list', 'repeat', 'order_checks', 'shopping_list_update', 'pin_change', 'rewards_balance', 'card_declined', 'exchange_rate', 'vaccines']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0803, grad_fn=<DivBackward0>)

----
masking label debug : ['thank_you' 'thank_you']
current labels  thank_you
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 3
current skips : [ 8 15]
current labels : thank_you
pair class : ['thank_you' 'thank_you']
mask: [False False False False False False False False  True False False False
 False False False  True]
count: 16
numbers of pairs one label : 2
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [8, 15]
['calendar', 'order_status', 'next_holiday', 'thank_you', 'reminder', 'carry_on', 'accept_reservations', 'who_made_you', 'thank_you', 'change_ai_name', 'oil_change_how', 'how_old_are_you', 'change_language', 'change_user_name', 'current_location', 'thank_you']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0763, grad_fn=<DivBackward0>)

[29,    40] loss_total: 40.950 loss_supervised_contrasive:  16.324 loss_intent :3006.739 
skip_time: 0
total : 1356
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [12]
current labels : what_can_i_ask_you
pair class : ['what_can_i_ask_you']
mask: [False False False False False False False False False False False False
  True False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [15]
current labels : update_playlist
pair class : ['update_playlist']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [12, 15]
['reminder', 'what_can_i_ask_you', 'improve_credit_score', 'are_you_a_bot', 'change_accent', 'greeting', 'meeting_schedule', 'time', 'account_blocked', 'nutrition_info', 'update_playlist', 'travel_notification', 'what_can_i_ask_you', 'flip_coin', 'change_speed', 'update_playlist']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0742, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : sync_device
pair class : ['sync_device']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['directions', 'spending_history', 'todo_list', 'schedule_meeting', 'sync_device', 'balance', 'last_maintenance', 'pto_used', 'replacement_card_duration', 'repeat', 'order', 'restaurant_reviews', 'roll_dice', 'ingredient_substitution', 'spelling', 'sync_device']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5359, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [4]
current labels : transactions
pair class : ['transactions']
mask: [False False False False  True False False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : credit_limit
pair class : ['credit_limit']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [4, 15]
['distance', 'improve_credit_score', 'transactions', 'new_card', 'transactions', 'flight_status', 'tire_change', 'tell_joke', 'payday', 'change_volume', 'alarm', 'what_song', 'no', 'balance', 'credit_limit', 'credit_limit']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0782, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : sync_device
pair class : ['sync_device']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['share_location', 'international_fees', 'book_hotel', 'how_busy', 'translate', 'spending_history', 'reminder_update', 'payday', 'roll_dice', 'car_rental', 'confirm_reservation', 'todo_list', 'card_declined', 'exchange_rate', 'sync_device', 'sync_device']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5352, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : restaurant_reservation
pair class : ['restaurant_reservation']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [9]
current labels : what_song
pair class : ['what_song']
mask: [False False False False False False False False False  True False False
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 9]
['bill_balance', 'restaurant_reservation', 'what_song', 'no', 'what_is_your_name', 'pto_request', 'international_fees', 'car_rental', 'order', 'what_song', 'definition', 'account_blocked', 'shopping_list_update', 'book_flight', 'time', 'restaurant_reservation']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0730, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : interest_rate
pair class : ['interest_rate']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['application_status', 'bill_due', 'pto_request', 'how_old_are_you', 'date', 'book_flight', 'travel_alert', 'tire_change', 'reminder_update', 'interest_rate', 'direct_deposit', 'travel_notification', 'taxes', 'definition', 'schedule_meeting', 'interest_rate']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5402, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [13]
current labels : share_location
pair class : ['share_location']
mask: [False False False False False False False False False False False False
 False  True]
count: 14
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [13]
['new_card', 'pay_bill', 'next_holiday', 'share_location', 'credit_limit_change', 'transfer', 'weather', 'tire_pressure', 'restaurant_suggestion', 'calories', 'pin_change', 'yes', 'order_status', 'share_location']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([14, 14])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.2676, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [15]
current labels : application_status
pair class : ['application_status']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['application_status', 'gas_type', 'ingredient_substitution', 'make_call', 'flip_coin', 'transactions', 'how_busy', 'meal_suggestion', 'oil_change_when', 'next_song', 'change_volume', 'pto_balance', 'rewards_balance', 'travel_alert', 'find_phone', 'application_status']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5461, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [1]
current labels : order
pair class : ['order']
mask: [False  True False False False False False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : change_ai_name
pair class : ['change_ai_name']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [1, 15]
['order', 'order', 'greeting', 'pin_change', 'transfer', 'insurance_change', 'last_maintenance', 'meeting_schedule', 'change_ai_name', 'change_speed', 'interest_rate', 'balance', 'smart_home', 'no', 'definition', 'change_ai_name']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0760, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : change_user_name
pair class : ['change_user_name']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['how_old_are_you', 'todo_list_update', 'schedule_maintenance', 'update_playlist', 'confirm_reservation', 'time', 'account_blocked', 'international_visa', 'shopping_list', 'meal_suggestion', 'w2', 'nutrition_info', 'change_user_name', 'calendar', 'calories', 'change_user_name']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.4796, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : order_checks
pair class : ['order_checks']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['travel_alert', 'change_language', 'damaged_card', 'min_payment', 'order_checks', 'are_you_a_bot', 'no', 'oil_change_how', 'meaning_of_life', 'shopping_list_update', 'current_location', 'smart_home', 'cancel', 'measurement_conversion', 'roll_dice', 'order_checks']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5262, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [15]
current labels : alarm
pair class : ['alarm']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['pto_request', 'todo_list_update', 'where_are_you_from', 'pto_used', 'calories', 'improve_credit_score', 'definition', 'repeat', 'restaurant_reservation', 'new_card', 'alarm', 'smart_home', 'update_playlist', 'roll_dice', 'traffic', 'alarm']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5390, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [7]
current labels : new_card
pair class : ['new_card']
mask: [False False False False False False False  True False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 13
current skips : [15]
current labels : reminder
pair class : ['reminder']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [7, 15]
['order_checks', 'new_card', 'redeem_rewards', 'pto_request_status', 'rewards_balance', 'no', 'gas', 'new_card', 'report_fraud', 'uber', 'translate', 'order_status', 'credit_score', 'reminder', 'car_rental', 'reminder']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0759, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : last_maintenance
pair class : ['last_maintenance']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['reset_settings', 'pto_used', 'fun_fact', 'what_song', 'todo_list', 'no', 'bill_balance', 'interest_rate', 'last_maintenance', 'find_phone', 'lost_luggage', 'schedule_meeting', 'schedule_maintenance', 'transfer', 'who_do_you_work_for', 'last_maintenance']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5511, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [15]
current labels : income
pair class : ['income']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['confirm_reservation', 'next_holiday', 'weather', 'income', 'rollover_401k', 'restaurant_suggestion', 'change_user_name', 'replacement_card_duration', 'cook_time', 'todo_list_update', 'what_are_your_hobbies', 'insurance', 'food_last', 'car_rental', 'gas_type', 'income']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5384, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : pay_bill
pair class : ['pay_bill']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['international_visa', 'balance', 'card_declined', 'who_do_you_work_for', 'weather', 'are_you_a_bot', 'exchange_rate', 'redeem_rewards', 'what_is_your_name', 'recipe', 'jump_start', 'rollover_401k', 'pay_bill', 'confirm_reservation', 'whisper_mode', 'pay_bill']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5388, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [14]
current labels : improve_credit_score
pair class : ['improve_credit_score']
mask: [False False False False False False False False False False False False
 False False  True False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : calendar
pair class : ['calendar']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [14, 15]
['redeem_rewards', 'repeat', 'improve_credit_score', 'shopping_list', 'calendar', 'reminder', 'card_declined', 'update_playlist', 'what_can_i_ask_you', 'accept_reservations', 'whisper_mode', 'spelling', 'calendar_update', 'time', 'improve_credit_score', 'calendar']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0745, grad_fn=<DivBackward0>)

[30,    10] loss_total: 42.079 loss_supervised_contrasive:  4.495 loss_intent :751.697 
skip_time: 0
total : 1373
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [4]
current labels : routing
pair class : ['routing']
mask: [False False False False  True False False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [15]
current labels : traffic
pair class : ['traffic']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [4, 15]
['greeting', 'routing', 'plug_type', 'user_name', 'routing', 'balance', 'traffic', 'who_made_you', 'calculator', 'meeting_schedule', 'credit_limit_change', 'yes', 'damaged_card', 'mpg', 'todo_list_update', 'traffic']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0759, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : payday
pair class : ['payday']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['replacement_card_duration', 'who_made_you', 'change_ai_name', 'user_name', 'translate', 'redeem_rewards', 'time', 'cancel_reservation', 'payday', 'insurance_change', 'pay_bill', 'book_flight', 'timezone', 'lost_luggage', 'bill_due', 'payday']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5442, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : meaning_of_life
pair class : ['meaning_of_life']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['change_ai_name', 'min_payment', 'report_fraud', 'jump_start', 'shopping_list', 'report_lost_card', 'book_hotel', 'order', 'tell_joke', 'make_call', 'taxes', 'new_card', 'restaurant_suggestion', 'schedule_meeting', 'meaning_of_life', 'meaning_of_life']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5302, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : direct_deposit
pair class : ['direct_deposit']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [11]
current labels : next_song
pair class : ['next_song']
mask: [False False False False False False False False False False False  True
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 11]
['freeze_account', 'next_holiday', 'greeting', 'repeat', 'direct_deposit', 'share_location', 'spending_history', 'next_song', 'distance', 'alarm', 'smart_home', 'next_song', 'tire_change', 'book_hotel', 'min_payment', 'direct_deposit']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0723, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : damaged_card
pair class : ['damaged_card']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['routing', 'translate', 'replacement_card_duration', 'alarm', 'international_fees', 'vaccines', 'do_you_have_pets', 'mpg', 'uber', 'exchange_rate', 'apr', 'change_speed', 'date', 'ingredient_substitution', 'damaged_card', 'damaged_card']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5300, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : directions
pair class : ['directions']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['who_made_you', 'restaurant_reservation', 'timezone', 'change_language', 'restaurant_suggestion', 'calories', 'cook_time', 'travel_alert', 'directions', 'change_speed', 'how_busy', 'play_music', 'accept_reservations', 'meaning_of_life', 'flight_status', 'directions']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5392, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [3]
current labels : are_you_a_bot
pair class : ['are_you_a_bot']
mask: [False False False  True False False False False False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : book_hotel
pair class : ['book_hotel']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [3, 15]
['what_song', 'are_you_a_bot', 'maybe', 'are_you_a_bot', 'date', 'tire_change', 'transactions', 'tire_pressure', 'book_hotel', 'pto_request', 'confirm_reservation', 'change_accent', 'meal_suggestion', 'change_user_name', 'rewards_balance', 'book_hotel']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0691, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 7
current skips : [15]
current labels : car_rental
pair class : ['car_rental']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['spending_history', 'exchange_rate', 'what_are_your_hobbies', 'what_song', 'sync_device', 'food_last', 'insurance', 'car_rental', 'nutrition_info', 'gas_type', 'tell_joke', 'calendar', 'yes', 'last_maintenance', 'ingredients_list', 'car_rental']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5331, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [9]
current labels : tell_joke
pair class : ['tell_joke']
mask: [False False False False False False False False False  True False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : measurement_conversion
pair class : ['measurement_conversion']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [9, 15]
['expiration_date', 'spending_history', 'tell_joke', 'what_is_your_name', 'direct_deposit', 'travel_suggestion', 'change_accent', 'account_blocked', 'measurement_conversion', 'tell_joke', 'thank_you', 'yes', 'are_you_a_bot', 'calendar_update', 'payday', 'measurement_conversion']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0726, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : bill_due
pair class : ['bill_due']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['insurance_change', 'bill_due', 'exchange_rate', 'credit_limit_change', 'food_last', 'thank_you', 'cancel_reservation', 'tell_joke', 'change_language', 'gas', 'directions', 'no', 'transactions', 'todo_list', 'jump_start', 'bill_due']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5459, grad_fn=<DivBackward0>)

[30,    20] loss_total: 41.736 loss_supervised_contrasive:  8.646 loss_intent :1503.384 
skip_time: 0
total : 1383
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [10]
current labels : international_visa
pair class : ['international_visa']
mask: [False False False False False False False False False False  True False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : rollover_401k
pair class : ['rollover_401k']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [10, 15]
['timer', 'income', 'reminder_update', 'restaurant_reservation', 'international_visa', 'credit_limit_change', 'whisper_mode', 'bill_due', 'reminder', 'order_status', 'international_visa', 'sync_device', 'rollover_401k', 'w2', 'how_old_are_you', 'rollover_401k']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0729, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : replacement_card_duration
pair class : ['replacement_card_duration']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['what_song', 'plug_type', 'last_maintenance', 'what_is_your_name', 'order_checks', 'credit_limit', 'weather', 'ingredients_list', 'current_location', 'replacement_card_duration', 'timezone', 'user_name', 'make_call', 'recipe', 'update_playlist', 'replacement_card_duration']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5431, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : next_song
pair class : ['next_song']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['mpg', 'report_fraud', 'next_song', 'ingredients_list', 'order', 'pay_bill', 'pto_request_status', 'current_location', 'what_can_i_ask_you', 'vaccines', 'text', 'timer', 'distance', 'what_is_your_name', 'book_hotel', 'next_song']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5333, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [9]
current labels : how_old_are_you
pair class : ['how_old_are_you']
mask: [False False False False False False False False False  True False False
 False False False False]
count: 16
numbers of pairs one label : 1
----
masking label debug : ['what_can_i_ask_you' 'what_can_i_ask_you']
current labels  what_can_i_ask_you
---
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([2, 768])
before append h_i and h_j
h_i :  torch.Size([2, 768])
h_j :  torch.Size([2, 768])
idx: 4
current skips : [ 7 15]
current labels : what_can_i_ask_you
pair class : ['what_can_i_ask_you' 'what_can_i_ask_you']
mask: [False False False False False False False  True False False False False
 False False False  True]
count: 16
numbers of pairs one label : 2
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [9, 7, 15]
['how_old_are_you', 'plug_type', 'thank_you', 'pto_request', 'what_can_i_ask_you', 'play_music', 'reset_settings', 'what_can_i_ask_you', 'oil_change_how', 'how_old_are_you', 'do_you_have_pets', 'credit_limit', 'who_do_you_work_for', 'calculator', 'tire_change', 'what_can_i_ask_you']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.1142, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [15]
current labels : thank_you
pair class : ['thank_you']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['do_you_have_pets', 'pto_used', 'timer', 'pto_balance', 'oil_change_when', 'schedule_maintenance', 'thank_you', 'improve_credit_score', 'recipe', 'flip_coin', 'travel_suggestion', 'measurement_conversion', 'new_card', 'restaurant_reservation', 'last_maintenance', 'thank_you']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5356, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : ingredient_substitution
pair class : ['ingredient_substitution']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['who_do_you_work_for', 'ingredient_substitution', 'meaning_of_life', 'expiration_date', 'replacement_card_duration', 'pay_bill', 'bill_due', 'food_last', 'order_status', 'calories', 'timezone', 'income', 'maybe', 'pto_request_status', 'flip_coin', 'ingredient_substitution']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5350, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : calendar_update
pair class : ['calendar_update']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['restaurant_reservation', 'yes', 'routing', 'accept_reservations', 'change_accent', 'lost_luggage', 'carry_on', 'rollover_401k', 'credit_score', 'reminder', 'goodbye', 'play_music', 'damaged_card', 'definition', 'calendar_update', 'calendar_update']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5452, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [15]
current labels : share_location
pair class : ['share_location']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['traffic', 'share_location', 'spending_history', 'current_location', 'application_status', 'account_blocked', 'find_phone', 'report_lost_card', 'who_made_you', 'rollover_401k', 'calculator', 'repeat', 'play_music', 'alarm', 'directions', 'share_location']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5345, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [12]
current labels : report_lost_card
pair class : ['report_lost_card']
mask: [False False False False False False False False False False False False
  True False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 10
current skips : [15]
current labels : restaurant_suggestion
pair class : ['restaurant_suggestion']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [12, 15]
['directions', 'payday', 'current_location', 'todo_list', 'taxes', 'change_accent', 'report_lost_card', 'yes', 'interest_rate', 'apr', 'restaurant_suggestion', 'how_old_are_you', 'report_lost_card', 'income', 'balance', 'restaurant_suggestion']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0807, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [12]
current labels : distance
pair class : ['distance']
mask: [False False False False False False False False False False False False
  True False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [15]
current labels : income
pair class : ['income']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [12, 15]
['schedule_maintenance', 'greeting', 'distance', 'taxes', 'measurement_conversion', 'balance', 'income', 'travel_notification', 'smart_home', 'change_language', 'goodbye', 'change_volume', 'distance', 'food_last', 'pin_change', 'income']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0692, grad_fn=<DivBackward0>)

[30,    30] loss_total: 41.641 loss_supervised_contrasive:  12.702 loss_intent :2255.078 
skip_time: 0
total : 1393
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [7]
current labels : oil_change_how
pair class : ['oil_change_how']
mask: [False False False False False False False  True False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 13
current skips : [15]
current labels : apr
pair class : ['apr']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [7, 15]
['translate', 'where_are_you_from', 'schedule_meeting', 'oil_change_how', 'who_made_you', 'restaurant_suggestion', 'routing', 'oil_change_how', 'what_is_your_name', 'travel_suggestion', 'change_ai_name', 'roll_dice', 'w2', 'apr', 'credit_limit', 'apr']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0718, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : change_user_name
pair class : ['change_user_name']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['do_you_have_pets', 'time', 'jump_start', 'pto_used', 'change_user_name', 'make_call', 'insurance_change', 'application_status', 'gas_type', 'spelling', 'plug_type', 'redeem_rewards', 'pto_request_status', 'order_status', 'change_language', 'change_user_name']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5345, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 4
current skips : [15]
current labels : calendar
pair class : ['calendar']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [7]
current labels : goodbye
pair class : ['goodbye']
mask: [False False False False False False False  True False False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 13
current skips : [14]
current labels : flight_status
pair class : ['flight_status']
mask: [False False False False False False False False False False False False
 False False  True False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [15, 7, 14]
['pto_used', 'pay_bill', 'roll_dice', 'todo_list_update', 'calendar', 'goodbye', 'min_payment', 'goodbye', 'plug_type', 'meeting_schedule', 'account_blocked', 'book_flight', 'direct_deposit', 'flight_status', 'flight_status', 'calendar']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.1163, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 14
current skips : [15]
current labels : share_location
pair class : ['share_location']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['car_rental', 'international_fees', 'traffic', 'change_volume', 'oil_change_when', 'credit_score', 'freeze_account', 'weather', 'cancel', 'change_user_name', 'text', 'apr', 'whisper_mode', 'flip_coin', 'share_location', 'share_location']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5545, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 1
current skips : [13]
current labels : cancel
pair class : ['cancel']
mask: [False False False False False False False False False False False False
 False  True False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : sync_device
pair class : ['sync_device']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [13, 15]
['how_busy', 'cancel', 'update_playlist', 'calculator', 'traffic', 'transfer', 'car_rental', 'definition', 'change_speed', 'improve_credit_score', 'spending_history', 'travel_alert', 'sync_device', 'cancel', 'accept_reservations', 'sync_device']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0769, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 13
current skips : [15]
current labels : change_volume
pair class : ['change_volume']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['credit_score', 'where_are_you_from', 'cancel', 'pto_balance', 'payday', 'carry_on', 'ingredient_substitution', 'card_declined', 'gas_type', 'fun_fact', 'spelling', 'recipe', 'travel_suggestion', 'change_volume', 'next_holiday', 'change_volume']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5391, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [11]
current labels : bill_balance
pair class : ['bill_balance']
mask: [False False False False False False False False False False False  True
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 13
current skips : [15]
current labels : direct_deposit
pair class : ['direct_deposit']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [11, 15]
['restaurant_reviews', 'distance', 'rewards_balance', 'pto_request', 'whisper_mode', 'insurance', 'cook_time', 'freeze_account', 'fun_fact', 'bill_balance', 'maybe', 'bill_balance', 'change_speed', 'direct_deposit', 'translate', 'direct_deposit']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0734, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : meal_suggestion
pair class : ['meal_suggestion']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['international_fees', 'ingredient_substitution', 'expiration_date', 'calendar_update', 'bill_due', 'schedule_maintenance', 'what_can_i_ask_you', 'confirm_reservation', 'meal_suggestion', 'interest_rate', 'apr', 'date', 'reminder_update', 'meeting_schedule', 'reset_settings', 'meal_suggestion']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5335, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [15]
current labels : uber
pair class : ['uber']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['travel_notification', 'international_fees', 'greeting', 'what_are_your_hobbies', 'shopping_list_update', 'credit_score', 'uber', 'measurement_conversion', 'date', 'meaning_of_life', 'alarm', 'schedule_meeting', 'timer', 'gas', 'change_ai_name', 'uber']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5433, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 3
current skips : [11]
current labels : find_phone
pair class : ['find_phone']
mask: [False False False False False False False False False False False  True
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 8
current skips : [15]
current labels : transactions
pair class : ['transactions']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [11, 15]
['uber', 'oil_change_how', 'insurance_change', 'find_phone', 'report_lost_card', 'tire_pressure', 'interest_rate', 'freeze_account', 'transactions', 'shopping_list_update', 'pin_change', 'find_phone', 'do_you_have_pets', 'fun_fact', 'spelling', 'transactions']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0759, grad_fn=<DivBackward0>)

[30,    40] loss_total: 41.296 loss_supervised_contrasive:  16.414 loss_intent :3006.769 
skip_time: 0
total : 1403
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : calendar_update
pair class : ['calendar_update']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [11]
current labels : nutrition_info
pair class : ['nutrition_info']
mask: [False False False False False False False False False False False  True
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 2
pairs see from labels :  2
All skippings : [15, 11]
['shopping_list', 'schedule_meeting', 'calendar_update', 'freeze_account', 'transactions', 'nutrition_info', 'next_holiday', 'fun_fact', 'uber', 'cancel_reservation', 'travel_notification', 'nutrition_info', 'sync_device', 'todo_list', 'shopping_list_update', 'calendar_update']
---------------------------------------------
concatenate got h_i : torch.Size([2, 768])
concatenate got h_j :  torch.Size([2, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([2, 1])
pos_sim.shape : torch.Size([2])
len(neg) : 2
loss_s_cl: tensor(2.0761, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 12
current skips : [15]
current labels : w2
pair class : ['w2']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['cook_time', 'what_are_your_hobbies', 'lost_luggage', 'change_accent', 'time', 'direct_deposit', 'exchange_rate', 'vaccines', 'insurance', 'pto_request_status', 'tire_pressure', 'credit_limit', 'w2', 'directions', 'who_do_you_work_for', 'w2']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5288, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : recipe
pair class : ['recipe']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 5
current skips : [14]
current labels : mpg
pair class : ['mpg']
mask: [False False False False False False False False False False False False
 False False  True False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [11]
current labels : user_name
pair class : ['user_name']
mask: [False False False False False False False False False False False  True
 False False False False]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [15, 14, 11]
['book_hotel', 'calculator', 'recipe', 'goodbye', 'w2', 'mpg', 'expiration_date', 'pin_change', 'restaurant_reviews', 'user_name', 'application_status', 'user_name', 'vaccines', 'change_volume', 'mpg', 'recipe']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.1138, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [12]
current labels : travel_notification
pair class : ['travel_notification']
mask: [False False False False False False False False False False False False
  True False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [8]
current labels : text
pair class : ['text']
mask: [False False False False False False False False  True False False False
 False False False False]
count: 16
numbers of pairs one label : 1
>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 9
current skips : [15]
current labels : where_are_you_from
pair class : ['where_are_you_from']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 3
pairs see from labels :  3
All skippings : [12, 8, 15]
['travel_notification', 'ingredients_list', 'make_call', 'book_flight', 'accept_reservations', 'reminder_update', 'text', 'bill_balance', 'text', 'where_are_you_from', 'timer', 'thank_you', 'travel_notification', 'cook_time', 'shopping_list_update', 'where_are_you_from']
---------------------------------------------
concatenate got h_i : torch.Size([3, 768])
concatenate got h_j :  torch.Size([3, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([3, 1])
pos_sim.shape : torch.Size([3])
len(neg) : 3
loss_s_cl: tensor(1.1146, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 0
current skips : [15]
current labels : min_payment
pair class : ['min_payment']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['min_payment', 'calories', 'order_status', 'pto_balance', 'what_are_your_hobbies', 'tire_pressure', 'gas', 'vaccines', 'credit_limit_change', 'reminder_update', 'transfer', 'spelling', 'jump_start', 'damaged_card', 'travel_alert', 'min_payment']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5358, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 2
current skips : [15]
current labels : cancel_reservation
pair class : ['cancel_reservation']
mask: [False False False False False False False False False False False False
 False False False  True]
count: 16
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [15]
['reset_settings', 'timezone', 'cancel_reservation', 'definition', 'next_holiday', 'sync_device', 'flip_coin', 'book_flight', 'share_location', 'play_music', 'card_declined', 'where_are_you_from', 'oil_change_when', 'report_fraud', 'tire_pressure', 'cancel_reservation']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([16, 16])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.5286, grad_fn=<DivBackward0>)

>>>>>>>>>>>>>
repeat for broadcast : torch.Size([1, 768])
before append h_i and h_j
h_i :  torch.Size([1, 768])
h_j :  torch.Size([1, 768])
idx: 6
current skips : [13]
current labels : account_blocked
pair class : ['account_blocked']
mask: [False False False False False False False False False False False False
 False  True]
count: 14
numbers of pairs one label : 1
the number of pairs for entire batch: 1
pairs see from labels :  1
All skippings : [13]
['carry_on', 'payday', 'pin_change', 'reminder_update', 'expiration_date', 'international_visa', 'account_blocked', 'order_checks', 'travel_suggestion', 'repeat', 'maybe', 'credit_limit', 'flight_status', 'account_blocked']
---------------------------------------------
concatenate got h_i : torch.Size([1, 768])
concatenate got h_j :  torch.Size([1, 768])
<<<<<<<<<<<<<<<<<<<<<
----------
res.shape : torch.Size([14, 14])
after summing res.shape : torch.Size([])
neg_sim.shape : torch.Size([1, 1])
pos_sim.shape : torch.Size([1])
len(neg) : 1
loss_s_cl: tensor(5.2760, grad_fn=<DivBackward0>)

deleate logger for one combination
../../models/Load=True_roberta-base_B=16_lr=5e-06_22_12_2021_08:17.pth
Finished Training
Saving Done !
