{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57578c42-10c1-4187-bc41-007d70b3ed49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "import numpy as np\n",
    "\n",
    "from utils import loss\n",
    "from dataloader import IntentExample\n",
    "from dataloader import load_intent_examples\n",
    "from dataloader import sample\n",
    "from dataloader import InputExample\n",
    "#from dataloader import convert_examples_to_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3b34d6-82c0-4e11-83f7-13ab77b7066a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8b8c880-cfe1-4119-9d56-ce7bdbfdc525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sitindustry/Documents/Reading/implement_papers\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5056a00-c494-4569-9587-7589fa404d9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "473f683e-720f-4bf1-aa42-68a0a13011ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few-Shot Intent Detection via Contrastive Pre-Training and Fine-Tuning\n",
    "# https://github.com/jianguoz/Few-Shot-Intent-Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eba51ac-077e-4bb7-9adf-648c0a4d699e",
   "metadata": {},
   "source": [
    "## Overall Methodlogy \n",
    " - They Pre-train on all relative intent dataset and or target intent  dataset with MLM \n",
    " - Pretained -> self-supervised contrasive learning \n",
    " - Finned Tunned -> Joinly Few intent detection and supervised contrasive learning \n",
    "     - help the problem of fined grain on lots of intents and semantically similar intennts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c417745d-a746-44a2-ab5b-10ef0ae9274b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27bd1614-a747-49a2-8e5c-22e1363f261e",
   "metadata": {},
   "source": [
    "## Todo\n",
    " 1. load data and combined them from every sources\n",
    " 2. preprocess -> embedded two sentence into single encoder -> h\n",
    " 3. use h to calculate contrasive learning loss \n",
    "* dont forget to use nn.CosineSimilarity instead of from scartch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0822ef-a934-4c68-9210-f68b847d4007",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "- N - number of sentences in the batch \n",
    "- t - temperature paramter -> control penalty for neg samples\n",
    "- u - sentecnce or utterances \n",
    "- hi - representation of ui\n",
    "- hi - BERT(ui) -> we use bert-base-uncased\n",
    "- i-th -> order of sentence\n",
    "- hi_bar -> the representation of ui_bar \n",
    "- ui_bar -> the same sentence as ui but (10%) tokens are random masked  (Devlin et al.,2019)\n",
    "- M - the number of maksed tokens in each batch "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcbfec1-08df-4d40-aeeb-e6692764e44f",
   "metadata": {},
   "source": [
    "## Preprocessesing data\n",
    "  Todo \n",
    "  1. tokenize prepare feature\n",
    "  2. dont forget to remove utterances that less than five tokens\n",
    "   * we will all exclude  CLINC-Single-Domain-OOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab74193d-7af1-401e-8bdf-4d2219bc622c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7364f8cd-a722-4615-b700-749e4876f8cc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Trainning process\n",
    " 1. during batch trainning dont forget to randomly maksed, a sentence has \n",
    "    different maksed postion across different trannig epochs\n",
    " 2. (ui ,ui_bar) -> single encoder during batch tranning  (Gao et al., 2021)\n",
    "      #### detail\n",
    "      - they take same sentence to encoder twice : by applying the standard          dropout twice -> possivtive pairs\n",
    "      - take other sentence in mini-batch as negative pairs\n",
    "      - model -> predict positive one among neg\n",
    "      - we have to use BERT encoder from (SimCSE) to do standard dropout \n",
    "      - the way we make match pos or neg samples to feed encoder (SimCSE)\n",
    " 3. add mask language modelling loss  (Devlinet al., 2019; Wu et al., 2020a)\n",
    " 4. P(xm) -> predicted probabilty of maksed token xm over total vocabulary\n",
    " \n",
    " \n",
    " * ref -  (SimCSE: Simple Contrastive Learning of Sentence Embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7825be64-c865-4b14-a905-a700e696692b",
   "metadata": {},
   "source": [
    "## Experiment Setting\n",
    "- contrasive Pretraining \n",
    " 1. Pre-train the combined intent datasets -> combine every dataset (guess)\n",
    " 2. 15 epochs\n",
    " 3. batch size = 64\n",
    " 4. t = 0.1 , lamda = 1.0\n",
    "\n",
    "- Fine tunning \n",
    " 1. 5-shot -> five trainning examples per intents\n",
    " 2. 10-shot -> tens trainning examples per intents\n",
    " 3. batch size = 16\n",
    " 4. t =  {0.1, 0.3, 0.5}\n",
    " 5. λ ∈ {0.01, 0.03, 0.05}\n",
    " 6. 30 epochs\n",
    " 7. apply label smoothing to the intent classifcation loss (Zhang et al. (2020a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0291fadc-cbce-40e3-9912-0bb20e147e90",
   "metadata": {},
   "source": [
    "## Load dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfc1ea66-f77f-4e61-8a61-d92597eb027e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label  seq.in\n"
     ]
    }
   ],
   "source": [
    "!ls ../../dataset/Few-Shot-Intent-Detection/Datasets/CLINC150/oos/train/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac6ca81-ea7a-45c4-90d7-dfa88da41287",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7df78091-2fb2-4362-9771-7060aa6e1635",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_path = '../../dataset/Few-Shot-Intent-Detection/Datasets/CLINC150/train/'\n",
    "N = 100  # number of samples per class (100 full-shot)\n",
    "T = 1 # number of Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92dd38ab-920b-442e-96ca-44e03c9ce8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples = load_intent_examples(train_file_path)\n",
    "# structure of this data  [trials] \n",
    "# trail -> [dict1,dict2,...,dict#intents]\n",
    "# every dict -> {'task':'lable name','examples':[text1,text2,..,textN]}\n",
    "sampled_tasks = [sample(N, train_examples) for i in range(T)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2404605f-ca29-47b1-88f5-a5ae06ce36d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(sampled_tasks[0][0]['examples'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a58a98-acb0-44ed-a030-53ec041f9702",
   "metadata": {},
   "source": [
    "#### summarize make batch procedure\n",
    "  1. tokenize sentence -> ui\n",
    "  2. ui orignal , ui_bar -> masked 10% traditional BERT did\n",
    "  2. use BERT encoder to do standard dropout(SimCSE) to get hi , hi_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d363567-3cc4-47a6-87d1-65e48e5b1e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_lists = []\n",
    "intent_train_examples = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6648437-8c2b-4db4-82fa-0a083c58ca9a",
   "metadata": {},
   "outputs": [],
   "source": [
    " for i in range(T):\n",
    "        tasks = sampled_tasks[i]\n",
    "        label_lists.append([])\n",
    "        intent_train_examples.append([])\n",
    "        \n",
    "        for task in tasks:\n",
    "            label = task['task']\n",
    "            examples = task['examples']\n",
    "            label_lists[-1].append(label)\n",
    "\n",
    "            for j in range(len(examples)):\n",
    "                intent_train_examples[-1].append(InputExample(examples[j], None, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "371554d3-791b-4348-9471-2680f0f404fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(intent_train_examples[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68842c0e-0f7c-412b-9a63-2a174ebae8a8",
   "metadata": {},
   "source": [
    "### Let's breakdown BERT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "67424c0d-ef41-46ea-bc43-6840b236ca16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize input\n",
    "text = \"[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]\"\n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    "\n",
    "# Mask a token that we will try to predict back with `BertForMaskedLM`\n",
    "masked_index = 8\n",
    "tokenized_text[masked_index] = '[MASK]'\n",
    "assert tokenized_text == ['[CLS]', 'who', 'was', 'jim', 'henson', '?', '[SEP]', 'jim', '[MASK]', 'was', 'a', 'puppet', '##eer', '[SEP]']\n",
    "\n",
    "# Convert token to vocabulary indices\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "# Define sentence A and B indices associated to 1st and 2nd sentences (see paper)\n",
    "segments_ids = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n",
    "\n",
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b868ab65-9b6e-45f2-8b1e-d8171d1be86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "model.eval()\n",
    "\n",
    "# If you have a GPU, put everything on cuda\n",
    "tokens_tensor = tokens_tensor.to('cuda')\n",
    "segments_tensors = segments_tensors.to('cuda')\n",
    "model.to('cuda')\n",
    "\n",
    "# Predict all tokens\n",
    "with torch.no_grad():\n",
    "    outputs = model(tokens_tensor, token_type_ids=segments_tensors)\n",
    "    predictions = outputs[0]\n",
    "\n",
    "# confirm we were able to predict 'henson'\n",
    "predicted_index = torch.argmax(predictions[0, masked_index]).item()\n",
    "predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62b3f87-557c-4a1c-9c0f-aa31daecadc8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Note BERT system\n",
    " ### BERT Models\n",
    "- tokenized_text -> tokenize it just the step of \n",
    "breakdown sen into list of word \n",
    "- Mask is the step of let BERT to predict that tokens\n",
    "- convert token to vocabuary indices\n",
    "- define sen A and B by indices\n",
    "- change list of seq -> tensor , both token tensor and segment tensors\n",
    "- duirng evaluate the model require tokens_tensor and segment tensors\n",
    "- trainning_step -> model.train()            -> \n",
    "                 -> ._prepare_inputs(inputs)\n",
    "ref - https://huggingface.co/transformers/v1.0.0/quickstart.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89d37c30-229b-49d2-9cff-d1acd3bdc746",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_features, label_distribution = convert_examples_to_features(intent_train_examples[0], train = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d0bc1267-6244-4ddb-a619-d1f16cb4d8cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('transfer $40 from account a to b', None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intent_train_examples[0][100].text_a , intent_train_examples[0][100].text_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d70caa2a-2d22-4985-b9b6-0c75eb6b78fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tasks = sampled_tasks[0]\n",
    "# label = tasks[0]['task']\n",
    "# label_lists[-1].append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7720cd3f-44d6-4c7e-9e65-d8c1542c13d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://neptune.ai/blog/how-to-code-bert-using-pytorch-tutorial\n",
    "# https://huggingface.co/transformers/v1.0.0/quickstart.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ea7425-c147-468c-b38f-4639e52d1789",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load Pretrain model from vocabuary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d2517a-c1b0-420a-86ca-09ab3120a9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17b4852-25c6-461d-9271-94223363833c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize input \n",
    "text = \"who was Elon musk ? Elon musk was an entrepreneur and business magnate\"\n",
    "tokenized_text = tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9330c8fd-1c14-4268-b917-31b652e627e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask a token that we will try to predict with MLM\n",
    "masked_index = 7\n",
    "tokenized_text[masked_index] = '[MASK]'\n",
    "#assert tokenized_text == [who was Elon musk ? Elon musk was an entrepreneur 'and','business','magnate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1220adb3-a5fb-4c85-a135-d285dfacfc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(tokenized_text))\n",
    "tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436f5386-6a4f-4884-9f62-62a8b42ef61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert token to vocabulary indices\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "# Define sentence A and B indices associated to 1st and 2nd \n",
    "segments_ids = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09447ce1-bef6-4dc9-80f2-513787ffff3c",
   "metadata": {},
   "source": [
    "## Load weight Pretrain model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a4a138-5df4-49c1-8c42-b528fb0c943a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "model.eval()\n",
    "print(\"eval done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce267f1-0e3c-47c3-8156-519f55445d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict hidden states features for each layer\n",
    "encoded_layers, _ = model(tokens_tensor, segments_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba38990a-06d7-4235-afb2-0d9d58efe3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have a hidden states for each of the 12 layers in model bert-base-uncased\n",
    "assert len(encoded_layers) == 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bfb2ee-fbb6-4b99-b8c0-a553680b5182",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "model.eval()\n",
    "\n",
    "# Predict all tokens\n",
    "predictions = model(tokens_tensor, segments_tensors)\n",
    "\n",
    "# confirm we were able to predict 'henson'\n",
    "predicted_index = torch.argmax(predictions[0, masked_index]).item()\n",
    "predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])\n",
    "#assert predicted_token == 'musk'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f340a30-6bd8-4e09-97d6-e92f438451bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a25429-437e-4830-becf-323dd7e38d31",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf7a996-0ae8-4e59-bc5b-5470a28c9802",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10 #None\n",
    "temperature = 0.5  #None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac47bcf-7511-4611-888e-70682e5aa904",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c4be35-757b-4a67-8711-401d5c7e9c6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1464c4e6-2aa4-439f-97fb-cb83aeb744f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.rand(3,2)\n",
    "b = np.random.rand(3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c87e426-08d2-430e-b292-2f42482dde42",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape , b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b19de9-13c9-44be-bc20-d2eb02607daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "self_supervised_cl(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1e0bca-d5a3-4d9c-a586-5051b12097b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1d0a2b-4af5-4d14-9750-d864a8d954a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
