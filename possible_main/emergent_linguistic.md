# Emergent linquistic structure in ann trained by self-supervison 



 Question 
---
 what is this work is about ?   
   show that how large language pretrain model works
      
      * how ?  
          * can learn linquistic structure and syntatic parsing and be able to aproximately reconstruct linquistic stucture   
  



 Keys
--- 
 * language understanding require -> structure are not looking explicitly 
 * normally humand label on treebank structure
 * what the model can captures 
      1. word class (part of speech)
      2. syntatic stucture (grammatical relations or dependencies) 
      3. coreference (which mentions of an entity refer to the same entity) eg. "she" refers back to Rachel
      
      * which is useful to multi-dimentional supervison signal 

  * how often particular word pay attetion to word that have same linguistic relationships ?    


