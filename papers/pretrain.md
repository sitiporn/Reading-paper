# XLNet: Generalize Autoregressive Pretraining for Language Understanding 


## what did authors try to accomplish ?
   - they can get more context from two directional and model good at question answering, semtiment analysis

   - not suffers in fine-tunning process 


# what were the key element of the approach ?
   - they get depency between mask position which BERT neglect that 

# what can you use yourself ?
   - can use as one of  pretrain models 


